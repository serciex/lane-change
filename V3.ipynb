{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serciex/lane-change/blob/main/V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAqx0bsIEQbi"
      },
      "source": [
        "Installing Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AAc_-AeGEE5H",
        "outputId": "c9e91ba0-9c3f-434a-896f-9c2aa2ca9c8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Collecting highway-env\n",
            "  Downloading highway_env-1.10.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a2 in /usr/local/lib/python3.11/dist-packages (from highway-env) (1.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from highway-env) (0.0.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from highway-env) (2.0.2)\n",
            "Requirement already satisfied: pygame>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from highway-env) (2.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from highway-env) (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from highway-env) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from highway-env) (1.15.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a2->highway-env) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a2->highway-env) (4.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->highway-env) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->highway-env) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->highway-env) (1.17.0)\n",
            "Downloading highway_env-1.10.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: highway-env\n",
            "Successfully installed highway-env-1.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pygame\n",
        "!pip install highway-env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Civ2P__KEUSq"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-ZwGd9eVEWGY"
      },
      "outputs": [],
      "source": [
        "#Import Libraries\n",
        "from os import truncate\n",
        "import math\n",
        "import gymnasium\n",
        "import highway_env\n",
        "from matplotlib import pyplot as plt\n",
        "import pygame\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "import gym\n",
        "from random import randint\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import tqdm\n",
        "\n",
        "# Workaround for gym compatibility\n",
        "if not hasattr(np, 'bool8'):\n",
        "    np.bool8 = np.bool_\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF-2bRYTIjH6"
      },
      "source": [
        "Environment Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y922XK-cIewr",
        "outputId": "cc39f81a-5035-46d9-b054-10b9711afa96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<highway_env.envs.common.action.ContinuousAction at 0x7e13bf9a64d0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Configure Environment Conditions\n",
        "config = {\n",
        "    \"lanes_count\": 3,\n",
        "    \"lane_width\": 3.75,\n",
        "    \"observation\": {\n",
        "        \"type\": \"Kinematics\",\n",
        "        \"features\": [\"x\", \"y\", \"vx\", \"vy\", \"heading\", \"lat_off\"]\n",
        "    },\n",
        "    \"action\": {\"type\": \"ContinuousAction\"},\"ego_spawn_random\": True,\n",
        "    \"policy_frequency\": 10,\n",
        "}\n",
        "env = gymnasium.make('highway-v0', render_mode='rgb_array', config=config)\n",
        "frames = []\n",
        "\n",
        "# Action Setup\n",
        "highway_env.envs.common.action.ContinuousAction(env, lateral=True,\n",
        "                                                longitudinal=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IDM Controller"
      ],
      "metadata": {
        "id": "t2j5j5Q8Fh9C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2HbWr4urTXQ8"
      },
      "outputs": [],
      "source": [
        "# Environment Manager\n",
        "class ENV(env.__class__):\n",
        "  \"\"\"\n",
        "  s = (v,a,x,y,thetha,id,w,c) ∈ S\n",
        "\n",
        "  Lateral Agent State:\n",
        "  Obs Data:\n",
        "  x = vehicle x position (x)\n",
        "  y = vehicle y position (y)\n",
        "  v = vehicle speed (vx)\n",
        "  thetha = yaw angle (heading)\n",
        "\n",
        "  Input:\n",
        "  a = longitudinal acceleration (longitudinal_control)\n",
        "  id = target lane id\n",
        "  w = lane width\n",
        "  c = road curvature\n",
        "\n",
        "  Extra Data:\n",
        "  vy = lateral rate (vy)\n",
        "  delta_lat_deviation = change in lateral deviation (lat_off)\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, obs, desired_parameters, control_parameters):\n",
        "    self.obs = obs\n",
        "    # Unpack initial parameters: [s0, v0]\n",
        "    self.s0, self.v0 = desired_parameters\n",
        "\n",
        "    # Unpack control parameters: [a, b, delta, T]\n",
        "    self.a, self.b, self.delta, self.T = control_parameters\n",
        "\n",
        "  def ego_state_idm(self):\n",
        "      ax, ay, avx, avy, athetha, _ = self.obs[0]\n",
        "      self._ego_state_idm = {\"x\": ax, \"y\": ay, \"vx\": avx,\"thetha\": athetha,\n",
        "                         \"longitudinal_acceleration\":self.a}\n",
        "\n",
        "      return self._ego_state_idm\n",
        "\n",
        "  def update(self,obs):\n",
        "    self.obs = obs\n",
        "\n",
        "  def ego_state_agent(self,target_id):\n",
        "      ax, ay, avx, avy, athetha, _ = self.obs[0]\n",
        "      vehicle = env.unwrapped.vehicle\n",
        "\n",
        "      # Control Parameters\n",
        "      self.id = target_id\n",
        "\n",
        "      # Environment Parameters\n",
        "      self.s,_ = vehicle.lane.local_coordinates(vehicle.position)\n",
        "      self.w = vehicle.lane.width\n",
        "      self.c = vehicle.lane.heading_at(np.clip(\n",
        "          vehicle.lane.local_coordinates(vehicle.position)[0],\n",
        "          0, vehicle.lane.length))\n",
        "      self.v = math.sqrt(avx**2+avy**2)\n",
        "\n",
        "      self._ego_state_agent = {\"x\": ax, \"y\": ay, \"vx\": self.v,\"thetha\": athetha,\n",
        "                         \"lane_width\":self.w,\"lane_id\":self.id,\n",
        "                         \"self_curvature\":self.c,\n",
        "                         \"longitudinal_acceleration\":self.a}\n",
        "\n",
        "      return self._ego_state_agent\n",
        "\n",
        "  def longitudinal_lead_state(self):\n",
        "      # Lead State Parameters on the same Lane (5 columns: presence, x, y, vx, vy)\n",
        "      ego_vehicle = env.unwrapped.vehicle\n",
        "      lead_vehicle = env.unwrapped.road.neighbour_vehicles(ego_vehicle, lane_index=ego_vehicle.lane_index)[0]\n",
        "\n",
        "      if lead_vehicle:\n",
        "          gap = lead_vehicle.position[0] - ego_vehicle.position[0]\n",
        "          delta_velocity = ego_vehicle.velocity[0] - lead_vehicle.velocity[0]\n",
        "          self._longitudinal_lead_state = {\"x\": gap, \"vx\": delta_velocity}\n",
        "      else:\n",
        "          self._longitudinal_lead_state = {\"x\": 10, \"vx\": 0}\n",
        "\n",
        "      return self._longitudinal_lead_state\n",
        "\n",
        "  def idm_controller(self):\n",
        "      # Unpack input variables: [s, v, delta_v]\n",
        "      s, v, delta_v = self.longitudinal_lead_state()['x'], self.ego_state_idm()['vx'], self.longitudinal_lead_state()['vx']\n",
        "\n",
        "      # Small epsilon to account for very small gaps and avoid division by zero\n",
        "      epsilon = 1e-6\n",
        "\n",
        "      # Desired gap: s* = s0 + v*T + (v * delta_v) / (2 * sqrt(a * b))\n",
        "      desired_gap = self.s0 + max(0, v * self.T + ((v * delta_v) / (2 * math.sqrt(self.a * self.b))))\n",
        "\n",
        "      # IDM acceleration: a_IDM = a * [ 1 - (v / v0)^delta - (s* / s)^2 ]\n",
        "      acceleration = self.a * (1 - (v / self.v0)**self.delta - (desired_gap / (s + epsilon))**2)\n",
        "\n",
        "      return acceleration\n",
        "\n",
        "\n",
        "  #Reward Function\n",
        "  def reward_function(self, obs_old, obs_new, target_id, w1=1, w2=1, w3=0.05):\n",
        "    \"\"\"\n",
        "    Reward Function:\n",
        "\n",
        "    Acceleration Reward: r_acce = w1*f_acce(a_yaw)\n",
        "    a_yaw = lateral acceleration (self.action)\n",
        "\n",
        "    Rate Reward: r_rate = w2*f_rate(w_yaw)\n",
        "    w_yaw = lateral rate (vy)\n",
        "\n",
        "    Time Reward: r_time = w3*f_time (delta_lat_deviation)\n",
        "    delta_lat_deviation = change in lateral deviation (self.lat_off)\n",
        "\n",
        "    Reward = Cummulative Sum of r_acce + Cummulative Sum of r_rate + Cummulative Sum of r_time\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    self.target_id = (\"0\",\"1\",target_id)\n",
        "    target_lane_object = env.unwrapped.road.network.get_lane(self.target_id)\n",
        "    vehicle_s, _ = env.unwrapped.vehicle.lane.local_coordinates(env.unwrapped.vehicle.position)\n",
        "    _ , self.delta_lat_deviaton = target_lane_object.local_coordinates(env.unwrapped.vehicle.position)\n",
        "\n",
        "    obs = obs_new[0]\n",
        "    obs_old = obs_old[0]\n",
        "\n",
        "    w_yaw = (obs[0] * obs[3] - obs[1] * obs[2]) / (obs[0]**2 + obs[1]**2 + 1e-8)\n",
        "    w_yaw_old = (obs_old[0] * obs_old[3] - obs_old[1] * obs_old[2]) / (obs_old[0]**2 + obs_old[1]**2 + 1e-8)\n",
        "\n",
        "    self.w_acce = (w_yaw-w_yaw_old)*env.unwrapped.config['policy_frequency']\n",
        "\n",
        "    # Acceleration Reward\n",
        "    acce_reward = -1*abs(self.w_acce)\n",
        "\n",
        "    # Rate Reward\n",
        "    rate_reward = -1*abs(w_yaw)\n",
        "\n",
        "    # Time Reward\n",
        "    time_reward = -0.05 * abs(self.delta_lat_deviaton)\n",
        "\n",
        "    # Overall Reward\n",
        "    self.reward = w1*acce_reward + w2*rate_reward + w3*time_reward\n",
        "\n",
        "    return [self.reward, acce_reward, rate_reward, time_reward]\n",
        "\n",
        "\n",
        "  def action(self, obs, lateral_action):\n",
        "      self.obs = obs\n",
        "      L = env.unwrapped.vehicle.LENGTH\n",
        "      self.acc = self.idm_controller()\n",
        "\n",
        "      # Calculate steering angle from lateral acceleration\n",
        "      self.angle = math.atan(L * lateral_action / self.ego_state_idm()['vx']**2)\n",
        "\n",
        "      # Return the action as [longitudinal_acceleration, steering_angle]\n",
        "      calculated_action = [self.acc, self.angle]\n",
        "\n",
        "      return calculated_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEPNdVET6-mv"
      },
      "source": [
        "Agent Defintion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim=4, action_dim=2, hidden_size=128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Policy head (actor)\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, action_dim)\n",
        "        )\n",
        "\n",
        "        # Value head (critic)\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.actor(x), self.critic(x)\n",
        "\n",
        "    def action(self, state):\n",
        "        action_logits, value = self.forward(state)\n",
        "\n",
        "        logits = torch.softmax(action_logits,dim=1)\n",
        "        m = torch.distributions.Categorical(logits)\n",
        "        action = m.sample()\n",
        "        log_prob = m.log_prob(action)\n",
        "\n",
        "        return action, log_prob, value"
      ],
      "metadata": {
        "id": "MW5pZ2IxtbyI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enKF1jUjQxpL"
      },
      "source": [
        "Lateral Controller (Gap Checker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ydU55ZY5eX4U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Gap_Controller(env.__class__):\n",
        "    def __init__(self, obs=None, following_gap_threshold=10.0):\n",
        "        # Optionally store an initial observation if provided.\n",
        "        if obs is not None:\n",
        "            self.obs = obs\n",
        "        # Threshold for safety gap with the following vehicle\n",
        "        self.following_gap_threshold = following_gap_threshold\n",
        "\n",
        "    def lane_checker(self):\n",
        "        \"\"\"\n",
        "        Determines which adjacent lane offers the safest gap for a lane change.\n",
        "        It examines the current lane, the lane to the left, and the lane to the right,\n",
        "        returning the lane index that has the largest safe front gap available.\n",
        "        In addition, if the gap with the following vehicle in the target lane is below the safety\n",
        "        threshold, the lane change is aborted and the current lane is chosen.\n",
        "\n",
        "        Returns:\n",
        "            int: The target lane index (the lane with the largest safe gap, or the current lane)\n",
        "                 if the candidate lane's following vehicle gap is unsafe.\n",
        "        \"\"\"\n",
        "        lane_number = len(env.unwrapped.road.network.lanes_list())\n",
        "        ego = env.unwrapped.vehicle\n",
        "\n",
        "        # Initialize front gap values with a default that indicates an unsafe or non-existent gap.\n",
        "        gap_current_front = -float('inf')\n",
        "        gap_right_front = -float('inf')\n",
        "        gap_left_front = -float('inf')\n",
        "\n",
        "        # Initialize following gap values (gap between ego and the vehicle behind)\n",
        "        gap_current_follower = float('inf')\n",
        "        gap_right_follower = float('inf')\n",
        "        gap_left_follower = float('inf')\n",
        "\n",
        "        # Get vehicle neighbours in current lane:\n",
        "        neighbours_current = env.unwrapped.road.neighbour_vehicles(ego, ego.lane_index)\n",
        "        # neighbours_current[0] is the vehicle ahead and [1] is the following vehicle.\n",
        "        if neighbours_current:\n",
        "            if neighbours_current[0]:\n",
        "                gap_current_front = neighbours_current[0].position[0] - ego.position[0]\n",
        "            if len(neighbours_current) > 1 and neighbours_current[1]:\n",
        "                gap_current_follower = ego.position[0] - neighbours_current[1].position[0]\n",
        "\n",
        "        # Compute the left and right lane indices.\n",
        "        current_lane = list(ego.lane_index)\n",
        "        if current_lane[2] > 0:\n",
        "            lane_left = (current_lane[0], current_lane[1], current_lane[2] - 1)\n",
        "        else:\n",
        "            lane_left = ego.lane_index\n",
        "\n",
        "        if current_lane[2] < (lane_number - 1):\n",
        "            lane_right = (current_lane[0], current_lane[1], current_lane[2] + 1)\n",
        "        else:\n",
        "            lane_right = ego.lane_index\n",
        "\n",
        "        # Retrieve neighbour vehicles for the right lane.\n",
        "        neighbours_right = env.unwrapped.road.neighbour_vehicles(ego, lane_index=lane_right)\n",
        "        if neighbours_right:\n",
        "            if neighbours_right[0]:\n",
        "                gap_right_front = neighbours_right[0].position[0] - ego.position[0]\n",
        "            if len(neighbours_right) > 1 and neighbours_right[1]:\n",
        "                gap_right_follower = ego.position[0] - neighbours_right[1].position[0]\n",
        "\n",
        "        # Retrieve neighbour vehicles for the left lane.\n",
        "        neighbours_left = env.unwrapped.road.neighbour_vehicles(ego, lane_index=lane_left)\n",
        "        if neighbours_left:\n",
        "            if neighbours_left[0]:\n",
        "                gap_left_front = neighbours_left[0].position[0] - ego.position[0]\n",
        "            if len(neighbours_left) > 1 and neighbours_left[1]:\n",
        "                gap_left_follower = ego.position[0] - neighbours_left[1].position[0]\n",
        "\n",
        "        # Compare the front gaps: current, right, and left.\n",
        "        front_gaps = [gap_current_front, gap_right_front, gap_left_front]\n",
        "        best_index = np.argmax(front_gaps)\n",
        "\n",
        "        # Determine the target lane index based on the best candidate.\n",
        "        # best_index: 0 => current lane, 1 => right lane, 2 => left lane.\n",
        "        if best_index == 1:\n",
        "            candidate_lane = lane_right\n",
        "            candidate_follower_gap = gap_right_follower\n",
        "        elif best_index == 2:\n",
        "            candidate_lane = lane_left\n",
        "            candidate_follower_gap = gap_left_follower\n",
        "        else:\n",
        "            candidate_lane = ego.lane_index\n",
        "            candidate_follower_gap = gap_current_follower  # in current lane, we don't enforce follower gap condition\n",
        "\n",
        "        # Check if the candidate lane (if different from the current lane)\n",
        "        # meets the follower gap condition.\n",
        "        if candidate_lane != ego.lane_index:\n",
        "            if candidate_follower_gap < self.following_gap_threshold:\n",
        "                # The follower gap is too small; do not change lanes.\n",
        "                target_lane_id = ego.lane_index[2]\n",
        "            else:\n",
        "                target_lane_id = candidate_lane[2]\n",
        "        else:\n",
        "            target_lane_id = ego.lane_index[2]\n",
        "\n",
        "        return target_lane_id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21gxd9D8au1O"
      },
      "source": [
        "Initialize Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gPl1lptqKdAy"
      },
      "outputs": [],
      "source": [
        "# Initialize Environment\n",
        "obs, _ = env.reset()\n",
        "# IDM Initialization\n",
        "control_parameters = [0.1, 5, 4, 4]  # a, b, δ, T\n",
        "desired_parameters = [20, 10.0]       # s0, v0\n",
        "\n",
        "# Initialize Environment Manager and Reward\n",
        "state_manager = ENV(obs,control_parameters=control_parameters,desired_parameters=desired_parameters)\n",
        "\n",
        "# State Manager for Ego and Lead State\n",
        "ego_state_idm = state_manager.ego_state_idm()\n",
        "lead_state = state_manager.longitudinal_lead_state()\n",
        "\n",
        "# Initial Longitundinal Positions\n",
        "ego_position_idm = ego_state_idm['x']\n",
        "lead_position_idm = lead_state['x']\n",
        "\n",
        "# Initial Velocities (using vx for longitudinal control)\n",
        "ego_velocity_idm = ego_state_idm['vx']\n",
        "lead_velocity_idm = lead_state['vx']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZvAgaQfIyK6"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhoITfrZ_nOt"
      },
      "source": [
        "Always Active (Option 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RYA-H09sIw2_"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "\"\"\"Initialization\"\"\"\n",
        "################################################################################\n",
        "\n",
        "# Agent Initialization\n",
        "total_steps_taken = 0\n",
        "lr = 0.01  # Reduced learning rate\n",
        "gamma = 0.99  # Higher discount factor for better long-term rewards\n",
        "\n",
        "# Reasonable hyperparameter sizes\n",
        "buffer_size = int(1e5)  # 100k buffer\n",
        "batch_size = 256  # Standard batch size\n",
        "max_timesteps = int(4e4)  # 40k total timesteps\n",
        "\n",
        "plot_freq_steps = 5000  # Plot every 5000 timesteps\n",
        "\n",
        "# Epsilon-Greedy Parameters\n",
        "epsilon = 0.3\n",
        "\n",
        "# Tracking for performance\n",
        "timestep_rewards = []  # Store rewards by timestep\n",
        "timestep_acce_rewards = []  # Store acceleration rewards\n",
        "timestep_rate_rewards = []  # Store rate rewards\n",
        "timestep_time_rewards = []  # Store time rewards\n",
        "global_returns = []  # Cumulative return tracking\n",
        "loss_history = []  # Loss values\n",
        "timesteps_list = []  # Track timesteps for plotting\n",
        "episode_lengths = []  # Track episode lengths\n",
        "episode_indices = []  # Track episode indices for plotting\n",
        "\n",
        "target_id = env.unwrapped.vehicle.lane_index[-1]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Environment Information\n",
        "L = env.unwrapped.vehicle.LENGTH\n",
        "agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "state_dim = len(agent_state)\n",
        "\n",
        "# Setup Lateral Control Agent\n",
        "target_network = ActorCritic(state_dim).to(device)\n",
        "policy_network = ActorCritic(state_dim).to(device)\n",
        "\n",
        "# Initialize target network\n",
        "target_network.load_state_dict(policy_network.state_dict())\n",
        "\n",
        "# Loss and Criterion Initialization\n",
        "policy_optimizer = torch.optim.Adam(policy_network.parameters(), lr=lr)\n",
        "policy_loss_fn = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "_r2sQPbixCmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "T = 1000  # Max timesteps per rollout\n",
        "K = 10    # Number of policy update epochs\n",
        "batch_size = 64\n",
        "gamma = 0.99\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "vf_coeff_cl = 0.5\n",
        "ent_coef_c2 = 0.01\n",
        "nb_episodes = 1000\n",
        "epsilon = 0.2\n",
        "gae_lambda = 0.99\n",
        "max_episodes_per_rollout = 10\n",
        "\n",
        "\n",
        "# Define action dimension (continuous action space)\n",
        "action_dim = 1  # For lateral acceleration\n",
        "state_dim = 8   # Based on your agent state\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = ActorCritic(state_dim=state_dim, action_dim=action_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Tracking variables\n",
        "total_env_episodes = 0\n",
        "max_reward = float('-inf')\n",
        "batch_average_reward_history = []\n",
        "batch_episode_history = []\n",
        "individual_test_rewards = []\n",
        "individual_test_episodes = []\n",
        "\n",
        "# Training loop\n",
        "for episode in tqdm.tqdm(range(nb_episodes)):\n",
        "\n",
        "    # PPO Initialization\n",
        "    advantages = torch.zeros(T, dtype=torch.float32, device=device)\n",
        "    buffer_states = torch.zeros((T, state_dim), dtype=torch.float32, device=device)\n",
        "    buffer_actions = torch.zeros((T, action_dim), dtype=torch.float32, device=device)\n",
        "    buffer_log_probs = torch.zeros((T), dtype=torch.float32, device=device)\n",
        "    buffer_state_values = torch.zeros((T+1), dtype=torch.float32, device=device)\n",
        "    buffer_rewards = torch.zeros((T), dtype=torch.float32, device=device)\n",
        "    buffer_is_terminal = torch.zeros((T), dtype=torch.float32, device=device)\n",
        "\n",
        "    # Episode control variables\n",
        "    episodes_completed_in_rollout = 0\n",
        "    t = 0  # Actual timesteps collected\n",
        "\n",
        "    # Env Initialization\n",
        "    obs, _ = env.reset()\n",
        "    state_manager.update(obs)\n",
        "\n",
        "    # Initialize target_id\n",
        "    target_id = env.unwrapped.vehicle.lane_index[-1]\n",
        "    agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "\n",
        "    episode_step = 0\n",
        "    episode_return = 0.0\n",
        "    done = False\n",
        "    obs_old = obs\n",
        "\n",
        "    # Collect trajectories with dual stopping condition\n",
        "    while t < T and episodes_completed_in_rollout < max_episodes_per_rollout:\n",
        "\n",
        "        gap_control = Gap_Controller(obs, following_gap_threshold=30)\n",
        "        activated_target_lane = gap_control.lane_checker()\n",
        "\n",
        "        # Determine target lane\n",
        "        target_id = activated_target_lane\n",
        "        agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "\n",
        "        # Convert agent state dict to tensor\n",
        "        state_tensor = torch.tensor(\n",
        "            [agent_state[key] for key in ['vx', 'longitudinal_acceleration', 'x', 'y', 'thetha', 'lane_id', 'lane_width', 'self_curvature']],\n",
        "            dtype=torch.float32, device=device\n",
        "        ).unsqueeze(0)\n",
        "\n",
        "        # Get action and value from model\n",
        "        with torch.no_grad():\n",
        "            agent_action, log_prob, value = model.action(state_tensor)\n",
        "            action = state_manager.action(obs,agent_action)\n",
        "\n",
        "        # Take action in environment\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        done = done or truncated\n",
        "\n",
        "        # Update state manager with new observation and applied longitudinal control\n",
        "        state_manager.update(obs)\n",
        "\n",
        "        # Compute reward based on the new state\n",
        "        reward_components = state_manager.reward_function(obs_old, obs, target_id)\n",
        "        reward = reward_components[0]\n",
        "\n",
        "        # Update episode return\n",
        "        episode_return += reward\n",
        "\n",
        "        # Store trajectory information\n",
        "        buffer_states[t] = state_tensor.squeeze(0)\n",
        "        buffer_actions[t] = agent_action\n",
        "        buffer_log_probs[t] = log_prob\n",
        "        buffer_state_values[t] = value\n",
        "        buffer_rewards[t] = torch.tensor(reward, dtype=torch.float32, device=device)\n",
        "        buffer_is_terminal[t] = done\n",
        "\n",
        "        t += 1  # Increment timestep counter\n",
        "        obs_old = obs\n",
        "\n",
        "        # Update IDM inputs for next iteration\n",
        "        ego_state_idm = state_manager.ego_state_idm()\n",
        "        lead_state = state_manager.longitudinal_lead_state()\n",
        "        gap = lead_state['x']\n",
        "        delta_velocity = lead_state['vx']\n",
        "        input_variables = [gap, ego_state_idm['vx'], delta_velocity]\n",
        "\n",
        "        # If episode is done, track results and increment episode counter\n",
        "        if done:\n",
        "            episodes_completed_in_rollout += 1\n",
        "            total_env_episodes += 1\n",
        "\n",
        "            # Reset environment if more episodes needed\n",
        "            if episodes_completed_in_rollout < max_episodes_per_rollout and t < T:\n",
        "                obs, _ = env.reset()\n",
        "                state_manager.update(obs)\n",
        "                target_id = env.unwrapped.vehicle.lane_index[-1]\n",
        "                obs_old = obs\n",
        "                done = False\n",
        "\n",
        "    # Actual number of timesteps collected\n",
        "    actual_T = t\n",
        "\n",
        "    # Get final value for bootstrapping\n",
        "    if not done and actual_T > 0:\n",
        "        target_id = env.unwrapped.vehicle.lane_index[-1]\n",
        "        agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "        final_state_tensor = torch.tensor(\n",
        "            [agent_state[key] for key in ['vx', 'longitudinal_acceleration', 'x', 'y', 'thetha', 'lane_id', 'lane_width', 'self_curvature']],\n",
        "            dtype=torch.float32, device=device\n",
        "        ).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, final_value = model(final_state_tensor)\n",
        "        buffer_state_values[actual_T] = final_value.squeeze(0)\n",
        "\n",
        "    # Calculate advantages using GAE\n",
        "    gae = 0\n",
        "    advantages = torch.zeros(actual_T, dtype=torch.float32, device=device)\n",
        "\n",
        "    for t_idx in range(actual_T-1, -1, -1):\n",
        "        if buffer_is_terminal[t_idx]:\n",
        "            terminal = 1\n",
        "            next_value = 0\n",
        "        else:\n",
        "            terminal = 0\n",
        "            if t_idx == actual_T-1:\n",
        "                next_value = buffer_state_values[actual_T]\n",
        "            else:\n",
        "                next_value = buffer_state_values[t_idx+1]\n",
        "\n",
        "        delta = buffer_rewards[t_idx] + gamma * next_value * (1-terminal) - buffer_state_values[t_idx]\n",
        "        gae = delta + (gamma * gae_lambda) * gae * (1-terminal)\n",
        "        advantages[t_idx] = gae\n",
        "\n",
        "    # Normalize advantages\n",
        "    if actual_T > 1:\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "    # Prepare data for policy update\n",
        "    advantages_data_loader = DataLoader(\n",
        "        TensorDataset(\n",
        "            advantages.detach(),\n",
        "            buffer_states[:actual_T].detach(),\n",
        "            buffer_actions[:actual_T].detach(),\n",
        "            buffer_log_probs[:actual_T].detach(),\n",
        "            buffer_state_values[:actual_T].detach()),\n",
        "        batch_size=min(batch_size, actual_T),\n",
        "        shuffle=True)\n",
        "\n",
        "    # Policy update (K epochs)\n",
        "    for epoch in range(K):\n",
        "        for batch_data in advantages_data_loader:\n",
        "            b_adv, obs_batch, action_batch, old_log_probs, old_state_values = batch_data\n",
        "\n",
        "            # Forward pass\n",
        "            _, log_probs, value = model.action(obs_batch)\n",
        "\n",
        "            # Calculate ratio\n",
        "            ratio = torch.exp(log_probs - old_log_probs)\n",
        "            returns = b_adv + old_state_values\n",
        "\n",
        "            # Calculate policy loss with clipping\n",
        "            policy_loss_1 = b_adv * ratio\n",
        "            clip_range = epsilon\n",
        "            policy_loss_2 = b_adv * torch.clamp(ratio, 1-clip_range, 1+clip_range)\n",
        "            policy_loss = -torch.min(policy_loss_1, policy_loss_2).mean()\n",
        "\n",
        "            # Calculate value loss\n",
        "            value_loss = F.mse_loss(returns, value.squeeze())\n",
        "\n",
        "            # Calculate entropy loss\n",
        "            m = torch.distributions.Categorical(torch.softmax(log_probs, dim=1))\n",
        "            entropy_loss = -(m.entropy()).mean()\n",
        "\n",
        "            # Combined loss\n",
        "            loss = policy_loss + vf_coeff_cl * value_loss + ent_coef_c2 * entropy_loss\n",
        "\n",
        "            # Optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "            optimizer.step()\n",
        "\n",
        "    # Print rollout info\n",
        "    print(f\"Training Batch {episode}: Collected {actual_T} timesteps, {episodes_completed_in_rollout} episodes\")\n",
        "\n",
        "    # Testing every 10 episodes\n",
        "    if episode % 10 == 0 and episode > 0:\n",
        "        test_rewards = []\n",
        "\n",
        "        for test_ep in range(3):  # Reduced to 3 test episodes for speed\n",
        "            obs, _ = env.reset()\n",
        "            state_manager = ENV(obs, 0.0)\n",
        "            episode_reward = 0\n",
        "            done = False\n",
        "            test_steps = 0\n",
        "\n",
        "            while not done and test_steps < 500:  # Limit test episode length\n",
        "                target_id = env.unwrapped.vehicle.lane_index[-1]\n",
        "                agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "\n",
        "                state_tensor = torch.tensor(\n",
        "                    [agent_state[key] for key in ['vx', 'longitudinal_acceleration', 'x', 'y', 'thetha', 'lane_id', 'lane_width', 'self_curvature']],\n",
        "                    dtype=torch.float32, device=device\n",
        "                ).unsqueeze(0)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    agent_action, log_prob, value = model.action(state_tensor)\n",
        "\n",
        "                action = state_manager.action(obs, agent_action)\n",
        "\n",
        "                obs, reward, done, truncated, info = env.step(action)\n",
        "                done = done or truncated\n",
        "                state_manager.update(obs)\n",
        "\n",
        "                episode_reward += reward\n",
        "                test_steps += 1\n",
        "\n",
        "            test_rewards.append(episode_reward)\n",
        "\n",
        "        avg_test_reward = sum(test_rewards) / len(test_rewards)\n",
        "        current_max = max(test_rewards)\n",
        "\n",
        "        if current_max > max_reward:\n",
        "            max_reward = current_max\n",
        "\n",
        "        batch_average_reward_history.append(avg_test_reward)\n",
        "        batch_episode_history.append(episode)\n",
        "\n",
        "        print(f\"Episode {episode}: Avg Test Reward = {avg_test_reward:.2f}, Max Test Reward = {current_max:.2f}\")\n",
        "\n",
        "    # Plot every 50 episodes\n",
        "    if episode % 50 == 0 and episode > 0 and batch_average_reward_history:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(batch_episode_history, batch_average_reward_history, 'r-o', linewidth=2)\n",
        "        plt.title(f\"Test Performance (Episode {episode}, Max: {max_reward:.1f})\")\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Average Reward\")\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "print(\"Training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "gMY396Rx5F1J",
        "outputId": "2e4f998c-d220-4e54-dd21-7611e2aee922"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1000 [00:26<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-3674191736.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# Reset environment if more episodes needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepisodes_completed_in_rollout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_episodes_per_rollout\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m                 \u001b[0mstate_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mtarget_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvehicle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlane_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;34m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRenderFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRenderFrame\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    331\u001b[0m     ) -> tuple[WrapperObsType, dict[str, Any]]:\n\u001b[1;32m    332\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRenderFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRenderFrame\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_reset_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRenderFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRenderFrame\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/highway_env/envs/common/abstract.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefine_spaces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Second, to link the obs and actions to the vehicles once the scene is created\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"human\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/highway_env/envs/common/observation.py\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m             )\n\u001b[1;32m    269\u001b[0m         \u001b[0;31m# Reorder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"shuffled\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4115\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4117\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_single_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   4151\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4152\u001b[0m         \"\"\"\n\u001b[0;32m-> 4153\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4154\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   4131\u001b[0m             )\n\u001b[1;32m   4132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4133\u001b[0;31m         new_data = self._mgr.take(\n\u001b[0m\u001b[1;32m   4134\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4135\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         return self.reindex_indexer(\n\u001b[0m\u001b[1;32m    895\u001b[0m             \u001b[0mnew_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m             \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             new_blocks = self._slice_take_blocks_ax0(\n\u001b[0m\u001b[1;32m    681\u001b[0m                 \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m                 \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_slice_take_blocks_ax0\u001b[0;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         sl_type, slobj, sllen = _preprocess_slice_or_indexer(\n\u001b[0;32m--> 741\u001b[0;31m             \u001b[0mslice_or_indexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m         )\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/base.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mShape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/base.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mShape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}