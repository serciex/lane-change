{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serciex/lane-change/blob/main/replication_of_rl_lateral_controller_for_autonomous_vehicles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAqx0bsIEQbi"
      },
      "source": [
        "Installing Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AAc_-AeGEE5H",
        "outputId": "fbdbb886-34bd-451f-f8ae-dfd1b4392917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: highway-env in /usr/local/lib/python3.11/dist-packages (1.10.1)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a2 in /usr/local/lib/python3.11/dist-packages (from highway-env) (1.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from highway-env) (0.0.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from highway-env) (2.0.2)\n",
            "Requirement already satisfied: pygame>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from highway-env) (2.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from highway-env) (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from highway-env) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from highway-env) (1.15.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a2->highway-env) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a2->highway-env) (4.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->highway-env) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->highway-env) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->highway-env) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pygame\n",
        "!pip install highway-env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Civ2P__KEUSq"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "-ZwGd9eVEWGY"
      },
      "outputs": [],
      "source": [
        "#Import Libraries\n",
        "from os import truncate\n",
        "import math\n",
        "import gymnasium\n",
        "import highway_env\n",
        "from matplotlib import pyplot as plt\n",
        "import pygame\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from IPython.display import Image, display\n",
        "from collections import deque\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF-2bRYTIjH6"
      },
      "source": [
        "Environment Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y922XK-cIewr",
        "outputId": "a567c136-ede6-4cb9-be20-ee61cad0534d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<highway_env.envs.common.action.ContinuousAction at 0x79ad077e48d0>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "# Configure Environment Conditions\n",
        "config = {\n",
        "    \"lanes_count\": 3,\n",
        "    \"lane_width\": 3.75,\n",
        "    \"observation\": {\n",
        "        \"type\": \"Kinematics\",\n",
        "        \"features\": [\"x\", \"y\", \"vx\", \"vy\", \"heading\", \"lat_off\"]\n",
        "    },\n",
        "    \"action\": {\"type\": \"ContinuousAction\"},\"ego_spawn_random\": True,\n",
        "    \"policy_frequency\": 10,\n",
        "}\n",
        "env = gymnasium.make('highway-v0', render_mode='rgb_array', config=config)\n",
        "frames = []\n",
        "\n",
        "# Action Setup\n",
        "highway_env.envs.common.action.ContinuousAction(env, lateral=True,\n",
        "                                                longitudinal=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "2HbWr4urTXQ8"
      },
      "outputs": [],
      "source": [
        "# Environment Manager\n",
        "class ENV(env.__class__):\n",
        "  \"\"\"\n",
        "  s = (v,a,x,y,thetha,id,w,c) âˆˆ S\n",
        "\n",
        "  Lateral Agent State:\n",
        "  Obs Data:\n",
        "  x = vehicle x position (x)\n",
        "  y = vehicle y position (y)\n",
        "  v = vehicle speed (vx)\n",
        "  thetha = yaw angle (heading)\n",
        "\n",
        "  Input:\n",
        "  a = longitudinal acceleration (longitudinal_control)\n",
        "  id = target lane id\n",
        "  w = lane width\n",
        "  c = road curvature\n",
        "\n",
        "  Extra Data:\n",
        "  vy = lateral rate (vy)\n",
        "  delta_lat_deviation = change in lateral deviation (lat_off)\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, obs,a):\n",
        "    self.obs = obs\n",
        "    self.a = a\n",
        "\n",
        "  def ego_state_idm(self):\n",
        "      ax, ay, avx, avy, athetha, _ = self.obs[0]\n",
        "      self._ego_state_idm = {\"x\": ax, \"y\": ay, \"vx\": avx,\"thetha\": athetha,\n",
        "                         \"longitudinal_acceleration\":self.a}\n",
        "\n",
        "      return self._ego_state_idm\n",
        "\n",
        "  def ego_state_agent(self,target_id):\n",
        "      ax, ay, avx, avy, athetha, _ = self.obs[0]\n",
        "      vehicle = env.unwrapped.vehicle\n",
        "\n",
        "      # Control Parameters\n",
        "      self.id = target_id\n",
        "\n",
        "      # Environment Parameters\n",
        "      self.s,_ = vehicle.lane.local_coordinates(vehicle.position)\n",
        "      self.w = vehicle.lane.width\n",
        "      self.c = vehicle.lane.heading_at(np.clip(\n",
        "          vehicle.lane.local_coordinates(vehicle.position)[0],\n",
        "          0, vehicle.lane.length))\n",
        "      self.v = math.sqrt(avx**2+avy**2)\n",
        "\n",
        "      self._ego_state_agent = {\"x\": ax, \"y\": ay, \"vx\": self.v,\"thetha\": athetha,\n",
        "                         \"lane_width\":self.w,\"lane_id\":self.id,\n",
        "                         \"self_curvature\":self.c,\n",
        "                         \"longitudinal_acceleration\":self.a}\n",
        "\n",
        "      return self._ego_state_agent\n",
        "\n",
        "  def longitudinal_lead_state(self):\n",
        "    # Lead State Parameters on the same Lane (5 columns: presence, x, y, vx, vy)\n",
        "    ego_vehicle = env.unwrapped.vehicle\n",
        "    lead_vehicle = env.unwrapped.road.neighbour_vehicles(ego_vehicle, lane_index=ego_vehicle.lane_index)[0]\n",
        "    if lead_vehicle:\n",
        "      gap = lead_vehicle.position[0] - ego_vehicle.position[0]\n",
        "      delta_velocity = ego_vehicle.velocity[0] - lead_vehicle.velocity[0]\n",
        "      self.longitudinal_lead_state = {\"x\": gap, \"vx\": delta_velocity}\n",
        "\n",
        "    else:\n",
        "      self.longitudinal_lead_state = {\"x\": 10, \"vx\": 0}\n",
        "\n",
        "    return self.longitudinal_lead_state\n",
        "\n",
        "  #Reward Function\n",
        "  def reward_function(self, obs_old, obs_new, target_id, w1=1, w2=1, w3=0.05):\n",
        "    \"\"\"\n",
        "    Reward Function:\n",
        "\n",
        "    Acceleration Reward: r_acce = w1*f_acce(a_yaw)\n",
        "    a_yaw = lateral acceleration (self.action)\n",
        "\n",
        "    Rate Reward: r_rate = w2*f_rate(w_yaw)\n",
        "    w_yaw = lateral rate (vy)\n",
        "\n",
        "    Time Reward: r_time = w3*f_time (delta_lat_deviation)\n",
        "    delta_lat_deviation = change in lateral deviation (self.lat_off)\n",
        "\n",
        "    Reward = Cummulative Sum of r_acce + Cummulative Sum of r_rate + Cummulative Sum of r_time\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    self.target_id = (\"0\",\"1\",target_id)\n",
        "    target_lane_object = env.unwrapped.road.network.get_lane(self.target_id)\n",
        "    vehicle_s, _ = env.unwrapped.vehicle.lane.local_coordinates(env.unwrapped.vehicle.position)\n",
        "    _ , self.delta_lat_deviaton = target_lane_object.local_coordinates(env.unwrapped.vehicle.position)\n",
        "\n",
        "    obs = obs_new[0]\n",
        "    obs_old = obs_old[0]\n",
        "\n",
        "    w_yaw = (obs[0] * obs[3] - obs[1] * obs[2]) / (obs[0]**2 + obs[1]**2 + 1e-8)\n",
        "    w_yaw_old = (obs_old[0] * obs_old[3] - obs_old[1] * obs_old[2]) / (obs_old[0]**2 + obs_old[1]**2 + 1e-8)\n",
        "\n",
        "    self.w_acce = (w_yaw-w_yaw_old)*env.unwrapped.config['policy_frequency']\n",
        "\n",
        "    # Acceleration Reward\n",
        "    acce_reward = -1*abs(self.w_acce)\n",
        "\n",
        "    # Rate Reward\n",
        "    rate_reward = -1*abs(w_yaw)\n",
        "\n",
        "    # Time Reward\n",
        "    time_reward = -0.05 * abs(self.delta_lat_deviaton)\n",
        "\n",
        "    # Overall Reward\n",
        "    self.reward = w1*acce_reward + w2*rate_reward + w3*time_reward\n",
        "\n",
        "    return [self.reward, acce_reward, rate_reward, time_reward]\n",
        "\n",
        "  #Acceleration to Steering angle\n",
        "  def steering_angle(self, agent_action,L=1):\n",
        "    \"\"\"\n",
        "    Steering Angle: theta = atan(a_yaw/v^2)\n",
        "    a_yaw = lateral acceleration (agent_action)\n",
        "    v = vehicle speed (vx)\n",
        "    \"\"\"\n",
        "    self.angle = math.atan(L*agent_action/self.ego_state_idm()['vx']**2)\n",
        "\n",
        "    return self.angle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEPNdVET6-mv"
      },
      "source": [
        "Agent Defintion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "SJ68SiyJTSx7"
      },
      "outputs": [],
      "source": [
        "class Agent(nn.Module):\n",
        "  \"\"\"\n",
        "  s = (x, y, vx, vy, thetha, lane_width, lane_id, self_curvature, longitudinal_acceleration) âˆˆ S\n",
        "\n",
        "  Lateral Agent State:\n",
        "  x = vehicle x position\n",
        "  y = vehicle y position\n",
        "  vx = vehicle speed (longitudinal)\n",
        "  thetha = yaw angle (heading)\n",
        "  lane_width = width of the lane\n",
        "  lane_id = target lane id\n",
        "  self_curvature = road curvature at the vehicle's position\n",
        "  longitudinal_acceleration = vehicle longitudinal acceleration\n",
        "  \"\"\"\n",
        "  def __init__(self, state_dim):\n",
        "    super(Agent, self).__init__()\n",
        "    self.state_dim = state_dim\n",
        "    # Define Network A & C\n",
        "    self.networkA = nn.Sequential(\n",
        "        nn.Linear(self.state_dim, 100),\n",
        "        nn.Linear(100, 1),\n",
        "        nn.Softplus()\n",
        "    )\n",
        "    self.networkC = nn.Sequential(\n",
        "        nn.Linear(self.state_dim + 1, 100),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(100, 1),\n",
        "    )\n",
        "\n",
        "    # Define Network B\n",
        "    self.networkB1 = nn.Sequential(\n",
        "        nn.Linear(self.state_dim, 150),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(150, 1),\n",
        "    )\n",
        "    self.networkB2 = nn.Sequential(\n",
        "        nn.Linear(self.state_dim, 150),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(150, 1),\n",
        "    )\n",
        "    self.networkB3 = nn.Sequential(\n",
        "        nn.Linear(self.state_dim, 150),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(150, 1),\n",
        "    )\n",
        "\n",
        "  def forward(self, state, a, terminal):\n",
        "    \"\"\"\n",
        "    Q(s,a) = A(s)*(B(s)-a)^2 + C(s)\n",
        "    \"\"\"\n",
        "    if isinstance(state, dict):\n",
        "      state_tensor = torch.tensor(\n",
        "          [state[key] for key in ['vx', 'longitudinal_acceleration', 'x', 'y', 'thetha', 'lane_id', 'lane_width', 'self_curvature']],\n",
        "          dtype=torch.float32\n",
        "      ).unsqueeze(0)\n",
        "    else:\n",
        "        self.state = state\n",
        "    # Use a directly if it's already a tensor\n",
        "    self.a = a if isinstance(a, torch.Tensor) else torch.tensor([[a]], dtype=torch.float32)\n",
        "    self.terminal_condition = terminal if isinstance(terminal, torch.Tensor) else torch.tensor([[terminal]], dtype=torch.float32)\n",
        "\n",
        "    # Output of the Networks\n",
        "    self.A = self.networkA(self.state)\n",
        "    self.C = self.networkC(torch.concat((self.state, self.terminal_condition), dim=1))\n",
        "    self.B = torch.max(self.networkB1(self.state) * self.networkB2(self.state),\n",
        "                         self.networkB3(self.state))\n",
        "\n",
        "    # Q-function Approximation\n",
        "    q_value = -self.A * ((self.B - self.a) ** 2) + self.C\n",
        "\n",
        "    return q_value\n",
        "\n",
        "  def action(self, state):\n",
        "      # Convert it to a tensor\n",
        "      if isinstance(state, dict):\n",
        "          state_tensor = torch.tensor(\n",
        "              [state[key] for key in ['vx', 'longitudinal_acceleration', 'x', 'y', 'thetha', 'lane_id', 'lane_width', 'self_curvature']],\n",
        "              dtype=torch.float32\n",
        "          ).unsqueeze(0)\n",
        "      else:\n",
        "          state_tensor = state\n",
        "      self.state = state_tensor\n",
        "      self.B = torch.max(self.networkB1(self.state) * self.networkB2(self.state),\n",
        "                         self.networkB3(self.state))\n",
        "      return self.B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo-7aVU9J681"
      },
      "source": [
        "Experience Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "7g2c9HRSJ8Xs"
      },
      "outputs": [],
      "source": [
        "from collections import deque # Ensure imported\n",
        "\n",
        "class Experience_Buffer():\n",
        "    \"\"\"\n",
        "    Stores transitions as tuples of NumPy arrays (state_vec, action_vec, reward_vec, next_state_vec, terminal_vec).\n",
        "    \"\"\"\n",
        "    def __init__(self, buffer_size):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.buffer = deque(maxlen=self.buffer_size)\n",
        "\n",
        "    def add(self, state_vec, action, reward, next_state_vec, terminal_condition):\n",
        "        state_np = np.asarray(state_vec, dtype=np.float32)\n",
        "        next_state_np = np.asarray(next_state_vec, dtype=np.float32)\n",
        "        action_np = np.asarray([action], dtype=np.float32)\n",
        "        reward_np = np.asarray([reward], dtype=np.float32)\n",
        "        terminal_np = np.asarray([float(terminal_condition)], dtype=np.float32)\n",
        "\n",
        "        self.transition = (state_np, action_np, reward_np, next_state_np, terminal_np)\n",
        "        self.buffer.append(self.transition)\n",
        "\n",
        "    def sample_experience(self, batch_size, device):\n",
        "        \"\"\" Samples batch and returns Tensors. \"\"\"\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, terminals = zip(*batch)\n",
        "\n",
        "        states_np = np.stack(states)\n",
        "        actions_np = np.stack(actions)\n",
        "        rewards_np = np.stack(rewards)\n",
        "        next_states_np = np.stack(next_states)\n",
        "        terminals_np = np.stack(terminals)\n",
        "\n",
        "        # Convert final batch arrays to Tensors\n",
        "        states_tensor = torch.tensor(states_np, dtype=torch.float32, device=device)\n",
        "        actions_tensor = torch.tensor(actions_np, dtype=torch.float32, device=device)\n",
        "        rewards_tensor = torch.tensor(rewards_np, dtype=torch.float32, device=device)\n",
        "        next_states_tensor = torch.tensor(next_states_np, dtype=torch.float32, device=device)\n",
        "        terminals_tensor = torch.tensor(terminals_np, dtype=torch.float32, device=device)\n",
        "\n",
        "        return states_tensor, actions_tensor, rewards_tensor, next_states_tensor, terminals_tensor\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enKF1jUjQxpL"
      },
      "source": [
        "Lateral Controller (Gap Checker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "ydU55ZY5eX4U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Gap_Controller(env.__class__):\n",
        "    def __init__(self, obs=None, following_gap_threshold=10.0):\n",
        "        # Optionally store an initial observation if provided.\n",
        "        if obs is not None:\n",
        "            self.obs = obs\n",
        "        # Threshold for safety gap with the following vehicle\n",
        "        self.following_gap_threshold = following_gap_threshold\n",
        "\n",
        "    def lane_checker(self):\n",
        "        \"\"\"\n",
        "        Determines which adjacent lane offers the safest gap for a lane change.\n",
        "        It examines the current lane, the lane to the left, and the lane to the right,\n",
        "        returning the lane index that has the largest safe front gap available.\n",
        "        In addition, if the gap with the following vehicle in the target lane is below the safety\n",
        "        threshold, the lane change is aborted and the current lane is chosen.\n",
        "\n",
        "        Returns:\n",
        "            int: The target lane index (the lane with the largest safe gap, or the current lane)\n",
        "                 if the candidate lane's following vehicle gap is unsafe.\n",
        "        \"\"\"\n",
        "        lane_number = len(env.unwrapped.road.network.lanes_list())\n",
        "        ego = env.unwrapped.vehicle\n",
        "\n",
        "        # Initialize front gap values with a default that indicates an unsafe or non-existent gap.\n",
        "        gap_current_front = -float('inf')\n",
        "        gap_right_front = -float('inf')\n",
        "        gap_left_front = -float('inf')\n",
        "\n",
        "        # Initialize following gap values (gap between ego and the vehicle behind)\n",
        "        gap_current_follower = float('inf')\n",
        "        gap_right_follower = float('inf')\n",
        "        gap_left_follower = float('inf')\n",
        "\n",
        "        # Get vehicle neighbours in current lane:\n",
        "        neighbours_current = env.unwrapped.road.neighbour_vehicles(ego, ego.lane_index)\n",
        "        # neighbours_current[0] is the vehicle ahead and [1] is the following vehicle.\n",
        "        if neighbours_current:\n",
        "            if neighbours_current[0]:\n",
        "                gap_current_front = neighbours_current[0].position[0] - ego.position[0]\n",
        "            if len(neighbours_current) > 1 and neighbours_current[1]:\n",
        "                gap_current_follower = ego.position[0] - neighbours_current[1].position[0]\n",
        "\n",
        "        # Compute the left and right lane indices.\n",
        "        current_lane = list(ego.lane_index)\n",
        "        if current_lane[2] > 0:\n",
        "            lane_left = (current_lane[0], current_lane[1], current_lane[2] - 1)\n",
        "        else:\n",
        "            lane_left = ego.lane_index\n",
        "\n",
        "        if current_lane[2] < (lane_number - 1):\n",
        "            lane_right = (current_lane[0], current_lane[1], current_lane[2] + 1)\n",
        "        else:\n",
        "            lane_right = ego.lane_index\n",
        "\n",
        "        # Retrieve neighbour vehicles for the right lane.\n",
        "        neighbours_right = env.unwrapped.road.neighbour_vehicles(ego, lane_index=lane_right)\n",
        "        if neighbours_right:\n",
        "            if neighbours_right[0]:\n",
        "                gap_right_front = neighbours_right[0].position[0] - ego.position[0]\n",
        "            if len(neighbours_right) > 1 and neighbours_right[1]:\n",
        "                gap_right_follower = ego.position[0] - neighbours_right[1].position[0]\n",
        "\n",
        "        # Retrieve neighbour vehicles for the left lane.\n",
        "        neighbours_left = env.unwrapped.road.neighbour_vehicles(ego, lane_index=lane_left)\n",
        "        if neighbours_left:\n",
        "            if neighbours_left[0]:\n",
        "                gap_left_front = neighbours_left[0].position[0] - ego.position[0]\n",
        "            if len(neighbours_left) > 1 and neighbours_left[1]:\n",
        "                gap_left_follower = ego.position[0] - neighbours_left[1].position[0]\n",
        "\n",
        "        # Compare the front gaps: current, right, and left.\n",
        "        front_gaps = [gap_current_front, gap_right_front, gap_left_front]\n",
        "        best_index = np.argmax(front_gaps)\n",
        "\n",
        "        # Determine the target lane index based on the best candidate.\n",
        "        # best_index: 0 => current lane, 1 => right lane, 2 => left lane.\n",
        "        if best_index == 1:\n",
        "            candidate_lane = lane_right\n",
        "            candidate_follower_gap = gap_right_follower\n",
        "        elif best_index == 2:\n",
        "            candidate_lane = lane_left\n",
        "            candidate_follower_gap = gap_left_follower\n",
        "        else:\n",
        "            candidate_lane = ego.lane_index\n",
        "            candidate_follower_gap = gap_current_follower  # in current lane, we don't enforce follower gap condition\n",
        "\n",
        "        # Check if the candidate lane (if different from the current lane)\n",
        "        # meets the follower gap condition.\n",
        "        if candidate_lane != ego.lane_index:\n",
        "            if candidate_follower_gap < self.following_gap_threshold:\n",
        "                # The follower gap is too small; do not change lanes.\n",
        "                target_lane_id = ego.lane_index[2]\n",
        "            else:\n",
        "                target_lane_id = candidate_lane[2]\n",
        "        else:\n",
        "            target_lane_id = ego.lane_index[2]\n",
        "\n",
        "        return target_lane_id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDNPakyp7APE"
      },
      "source": [
        "Longitudinal Controller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "jU_TDnfv7CzO"
      },
      "outputs": [],
      "source": [
        "class IDM():\n",
        "    ''' Intelligent Driving Model for Longitudinal Control\n",
        "\n",
        "    Control parameters:\n",
        "      a: maximum acceleration\n",
        "      b: comfortable deceleration\n",
        "      delta: acceleration exponent\n",
        "      T: safe time headway\n",
        "\n",
        "    Parameters:\n",
        "      s0: minimum gap\n",
        "      v0: desired speed\n",
        "\n",
        "      a (Maximum Acceleration): How fast the vehicle can speed up (m/sÂ²).\n",
        "      b (Comfortable Deceleration): How smoothly the vehicle slows down (m/sÂ²).\n",
        "      Î´ (Acceleration Exponent): The non-linearity factor in acceleration.\n",
        "      T (Safe Time Headway): The desired minimum following time gap (s).\n",
        "\n",
        "    Input variables:\n",
        "      s: current gap\n",
        "      v: current vehicle speed\n",
        "      delta_v: relative speed (difference between the vehicle's speed and the leading vehicle's speed)\n",
        "    '''\n",
        "\n",
        "    def __init__(self, desired_parameters, control_parameters):\n",
        "        # Unpack initial parameters: [s0, v0]\n",
        "        self.s0, self.v0 = desired_parameters\n",
        "        # Unpack control parameters: [a, b, delta, T]\n",
        "        self.a, self.b, self.delta, self.T = control_parameters\n",
        "\n",
        "    def longitudinal_controller(self, input_variables):\n",
        "        # Unpack input variables: [s, v, delta_v]\n",
        "        s, v, delta_v = input_variables\n",
        "\n",
        "        # Small epsilon to account for very small gaps and avoid division by zero\n",
        "        epsilon = 1e-6\n",
        "\n",
        "        # Desired gap: s* = s0 + v*T + (v * delta_v) / (2 * sqrt(a * b))\n",
        "        desired_gap = self.s0 + max(0, v * self.T + ((v * delta_v) / (2 * math.sqrt(self.a * self.b))))\n",
        "\n",
        "        # IDM acceleration: a_IDM = a * [ 1 - (v / v0)^delta - (s* / s)^2 ]\n",
        "        acceleration = self.a * (1 - (v / self.v0)**self.delta - (desired_gap / (s + epsilon))**2)\n",
        "\n",
        "        return acceleration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YusJR5WjFkxa"
      },
      "source": [
        "State Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "f2V_9AwIFlI3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# --- Define these globally or pass them to the normalizer ---\n",
        "# MUST match the order expected by the Agent input and feature extraction\n",
        "STATE_KEYS_ORDER = ['vx', 'longitudinal_acceleration', 'x', 'y', 'thetha',\n",
        "                    'lane_id', 'lane_width', 'self_curvature']\n",
        "STATE_DIM = len(STATE_KEYS_ORDER)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "class StateNormalizer:\n",
        "    \"\"\"\n",
        "    Computes running mean and standard deviation for state normalization (Z-score).\n",
        "    Uses Welford's online algorithm for numerical stability.\n",
        "    Includes methods to handle dictionary inputs.\n",
        "    \"\"\"\n",
        "    def __init__(self, size=STATE_DIM, epsilon=1e-5, clip_range=5.0, device='cpu'):\n",
        "        \"\"\"\n",
        "        Initializes the normalizer.\n",
        "\n",
        "        Args:\n",
        "            size (int): The dimension of the state vector (should match STATE_DIM).\n",
        "            epsilon (float): Small value to prevent division by zero in std dev.\n",
        "            clip_range (float): Range [-clip_range, clip_range] to clip normalized states.\n",
        "            device (str or torch.device): The device to put output tensors on.\n",
        "        \"\"\"\n",
        "        if size != STATE_DIM:\n",
        "            raise ValueError(f\"Normalizer size ({size}) does not match expected STATE_DIM ({STATE_DIM})\")\n",
        "        self.size = size\n",
        "        self.epsilon = epsilon\n",
        "        self.clip_range = clip_range\n",
        "        self.device = torch.device(device) # Store device\n",
        "\n",
        "        # Welford's algorithm variables\n",
        "        self.count = 0\n",
        "        self.mean = np.zeros(self.size, dtype=np.float32)\n",
        "        self.M2 = np.zeros(self.size, dtype=np.float32)\n",
        "\n",
        "    def _state_dict_to_vector(self, state_dict):\n",
        "        \"\"\"Converts the state dictionary to a NumPy vector.\"\"\"\n",
        "        # Internal helper, could also be a static method or defined outside\n",
        "        if state_dict is None:\n",
        "            # Return zeros or raise error? Let's return zeros for now.\n",
        "            print(\"Warning: Received None state_dict, returning zero vector.\")\n",
        "            return np.zeros(self.size, dtype=np.float32)\n",
        "        try:\n",
        "            # Ensure all keys exist before list comprehension\n",
        "            for key in STATE_KEYS_ORDER:\n",
        "                if key not in state_dict:\n",
        "                    raise KeyError(f\"Missing key '{key}'\")\n",
        "            state_vector = np.array([state_dict[key] for key in STATE_KEYS_ORDER], dtype=np.float32)\n",
        "            return state_vector\n",
        "        except KeyError as e:\n",
        "            raise KeyError(f\"Missing key {e} in state dictionary for normalization. Available keys: {list(state_dict.keys())}\")\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Error converting state dict to vector: {e}\")\n",
        "\n",
        "    def update_from_dict(self, state_dict):\n",
        "        \"\"\"\n",
        "        Updates the running mean and M2 using a new state dictionary.\n",
        "\n",
        "        Args:\n",
        "            state_dict (dict): A single state dictionary.\n",
        "        \"\"\"\n",
        "        state_vector = self._state_dict_to_vector(state_dict)\n",
        "        # Now use the existing update logic with the vector\n",
        "        self.count += 1\n",
        "        delta = state_vector - self.mean\n",
        "        self.mean += delta / self.count\n",
        "        delta2 = state_vector - self.mean\n",
        "        self.M2 += delta * delta2\n",
        "\n",
        "    def _get_stddev(self):\n",
        "        \"\"\"Calculates the current standard deviation.\"\"\"\n",
        "        if self.count < 2:\n",
        "            return np.ones(self.size, dtype=np.float32)\n",
        "        variance = self.M2 / (self.count -1) # Sample variance\n",
        "        stddev = np.sqrt(np.maximum(variance, self.epsilon**2)) # Use epsilon^2 with sqrt\n",
        "        # Ensure stddev is not exactly zero\n",
        "        stddev = np.maximum(stddev, self.epsilon)\n",
        "        return stddev\n",
        "\n",
        "    def normalize_dict_to_tensor(self, state_dict):\n",
        "        \"\"\"\n",
        "        Converts a state dictionary to a NumPy vector, normalizes it,\n",
        "        clips it, and returns it as a Tensor on the specified device.\n",
        "        Does NOT update running statistics.\n",
        "\n",
        "        Args:\n",
        "            state_dict (dict): The state dictionary to normalize.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The normalized state as a Tensor [1, state_dim].\n",
        "        \"\"\"\n",
        "        state_vector = self._state_dict_to_vector(state_dict)\n",
        "\n",
        "        mean_tensor = torch.tensor(self.mean, dtype=torch.float32, device=self.device)\n",
        "        stddev_tensor = torch.tensor(self._get_stddev(), dtype=torch.float32, device=self.device)\n",
        "        state_tensor = torch.tensor(state_vector, dtype=torch.float32, device=self.device)\n",
        "\n",
        "        # Normalize: (state - mean) / stddev\n",
        "        normalized_tensor = (state_tensor - mean_tensor) / stddev_tensor\n",
        "\n",
        "        # Clip and add batch dimension\n",
        "        clipped_tensor = torch.clamp(normalized_tensor, -self.clip_range, self.clip_range).unsqueeze(0)\n",
        "\n",
        "        return clipped_tensor\n",
        "\n",
        "    def normalize_batch_tensor(self, state_batch_tensor):\n",
        "        \"\"\"\n",
        "        Normalizes a batch of state vectors already in Tensor format.\n",
        "        Used during learning update after sampling from buffer.\n",
        "        Does NOT update running statistics.\n",
        "\n",
        "        Args:\n",
        "            state_batch_tensor (torch.Tensor): Batch of state vectors [batch_size, state_dim].\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The normalized batch of states, clipped.\n",
        "        \"\"\"\n",
        "        if not isinstance(state_batch_tensor, torch.Tensor):\n",
        "             state_batch_tensor = torch.tensor(state_batch_tensor, dtype=torch.float32, device=self.device)\n",
        "\n",
        "        mean_tensor = torch.tensor(self.mean, dtype=torch.float32, device=self.device)\n",
        "        stddev_tensor = torch.tensor(self._get_stddev(), dtype=torch.float32, device=self.device)\n",
        "\n",
        "        # Ensure tensors are on the same device as the input batch\n",
        "        mean_tensor = mean_tensor.to(state_batch_tensor.device)\n",
        "        stddev_tensor = stddev_tensor.to(state_batch_tensor.device)\n",
        "\n",
        "\n",
        "        # Normalize: (batch_state - mean) / stddev (broadcasting applies)\n",
        "        normalized_batch = (state_batch_tensor - mean_tensor) / stddev_tensor\n",
        "\n",
        "        # Clip\n",
        "        clipped_batch = torch.clamp(normalized_batch, -self.clip_range, self.clip_range)\n",
        "\n",
        "        return clipped_batch\n",
        "\n",
        "    # --- Optional: Load/Save ---\n",
        "    def get_state(self):\n",
        "        return {'count': self.count, 'mean': self.mean.tolist(), 'M2': self.M2.tolist()} # Save as lists\n",
        "\n",
        "    def set_state(self, state_dict):\n",
        "        self.count = state_dict['count']\n",
        "        self.mean = np.array(state_dict['mean'], dtype=np.float32)\n",
        "        self.M2 = np.array(state_dict['M2'], dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21gxd9D8au1O"
      },
      "source": [
        "Initialize Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "gPl1lptqKdAy"
      },
      "outputs": [],
      "source": [
        "# Initialize Environment\n",
        "obs, _ = env.reset()\n",
        "\n",
        "# Initialize Environment Manager and Reward\n",
        "state_manager = ENV(obs,obs[0][2])\n",
        "\n",
        "# State Manager for Ego and Lead State\n",
        "ego_state_idm = state_manager.ego_state_idm()\n",
        "lead_state = state_manager.longitudinal_lead_state()\n",
        "\n",
        "# Initial Longitundinal Positions\n",
        "ego_position_idm = ego_state_idm['x']\n",
        "lead_position_idm = lead_state['x']\n",
        "\n",
        "# Initial Velocities (using vx for longitudinal control)\n",
        "ego_velocity_idm = ego_state_idm['vx']\n",
        "lead_velocity_idm = lead_state['vx']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZvAgaQfIyK6"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-t_GJwAlEFI"
      },
      "source": [
        "CartPole (Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "prVGADshVUaU"
      },
      "outputs": [],
      "source": [
        "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ENV  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# env_name          = \"CartPole-v1\"\n",
        "# env               = gymnasium.make(env_name, render_mode=None)\n",
        "# state_dim         = env.observation_space.shape[0]      # 2  (pos, vel)\n",
        "# act_low, act_high = 0, env.action_space.n - 1\n",
        "# device            = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  HYPERPARAMETERS  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# gamma                 = 0.99\n",
        "# batch_size            = 100\n",
        "# num_episodes          = 5000\n",
        "# max_steps_per_episode = 999\n",
        "# lr                    = 1e-3\n",
        "# replay_buffer_size    = batch_size*1000\n",
        "# min_replay_size       = batch_size*100\n",
        "# plot_every            = 10\n",
        "\n",
        "# # --- Soft Update Parameters ---\n",
        "# tau                   = 0.005  # Target network soft update parameter\n",
        "\n",
        "# # --- Epsilon-Greedy Parameters ---\n",
        "# epsilon_start         = 1\n",
        "# epsilon_end           = 0.05\n",
        "# epsilon_decay_steps   = 20000\n",
        "\n",
        "# epsilon_decay_rate    = (epsilon_start - epsilon_end) / epsilon_decay_steps\n",
        "# epsilon               = epsilon_start\n",
        "\n",
        "# episode_returns = []\n",
        "# loss_history    = []\n",
        "# global_step     = 0\n",
        "\n",
        "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  NETWORKS  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# policy_net = Agent(state_dim).to(device)\n",
        "# target_net = Agent(state_dim).to(device)\n",
        "\n",
        "# target_net.load_state_dict(policy_net.state_dict())\n",
        "# target_net.eval()\n",
        "\n",
        "# optimizer  = optim.Adam(policy_net.parameters(), lr=lr)\n",
        "# loss_fn    = nn.MSELoss()\n",
        "\n",
        "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  REPLAY BUFFER  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# class ReplayBuf:\n",
        "#     def __init__(self, capacity):\n",
        "#         self.buf = deque(maxlen=capacity)\n",
        "\n",
        "#     def add(self, s, a, r, s2, d):\n",
        "#         s_tensor = s.squeeze(0).cpu()\n",
        "#         s2_tensor = s2.squeeze(0).cpu()\n",
        "#         self.buf.append((s_tensor, a, r, s2_tensor, d))\n",
        "\n",
        "#     def sample(self, n):\n",
        "#         if len(self.buf) < n: return None\n",
        "#         batch = random.sample(self.buf, n)\n",
        "\n",
        "#         s, a, r, s2, d = zip(*batch)\n",
        "#         s_batch = torch.stack(s).to(device)\n",
        "#         a_batch = torch.tensor(a, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "#         r_batch = torch.tensor(r, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "#         s2_batch = torch.stack(s2).to(device)\n",
        "#         d_batch = torch.tensor(d, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "#         return s_batch, a_batch, r_batch, s2_batch, d_batch\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.buf)\n",
        "\n",
        "# replay = ReplayBuf(replay_buffer_size)\n",
        "\n",
        "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SOFT UPDATE FUNCTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# def soft_update(target, source, tau):\n",
        "#     \"\"\"\n",
        "#     Perform soft update of target network parameters\n",
        "#     target = (1-tau)*target + tau*source\n",
        "#     \"\"\"\n",
        "#     for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
        "#         target_param.data.copy_(\n",
        "#             (1.0 - tau) * target_param.data + tau * source_param.data\n",
        "#         )\n",
        "\n",
        "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TRAINING LOOP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# for ep in range(1, num_episodes + 1):\n",
        "#     obs, _ = env.reset()\n",
        "#     state  = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "#     done   = False\n",
        "#     steps  = 0\n",
        "#     ep_ret = 0.0\n",
        "#     last_step_loss = None\n",
        "#     policy_net.train()\n",
        "\n",
        "#     while not done and steps < max_steps_per_episode:\n",
        "#         steps       += 1\n",
        "#         global_step += 1\n",
        "\n",
        "#         if random.random() < epsilon:\n",
        "#             action_val = env.action_space.sample()\n",
        "#         else:\n",
        "#             with torch.no_grad():\n",
        "#                 b_value = policy_net.action(state).item()\n",
        "#             action_val = 1/(1+np.exp(-b_value))\n",
        "#             action_val = 1 if action_val > 0.5 else 0\n",
        "\n",
        "#         epsilon = max(epsilon_end, epsilon - epsilon_decay_rate)\n",
        "\n",
        "#         obs2, reward, terminated, truncated, _ = env.step(action_val)\n",
        "#         done       = terminated or truncated\n",
        "#         ep_ret    += reward\n",
        "\n",
        "#         next_state = torch.tensor(obs2, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "#         replay.add(state, float(action_val), float(reward), next_state, float(done))\n",
        "#         state = next_state\n",
        "\n",
        "#         if len(replay) >= min_replay_size:\n",
        "#             sample = replay.sample(batch_size)\n",
        "#             if sample is None: continue\n",
        "#             s_b, a_b, r_b, s2_b, d_b = sample\n",
        "#             q_curr_b = policy_net(s_b, a_b, d_b)\n",
        "\n",
        "#             with torch.no_grad():\n",
        "#                 q_next_val = target_net.networkC(torch.concat((s2_b, d_b), dim=1))\n",
        "#                 q_target_b = r_b + gamma * q_next_val * (1 - d_b)\n",
        "\n",
        "#             loss = loss_fn(q_curr_b, q_target_b)\n",
        "#             last_step_loss = loss.item()\n",
        "#             loss_history.append(loss.item())\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             # Apply soft update to target network after each learning step\n",
        "#             soft_update(target_net, policy_net, tau)\n",
        "\n",
        "#         if done:\n",
        "#             break\n",
        "\n",
        "#     episode_returns.append(ep_ret)\n",
        "#     avg_return = np.mean(episode_returns[-100:])\n",
        "#     loss_str_repr = f\"{last_step_loss:.4f}\" if last_step_loss is not None else \"N/A\"\n",
        "\n",
        "#     print(f\"Ep {ep:4}/{num_episodes} â”‚ Steps:{steps:4} â”‚ Ep Return:{ep_ret:8.2f} \"\n",
        "#           f\"â”‚ Avg Return (100 ep):{avg_return:8.2f} â”‚ Epsilon:{epsilon:.3f} \"\n",
        "#           f\"â”‚ Buffer:{len(replay):6} â”‚ Loss:{loss_str_repr}\")\n",
        "\n",
        "# plt.plot(episode_returns)\n",
        "# plt.xlabel(\"Episode\")\n",
        "# plt.ylabel(\"Return\")\n",
        "# plt.title(\"Episode Returns over Time\")\n",
        "# plt.show()\n",
        "\n",
        "# plt.plot(loss_history)\n",
        "# plt.xlabel(\"Step\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.title(\"Training Loss over Time\")\n",
        "# plt.show()\n",
        "\n",
        "# env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhoITfrZ_nOt"
      },
      "source": [
        "Always Active (Option 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "WiV0AFmfkora"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from IPython.display import HTML\n",
        "from IPython.display import display\n",
        "from IPython import display as ipythondisplay\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYA-H09sIw2_",
        "outputId": "7b667ae2-c4c5-41e6-a1c4-05625471e487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training loop...\n",
            "Device: cpu, Buffer Size: 100000, Batch Size: 256\n",
            "Epsilon Start: 1.0, Epsilon End: 0.05, Decay Steps: 20000\n",
            "Learning Rate: 0.01, Gamma: 0.99\n",
            "SOFT UPDATES: Tau = 0.005, Update Frequency: 1 steps\n",
            "Max Timesteps: 40000\n",
            "Video Recording: Every 1000 steps\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "################################################################################\n",
        "\"\"\"Initialization\"\"\"\n",
        "################################################################################\n",
        "# IDM Initialization\n",
        "control_parameters = [0.1, 5, 4, 4]  # a, b, Î´, T\n",
        "desired_parameters = [20, 10.0]       # s0, v0\n",
        "\n",
        "# Set initial Input variables using the gap, current velocity, and relative lead velocity\n",
        "input_variables = [lead_position_idm, ego_velocity_idm, lead_velocity_idm]\n",
        "\n",
        "# Setup the IDM Model for Longitudinal control\n",
        "ego_vehicle_idm = IDM(desired_parameters, control_parameters)\n",
        "\n",
        "# Agent Initialization\n",
        "total_steps_taken = 0\n",
        "lr = 0.01  # Reduced learning rate\n",
        "gamma = 0.99  # Higher discount factor for better long-term rewards\n",
        "\n",
        "# Reasonable hyperparameter sizes\n",
        "buffer_size = int(1e5)  # 100k buffer\n",
        "batch_size = 256  # Standard batch size\n",
        "max_timesteps = int(4e4)  # 40k total timesteps\n",
        "\n",
        "# NEW: Soft update parameters\n",
        "tau = 0.005  # Small value for soft updates (typically between 0.001 and 0.1)\n",
        "update_target_freq_steps = 1  # Update softly every step\n",
        "\n",
        "plot_freq_steps = 5000  # Plot every 5000 timesteps\n",
        "video_freq_steps = 1000  # Record video every 5000 steps\n",
        "max_video_length = 500  # Maximum number of frames per video\n",
        "\n",
        "# Epsilon-Greedy Parameters\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.05\n",
        "epsilon_decay_steps = int(2e4)  # 20k steps for decay\n",
        "epsilon = epsilon_start\n",
        "\n",
        "# Tracking for performance\n",
        "timestep_rewards = []  # Store rewards by timestep\n",
        "timestep_acce_rewards = []  # Store acceleration rewards\n",
        "timestep_rate_rewards = []  # Store rate rewards\n",
        "timestep_time_rewards = []  # Store time rewards\n",
        "global_returns = []  # Cumulative return tracking\n",
        "loss_history = []  # Loss values\n",
        "timesteps_list = []  # Track timesteps for plotting\n",
        "episode_lengths = []  # Track episode lengths\n",
        "episode_indices = []  # Track episode indices for plotting\n",
        "\n",
        "target_id = env.unwrapped.vehicle.lane_index[-1]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Environment Information\n",
        "L = env.unwrapped.vehicle.LENGTH\n",
        "agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "state_dim = len(agent_state)\n",
        "\n",
        "# Initialize Buffer\n",
        "buffer = Experience_Buffer(buffer_size)\n",
        "\n",
        "# Initialize State Normalizer\n",
        "normalizer = StateNormalizer(size=state_dim, device=device)\n",
        "# Update the normalizer with initial state to start populating statistics\n",
        "normalizer.update_from_dict(agent_state)\n",
        "\n",
        "# Setup Lateral Control Agent\n",
        "target_network = Agent(state_dim).to(device)\n",
        "policy_network = Agent(state_dim).to(device)\n",
        "\n",
        "# Initialize target network\n",
        "target_network.load_state_dict(policy_network.state_dict())\n",
        "\n",
        "# Loss and Criterion Initialization\n",
        "policy_optimizer = torch.optim.Adam(policy_network.parameters(), lr=lr)\n",
        "policy_loss_fn = nn.MSELoss()\n",
        "\n",
        "# Create directories for saved data\n",
        "os.makedirs(\"training_plots\", exist_ok=True)\n",
        "os.makedirs(\"training_videos\", exist_ok=True)\n",
        "\n",
        "# NEW: Define a function for soft updates of target networks\n",
        "def soft_update(target, source, tau):\n",
        "    \"\"\"\n",
        "    Perform soft update on target network parameters\n",
        "    target = (1-tau)*target + tau*source\n",
        "    \"\"\"\n",
        "    for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
        "        target_param.data.copy_(target_param.data * (1.0 - tau) + source_param.data * tau)\n",
        "\n",
        "# Define a function to create a video only when needed during training\n",
        "class VideoRecorder:\n",
        "    def __init__(self, video_path, max_frames=500):\n",
        "        self.video_path = video_path\n",
        "        self.frames = []\n",
        "        self.max_frames = max_frames\n",
        "        self.recording = False\n",
        "        self.step_count = 0\n",
        "        self.episode_reward = 0.0\n",
        "\n",
        "    def start_recording(self):\n",
        "        \"\"\"Start recording frames\"\"\"\n",
        "        self.recording = True\n",
        "        self.frames = []\n",
        "        self.step_count = 0\n",
        "        self.episode_reward = 0.0\n",
        "        print(f\"Started recording to {self.video_path}\")\n",
        "\n",
        "    def add_frame(self, env, reward=0.0):\n",
        "        \"\"\"Add a frame to the video if recording is active\"\"\"\n",
        "        if not self.recording:\n",
        "            return\n",
        "\n",
        "        if self.step_count >= self.max_frames:\n",
        "            self.stop_recording()\n",
        "            return\n",
        "\n",
        "        # Render current frame and add to frame list\n",
        "        frame = env.render()\n",
        "        self.frames.append(frame)\n",
        "        self.episode_reward += reward\n",
        "        self.step_count += 1\n",
        "\n",
        "    def stop_recording(self):\n",
        "        \"\"\"Stop recording and save the video\"\"\"\n",
        "        if not self.recording or len(self.frames) == 0:\n",
        "            self.recording = False\n",
        "            return None, None\n",
        "\n",
        "        # Save video\n",
        "        imageio.mimsave(self.video_path, self.frames, fps=15)\n",
        "        self.recording = False\n",
        "\n",
        "        # Return stats\n",
        "        stats_text = f\"Steps: {self.step_count}, Reward: {self.episode_reward:.2f}\"\n",
        "\n",
        "        print(f\"Video saved to {self.video_path} - {stats_text}\")\n",
        "        return self.video_path, stats_text\n",
        "\n",
        "################################################################################\n",
        "\"\"\"Training & Testing Loop\"\"\"\n",
        "################################################################################\n",
        "print(\"Starting training loop...\")\n",
        "print(f\"Device: {device}, Buffer Size: {buffer_size}, Batch Size: {batch_size}\")\n",
        "print(f\"Epsilon Start: {epsilon_start}, Epsilon End: {epsilon_end}, Decay Steps: {epsilon_decay_steps}\")\n",
        "print(f\"Learning Rate: {lr}, Gamma: {gamma}\")\n",
        "print(f\"SOFT UPDATES: Tau = {tau}, Update Frequency: {update_target_freq_steps} steps\")\n",
        "print(f\"Max Timesteps: {max_timesteps}\")\n",
        "print(f\"Video Recording: Every {video_freq_steps} steps\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Track episode count\n",
        "episode_count = 0\n",
        "\n",
        "# Track average episode length for reporting\n",
        "avg_ep_length = 0\n",
        "\n",
        "# Track episode-specific rewards for calculating statistics\n",
        "episode_rewards_list = []\n",
        "\n",
        "# Initialize video recorder (but not recording yet)\n",
        "video_recorder = None\n",
        "\n",
        "# Continue until we reach the maximum number of timesteps\n",
        "while total_steps_taken < max_timesteps:\n",
        "    episode_count += 1\n",
        "\n",
        "    # Reset environment and state manager\n",
        "    obs, _ = env.reset()\n",
        "    state_manager = ENV(obs, 0.0)\n",
        "\n",
        "    # Initial state information\n",
        "    ego_state_idm = state_manager.ego_state_idm()\n",
        "    lead_state = state_manager.longitudinal_lead_state()\n",
        "    agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "\n",
        "    # Update normalizer with initial state\n",
        "    normalizer.update_from_dict(agent_state)\n",
        "\n",
        "    # Update IDM inputs based on initial state\n",
        "    gap = lead_state['x']\n",
        "    delta_velocity = lead_state['vx']\n",
        "    input_variables = [gap, ego_state_idm['vx'], delta_velocity]\n",
        "\n",
        "    episode_step = 0\n",
        "    episode_return = 0.0\n",
        "    done = False\n",
        "\n",
        "    should_record_this_episode = (\n",
        "        total_steps_taken // video_freq_steps <\n",
        "        (total_steps_taken + episode_step + 1) // video_freq_steps\n",
        "    )\n",
        "\n",
        "    if should_record_this_episode:\n",
        "        original_render_mode = env.unwrapped.config[\"render_mode\"]\n",
        "        env.unwrapped.config[\"render_mode\"] = \"rgb_array\"\n",
        "\n",
        "        # Create video recorder for this episode\n",
        "        video_path = f\"training_videos/agent_video_step_{total_steps_taken + 1}.mp4\"\n",
        "        video_recorder = VideoRecorder(video_path, max_frames=max_video_length)\n",
        "        video_recorder.start_recording()\n",
        "\n",
        "        # Record initial frame\n",
        "        video_recorder.add_frame(env)\n",
        "\n",
        "    # Track rewards for this episode specifically\n",
        "    current_episode_rewards = []\n",
        "\n",
        "    # Run until episode terminates or we hit max timesteps\n",
        "    while not done and total_steps_taken < max_timesteps:\n",
        "        episode_step += 1\n",
        "        total_steps_taken += 1\n",
        "        timesteps_list.append(total_steps_taken)\n",
        "\n",
        "        # Recompute gap\n",
        "        gap = lead_state['x']\n",
        "\n",
        "        ########################################################################\n",
        "        \"\"\"Action Preparation\"\"\"\n",
        "        ########################################################################\n",
        "        gap_control = Gap_Controller(obs, following_gap_threshold=30)\n",
        "        activated_target_lane = gap_control.lane_checker()\n",
        "\n",
        "        # Determine target lane\n",
        "        target_id = activated_target_lane\n",
        "        agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "\n",
        "        # Update the normalizer with the current state\n",
        "        normalizer.update_from_dict(agent_state)\n",
        "\n",
        "        # Epsilon-greedy action selection\n",
        "        if np.random.random() < epsilon:\n",
        "            # Exploration: choose random action\n",
        "            agent_action = np.random.uniform(-1.0, 1.0)\n",
        "        else:\n",
        "            # Exploitation: choose best action according to policy\n",
        "            with torch.no_grad():\n",
        "                state_tensor = normalizer.normalize_dict_to_tensor(agent_state)\n",
        "                agent_action = policy_network.action(state_tensor).item()\n",
        "\n",
        "        # Store prior state for buffer\n",
        "        old_state = agent_state\n",
        "        obs_old = obs\n",
        "\n",
        "        # IDM Longitudinal Control\n",
        "        idm_acceleration = ego_vehicle_idm.longitudinal_controller(input_variables)\n",
        "        longitudinal_control = idm_acceleration\n",
        "\n",
        "        # Transform agent action to steering angle\n",
        "        lateral_control = state_manager.steering_angle(agent_action, L)\n",
        "\n",
        "        # Combine longitudinal and lateral actions\n",
        "        action = [longitudinal_control, lateral_control]\n",
        "\n",
        "        ########################################################################\n",
        "        \"\"\"Data Collection\"\"\"\n",
        "        ########################################################################\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        done = done or truncated\n",
        "\n",
        "        # Update state manager with new observation and applied longitudinal control\n",
        "        state_manager = ENV(obs, longitudinal_control)\n",
        "\n",
        "        # Compute reward based on the new state\n",
        "        reward_components = state_manager.reward_function(obs_old, obs, target_id)\n",
        "        reward = reward_components[0]\n",
        "\n",
        "        # Record frame if we're recording this episode\n",
        "        if video_recorder and video_recorder.recording:\n",
        "            video_recorder.add_frame(env, reward)\n",
        "\n",
        "        # Add this reward to the current episode's rewards\n",
        "        current_episode_rewards.append(reward)\n",
        "\n",
        "        # Track rewards for each component\n",
        "        timestep_rewards.append(reward)\n",
        "        timestep_acce_rewards.append(reward_components[1])  # Acceleration reward\n",
        "        timestep_rate_rewards.append(reward_components[2])  # Rate reward\n",
        "        timestep_time_rewards.append(reward_components[3])  # Time reward\n",
        "\n",
        "        # Update episode return\n",
        "        episode_return += reward\n",
        "\n",
        "        ########################################################################\n",
        "        \"\"\"Update IDM Controller Inputs for next step\"\"\"\n",
        "        ########################################################################\n",
        "        ego_state_idm = state_manager.ego_state_idm()\n",
        "        lead_state = state_manager.longitudinal_lead_state()\n",
        "        agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "\n",
        "        # Update the normalizer with the new state\n",
        "        normalizer.update_from_dict(agent_state)\n",
        "\n",
        "        gap = lead_state['x']\n",
        "        delta_velocity = lead_state['vx']\n",
        "        input_variables = [gap, ego_state_idm['vx'], delta_velocity]\n",
        "\n",
        "        # Convert state dictionaries to vectors before adding to buffer\n",
        "        old_state_vec = normalizer._state_dict_to_vector(old_state)\n",
        "        new_state_vec = normalizer._state_dict_to_vector(agent_state)\n",
        "\n",
        "        # Update experience buffer with vector representations\n",
        "        terminal_flag = 1.0 if done else 0.0\n",
        "        buffer.add(old_state_vec, agent_action, reward, new_state_vec, terminal_flag)\n",
        "\n",
        "        ########################################################################\n",
        "        \"\"\"Q-Learning Update\"\"\"\n",
        "        ########################################################################\n",
        "        if buffer.size() >= batch_size:\n",
        "            if len(loss_history) == 0:\n",
        "                print(\"\\n\" + \"=\" * 50)\n",
        "                print(f\"Buffer filled with {buffer.size()} experiences. Starting Q-learning updates...\")\n",
        "                print(\"=\" * 50 + \"\\n\")\n",
        "\n",
        "            rand_experience = buffer.sample_experience(batch_size=batch_size, device=device)\n",
        "            states, actions_t, rewards_t, next_states, terminals_t = rand_experience\n",
        "\n",
        "            # Normalize states and next_states\n",
        "            states_normalized = normalizer.normalize_batch_tensor(states)\n",
        "            next_states_normalized = normalizer.normalize_batch_tensor(next_states)\n",
        "\n",
        "            # Use normalized states for Q-value computation\n",
        "            current_q_values = policy_network(states_normalized, actions_t, terminals_t)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                target_network.eval()\n",
        "\n",
        "                # Get optimal actions for next states\n",
        "                next_optimal_actions = target_network.action(next_states_normalized)\n",
        "\n",
        "                # Calculate full Q-values using optimal actions\n",
        "                next_q_values = target_network(\n",
        "                    next_states_normalized,\n",
        "                    next_optimal_actions,\n",
        "                    terminals_t\n",
        "                )\n",
        "\n",
        "                # Compute target values directly without reshaping\n",
        "                target_q_values = rewards_t + gamma * next_q_values * (1.0 - terminals_t)\n",
        "\n",
        "            loss = policy_loss_fn(current_q_values, target_q_values)\n",
        "            loss_history.append(loss.item())\n",
        "\n",
        "            policy_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            policy_optimizer.step()\n",
        "\n",
        "            if total_steps_taken % update_target_freq_steps == 0:\n",
        "                soft_update(target_network, policy_network, tau)\n",
        "\n",
        "                # Log soft update at milestone steps\n",
        "                if total_steps_taken % 5000 == 0:\n",
        "                    print(f\"**** Soft target network update at timestep {total_steps_taken} (tau={tau}) ****\")\n",
        "\n",
        "            # Update epsilon with decay\n",
        "            epsilon = max(\n",
        "                epsilon_end,\n",
        "                epsilon_start - (total_steps_taken * (epsilon_start - epsilon_end) / epsilon_decay_steps)\n",
        "            )\n",
        "\n",
        "            if total_steps_taken % 1000 == 0:\n",
        "                avg_loss = np.mean(loss_history[-100:]) if len(loss_history) >= 100 else np.mean(loss_history)\n",
        "                print(f\"Step: {total_steps_taken:5d}/{max_timesteps} | Recent Avg Loss: {avg_loss:.6f} | Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "        # Check if we should start recording at a specific step\n",
        "        if total_steps_taken % video_freq_steps == 0 and total_steps_taken > 0 and (not video_recorder or not video_recorder.recording):\n",
        "            print(f\"Recording evaluation video at step {total_steps_taken}...\")\n",
        "\n",
        "        # Plot results based on timesteps\n",
        "        if total_steps_taken % plot_freq_steps == 0 and total_steps_taken > 0:\n",
        "            # Create plot directory if it doesn't exist\n",
        "            plot_dir = \"training_plots\"\n",
        "            if not os.path.exists(plot_dir):\n",
        "                os.makedirs(plot_dir)\n",
        "\n",
        "            # Plot reward components\n",
        "            plt.figure(figsize=(12, 8))\n",
        "\n",
        "            # Plot total rewards\n",
        "            plt.subplot(2, 2, 1)\n",
        "            plt.plot(timesteps_list, timestep_rewards)\n",
        "            plt.xlabel(\"Timesteps\")\n",
        "            plt.ylabel(\"Total Reward\")\n",
        "            plt.title(\"Total Reward per Timestep\")\n",
        "\n",
        "            # Plot acceleration rewards\n",
        "            plt.subplot(2, 2, 2)\n",
        "            plt.plot(timesteps_list, timestep_acce_rewards)\n",
        "            plt.xlabel(\"Timesteps\")\n",
        "            plt.ylabel(\"Acceleration Reward\")\n",
        "            plt.title(\"Acceleration Reward per Timestep\")\n",
        "\n",
        "            # Plot rate rewards\n",
        "            plt.subplot(2, 2, 3)\n",
        "            plt.plot(timesteps_list, timestep_rate_rewards)\n",
        "            plt.xlabel(\"Timesteps\")\n",
        "            plt.ylabel(\"Rate Reward\")\n",
        "            plt.title(\"Rate Reward per Timestep\")\n",
        "\n",
        "            # Plot time rewards\n",
        "            plt.subplot(2, 2, 4)\n",
        "            plt.plot(timesteps_list, timestep_time_rewards)\n",
        "            plt.xlabel(\"Timesteps\")\n",
        "            plt.ylabel(\"Time Reward\")\n",
        "            plt.title(\"Time Reward per Timestep\")\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"{plot_dir}/rewards_timestep_{total_steps_taken}.png\")\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "\n",
        "            # Plot loss history if we have losses\n",
        "            if len(loss_history) > 0:\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                plt.plot(range(len(loss_history)), loss_history)\n",
        "                plt.xlabel(\"Q-Learning Updates\")\n",
        "                plt.ylabel(\"Loss\")\n",
        "                plt.title(\"Loss over Training Updates\")\n",
        "                plt.savefig(f\"{plot_dir}/loss_timestep_{total_steps_taken}.png\")\n",
        "                plt.show()\n",
        "                plt.close()\n",
        "\n",
        "            # Plot moving average of rewards (with window of 100)\n",
        "            if len(timestep_rewards) > 100:\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                window_size = 100\n",
        "                moving_avg = np.convolve(timestep_rewards, np.ones(window_size)/window_size, mode='valid')\n",
        "                plt.plot(range(len(moving_avg)), moving_avg)\n",
        "                plt.xlabel(\"Timesteps\")\n",
        "                plt.ylabel(f\"Average Reward (Window={window_size})\")\n",
        "                plt.title(f\"Moving Average Reward (Window={window_size})\")\n",
        "                plt.savefig(f\"{plot_dir}/moving_avg_reward_timestep_{total_steps_taken}.png\")\n",
        "                plt.show()\n",
        "                plt.close()\n",
        "\n",
        "            # Plot episode lengths if we have episodes\n",
        "            if len(episode_lengths) > 0:\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                plt.plot(episode_indices, episode_lengths)\n",
        "                plt.xlabel(\"Episode\")\n",
        "                plt.ylabel(\"Length (Steps)\")\n",
        "                plt.title(\"Episode Lengths\")\n",
        "                plt.savefig(f\"{plot_dir}/episode_lengths_{total_steps_taken}.png\")\n",
        "                plt.show()\n",
        "                plt.close()\n",
        "\n",
        "            print(f\"Plots saved at timestep {total_steps_taken}\")\n",
        "    # End of episode\n",
        "\n",
        "    # Finalize video recording if we were recording this episode\n",
        "    if video_recorder and video_recorder.recording:\n",
        "        video_path, stats = video_recorder.stop_recording()\n",
        "        # Reset rendering mode back to original\n",
        "        env.unwrapped.config[\"render_mode\"] = original_render_mode\n",
        "\n",
        "        # Display the video in the notebook if in interactive mode\n",
        "        try:\n",
        "            from IPython.display import Video\n",
        "            display(Video(video_path, embed=True, width=800))\n",
        "        except:\n",
        "            print(\"Video created but couldn't be displayed in the notebook.\")\n",
        "\n",
        "        # Clear the video recorder\n",
        "        video_recorder = None\n",
        "\n",
        "    global_returns.append(episode_return)\n",
        "    episode_lengths.append(episode_step)\n",
        "    episode_indices.append(episode_count)\n",
        "\n",
        "    # Store this episode's rewards\n",
        "    episode_rewards_list.append(current_episode_rewards)\n",
        "\n",
        "    # Calculate average episode length\n",
        "    avg_ep_length = np.mean(episode_lengths[-100:]) if len(episode_lengths) >= 100 else np.mean(episode_lengths)\n",
        "\n",
        "    # Get average return from recent episodes\n",
        "    avg_return = np.mean(global_returns[-100:]) if len(global_returns) >= 100 else np.mean(global_returns)\n",
        "\n",
        "    # Get most recent loss value\n",
        "    last_loss = loss_history[-1] if loss_history else \"N/A\"\n",
        "\n",
        "    # Calculate average and standard deviation of reward per timestep for this episode\n",
        "    avg_reward_per_step = np.mean(current_episode_rewards)\n",
        "    std_reward_per_step = np.std(current_episode_rewards)\n",
        "\n",
        "    # Print the progress report with enhanced statistics\n",
        "    print(f\"Ep {episode_count:3d} | Steps: {episode_step:4d} | Avg Ep Len: {avg_ep_length:.1f} | Total Steps: {total_steps_taken:5d}/{max_timesteps} | \"\n",
        "          f\"Ep Return: {episode_return:6.2f} | Avg Return: {avg_return:6.2f} | \"\n",
        "          f\"Reward/Step: {avg_reward_per_step:.3f}Â±{std_reward_per_step:.3f} | \"\n",
        "          f\"Epsilon: {epsilon:.3f} | Buffer: {buffer.size():5d} | Loss: {last_loss}\")\n",
        "\n",
        "# Record a final video after training is complete\n",
        "final_video_path = \"training_videos/final_agent_performance.mp4\"\n",
        "print(\"Recording final evaluation video...\")\n",
        "\n",
        "# Switch to rendering mode for final video\n",
        "original_render_mode = env.unwrapped.config[\"render_mode\"]\n",
        "env.unwrapped.config[\"render_mode\"] = \"rgb_array\"\n",
        "\n",
        "# Create recorder for final video\n",
        "final_recorder = VideoRecorder(final_video_path, max_frames=max_video_length)\n",
        "final_recorder.start_recording()\n",
        "\n",
        "# Reset environment for final video\n",
        "obs, _ = env.reset()\n",
        "state_manager = ENV(obs, 0.0)\n",
        "\n",
        "# Get initial state information\n",
        "ego_state_idm = state_manager.ego_state_idm()\n",
        "lead_state = state_manager.longitudinal_lead_state()\n",
        "gap = lead_state['x']\n",
        "delta_velocity = lead_state['vx']\n",
        "input_variables = [gap, ego_state_idm['vx'], delta_velocity]\n",
        "\n",
        "# Record initial frame\n",
        "final_recorder.add_frame(env)\n",
        "\n",
        "# Run until episode terminates or max length reached\n",
        "done = False\n",
        "while not done and final_recorder.step_count < max_video_length:\n",
        "    # Determine target lane\n",
        "    gap_control = Gap_Controller(obs, following_gap_threshold=30)\n",
        "    target_id = gap_control.lane_checker()\n",
        "\n",
        "    # Get agent state and normalize\n",
        "    agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "    state_tensor = normalizer.normalize_dict_to_tensor(agent_state)\n",
        "\n",
        "    # Get action from policy (no exploration for final video)\n",
        "    with torch.no_grad():\n",
        "        agent_action = policy_network.action(state_tensor).item()\n",
        "\n",
        "    # IDM Longitudinal Control\n",
        "    idm_acceleration = ego_vehicle_idm.longitudinal_controller(input_variables)\n",
        "    longitudinal_control = idm_acceleration\n",
        "\n",
        "    # Transform agent action to steering angle\n",
        "    lateral_control = state_manager.steering_angle(agent_action, L)\n",
        "\n",
        "    # Combine actions\n",
        "    action = [longitudinal_control, lateral_control]\n",
        "\n",
        "    # Execute action\n",
        "    obs, reward, done, truncated, info = env.step(action)\n",
        "    done = done or truncated\n",
        "\n",
        "    # Record frame\n",
        "    final_recorder.add_frame(env, reward)\n",
        "\n",
        "    # Update for next step\n",
        "    state_manager = ENV(obs, longitudinal_control)\n",
        "    ego_state_idm = state_manager.ego_state_idm()\n",
        "    lead_state = state_manager.longitudinal_lead_state()\n",
        "\n",
        "    gap = lead_state['x']\n",
        "    delta_velocity = lead_state['vx']\n",
        "    input_variables = [gap, ego_state_idm['vx'], delta_velocity]\n",
        "\n",
        "# Finalize the video\n",
        "video_path, stats = final_recorder.stop_recording()\n",
        "\n",
        "# Reset rendering mode\n",
        "env.unwrapped.config[\"render_mode\"] = original_render_mode\n",
        "\n",
        "# Try to display the final video\n",
        "try:\n",
        "    from IPython.display import Video\n",
        "    display(Video(final_video_path, embed=True, width=800))\n",
        "except:\n",
        "    print(\"Final video created but couldn't be displayed in the notebook.\")\n",
        "\n",
        "# Final plots after training is complete\n",
        "print(\"\\nTraining complete! Creating final plots...\")\n",
        "\n",
        "# Plot all rewards in separate figures\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(timesteps_list, timestep_rewards)\n",
        "plt.xlabel(\"Timesteps\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"Total Reward per Timestep\")\n",
        "plt.savefig(\"final_total_rewards.png\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(timesteps_list, timestep_acce_rewards)\n",
        "plt.xlabel(\"Timesteps\")\n",
        "plt.ylabel(\"Acceleration Reward\")\n",
        "plt.title(\"Acceleration Reward per Timestep\")\n",
        "plt.savefig(\"final_acce_rewards.png\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(timesteps_list, timestep_rate_rewards)\n",
        "plt.xlabel(\"Timesteps\")\n",
        "plt.ylabel(\"Rate Reward\")\n",
        "plt.title(\"Rate Reward per Timestep\")\n",
        "plt.savefig(\"final_rate_rewards.png\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(timesteps_list, timestep_time_rewards)\n",
        "plt.xlabel(\"Timesteps\")\n",
        "plt.ylabel(\"Time Reward\")\n",
        "plt.title(\"Time Reward per Timestep\")\n",
        "plt.savefig(\"final_time_rewards.png\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# Plot loss history\n",
        "if len(loss_history) > 0:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(len(loss_history)), loss_history)\n",
        "    plt.xlabel(\"Q-Learning Updates\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Loss over Training Updates\")\n",
        "    plt.savefig(\"final_loss.png\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# Plot episode returns\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(global_returns)), global_returns)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Episode Return\")\n",
        "plt.title(\"Episode Returns\")\n",
        "plt.savefig(\"final_episode_returns.png\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# Plot episode lengths\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(episode_indices, episode_lengths)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Length (Steps)\")\n",
        "plt.title(\"Episode Lengths\")\n",
        "plt.savefig(\"final_episode_lengths.png\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "print(\"Final plots saved!\")\n",
        "print(f\"All videos saved in the 'training_videos' directory\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}