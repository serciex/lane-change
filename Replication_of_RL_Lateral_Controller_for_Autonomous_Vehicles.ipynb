{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serciex/lane-change/blob/main/Replication_of_RL_Lateral_Controller_for_Autonomous_Vehicles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAqx0bsIEQbi"
      },
      "source": [
        "Installing Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "AAc_-AeGEE5H",
        "outputId": "d55d73ea-35df-4695-d964-ce89c79570f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Collecting highway-env\n",
            "  Downloading highway_env-1.10.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a2 in /usr/local/lib/python3.11/dist-packages (from highway-env) (1.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from highway-env) (0.0.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from highway-env) (2.0.2)\n",
            "Requirement already satisfied: pygame>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from highway-env) (2.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from highway-env) (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from highway-env) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from highway-env) (1.14.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a2->highway-env) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a2->highway-env) (4.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->highway-env) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->highway-env) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->highway-env) (1.17.0)\n",
            "Downloading highway_env-1.10.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: highway-env\n",
            "Successfully installed highway-env-1.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pygame\n",
        "!pip install highway-env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Civ2P__KEUSq"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-ZwGd9eVEWGY"
      },
      "outputs": [],
      "source": [
        "#Import Libraries\n",
        "from os import truncate\n",
        "import math\n",
        "import gymnasium\n",
        "import highway_env\n",
        "from matplotlib import pyplot as plt\n",
        "import pygame\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from IPython.display import Image, display\n",
        "from collections import deque\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF-2bRYTIjH6"
      },
      "source": [
        "Environment Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "y922XK-cIewr",
        "outputId": "9bc36d04-f821-4b27-b473-a44eab901be1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<highway_env.envs.common.action.ContinuousAction at 0x7a3915d307d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Configure Environment Conditions\n",
        "config = {\n",
        "    \"lanes_count\": 3,\n",
        "    \"lane_width\": 3.75,\n",
        "    \"observation\": {\n",
        "        \"type\": \"Kinematics\",\n",
        "        \"features\": [\"x\", \"y\", \"vx\", \"vy\", \"heading\", \"lat_off\"]\n",
        "    },\n",
        "    \"action\": {\"type\": \"ContinuousAction\"},\"ego_spawn_random\": True,\n",
        "    \"policy_frequency\": 10,\n",
        "}\n",
        "env = gymnasium.make('highway-v0', render_mode='rgb_array', config=config)\n",
        "frames = []\n",
        "\n",
        "# Action Setup\n",
        "highway_env.envs.common.action.ContinuousAction(env, lateral=True,\n",
        "                                                longitudinal=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2HbWr4urTXQ8"
      },
      "outputs": [],
      "source": [
        "# Environment Manager\n",
        "class ENV(env.__class__):\n",
        "  \"\"\"\n",
        "  s = (v,a,x,y,thetha,id,w,c) ∈ S\n",
        "\n",
        "  Lateral Agent State:\n",
        "  Obs Data:\n",
        "  x = vehicle x position (x)\n",
        "  y = vehicle y position (y)\n",
        "  v = vehicle speed (vx)\n",
        "  thetha = yaw angle (heading)\n",
        "\n",
        "  Input:\n",
        "  a = longitudinal acceleration (longitudinal_control)\n",
        "  id = target lane id\n",
        "  w = lane width\n",
        "  c = road curvature\n",
        "\n",
        "  Extra Data:\n",
        "  vy = lateral rate (vy)\n",
        "  delta_lat_deviation = change in lateral deviation (lat_off)\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, obs,a):\n",
        "    self.obs = obs\n",
        "    self.a = a\n",
        "\n",
        "  def ego_state_idm(self):\n",
        "      ax, ay, avx, avy, athetha, _ = self.obs[0]\n",
        "      self._ego_state_idm = {\"x\": ax, \"y\": ay, \"vx\": avx,\"thetha\": athetha,\n",
        "                         \"longitudinal_acceleration\":self.a}\n",
        "\n",
        "      return self._ego_state_idm\n",
        "\n",
        "  def ego_state_agent(self,target_id):\n",
        "      ax, ay, avx, avy, athetha, _ = self.obs[0]\n",
        "      vehicle = env.unwrapped.vehicle\n",
        "\n",
        "      # Control Parameters\n",
        "      self.id = target_id\n",
        "\n",
        "      # Environment Parameters\n",
        "      self.s,_ = vehicle.lane.local_coordinates(vehicle.position)\n",
        "      self.w = vehicle.lane.width\n",
        "      self.c = vehicle.lane.heading_at(np.clip(\n",
        "          vehicle.lane.local_coordinates(vehicle.position)[0],\n",
        "          0, vehicle.lane.length))\n",
        "      self.v = math.sqrt(avx**2+avy**2)\n",
        "\n",
        "      self._ego_state_agent = {\"x\": ax, \"y\": ay, \"vx\": self.v,\"thetha\": athetha,\n",
        "                         \"lane_width\":self.w,\"lane_id\":self.id,\n",
        "                         \"self_curvature\":self.c,\n",
        "                         \"longitudinal_acceleration\":self.a}\n",
        "\n",
        "      return self._ego_state_agent\n",
        "\n",
        "  def longitudinal_lead_state(self):\n",
        "    # Lead State Parameters on the same Lane (5 columns: presence, x, y, vx, vy)\n",
        "    ego_vehicle = env.unwrapped.vehicle\n",
        "    lead_vehicle = env.unwrapped.road.neighbour_vehicles(ego_vehicle, lane_index=ego_vehicle.lane_index)[0]\n",
        "    if lead_vehicle:\n",
        "      gap = lead_vehicle.position[0] - ego_vehicle.position[0]\n",
        "      delta_velocity = ego_vehicle.velocity[0] - lead_vehicle.velocity[0]\n",
        "      self.longitudinal_lead_state = {\"x\": gap, \"vx\": delta_velocity}\n",
        "\n",
        "    else:\n",
        "      self.longitudinal_lead_state = {\"x\": 10, \"vx\": 0}\n",
        "\n",
        "    return self.longitudinal_lead_state\n",
        "\n",
        "  #Reward Function\n",
        "  def reward_function(self, action, obs_old, obs_new, target_id, w1=1, w2=1, w3=0.05):\n",
        "    \"\"\"\n",
        "    Reward Function:\n",
        "\n",
        "    Acceleration Reward: r_acce = w1*f_acce(a_yaw)\n",
        "    a_yaw = lateral acceleration (self.action)\n",
        "\n",
        "    Rate Reward: r_rate = w2*f_rate(w_yaw)\n",
        "    w_yaw = lateral rate (vy)\n",
        "\n",
        "    Time Reward: r_time = w3*f_time (delta_lat_deviation)\n",
        "    delta_lat_deviation = change in lateral deviation (self.lat_off)\n",
        "\n",
        "    Reward = Cummulative Sum of r_acce + Cummulative Sum of r_rate + Cummulative Sum of r_time\n",
        "\n",
        "    \"\"\"\n",
        "    self.target_id = (\"0\",\"1\",target_id)\n",
        "    target_lane_object = env.unwrapped.road.network.get_lane(self.target_id)\n",
        "    vehicle_s, _ = env.unwrapped.vehicle.lane.local_coordinates(env.unwrapped.vehicle.position)\n",
        "    _ , self.delta_lat_deviaton = target_lane_object.local_coordinates(env.unwrapped.vehicle.position)\n",
        "\n",
        "    self.action = action\n",
        "    w_yaw = obs_new[0][3]-obs_old[0][3]\n",
        "\n",
        "    # Acceleration Reward\n",
        "    acce_reward = -1*abs(self.action)\n",
        "\n",
        "    # Rate Reward\n",
        "    rate_reward = -1*abs(w_yaw)\n",
        "\n",
        "    # Time Reward\n",
        "    time_reward = -0.05 * abs(self.delta_lat_deviaton)\n",
        "\n",
        "    # Overall Reward\n",
        "    self.reward = w1*acce_reward + w2*rate_reward + w3*time_reward\n",
        "\n",
        "    return [self.reward, acce_reward, rate_reward, time_reward]\n",
        "\n",
        "  #Acceleration to Steering angle\n",
        "  def steering_angle(self, agent_action,L=1):\n",
        "    \"\"\"\n",
        "    Steering Angle: theta = atan(a_yaw/v^2)\n",
        "    a_yaw = lateral acceleration (agent_action)\n",
        "    v = vehicle speed (vx)\n",
        "    \"\"\"\n",
        "    self.angle = math.atan(L*agent_action/self.ego_state_idm()['vx']**2)\n",
        "\n",
        "    return self.angle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEPNdVET6-mv"
      },
      "source": [
        "Agent Defintion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SJ68SiyJTSx7"
      },
      "outputs": [],
      "source": [
        "class Agent(nn.Module):\n",
        "  \"\"\"\n",
        "  s = (x, y, vx, vy, thetha, lane_width, lane_id, self_curvature, longitudinal_acceleration) ∈ S\n",
        "\n",
        "  Lateral Agent State:\n",
        "  x = vehicle x position\n",
        "  y = vehicle y position\n",
        "  vx = vehicle speed (longitudinal)\n",
        "  thetha = yaw angle (heading)\n",
        "  lane_width = width of the lane\n",
        "  lane_id = target lane id\n",
        "  self_curvature = road curvature at the vehicle's position\n",
        "  longitudinal_acceleration = vehicle longitudinal acceleration\n",
        "  \"\"\"\n",
        "  def __init__(self, state_dim):\n",
        "    super(Agent, self).__init__()\n",
        "    self.state_dim = state_dim\n",
        "    # Define Network A & C\n",
        "    self.networkA = nn.Sequential(\n",
        "        nn.Linear(self.state_dim, 100),\n",
        "        nn.Linear(100, 1),\n",
        "        nn.Softplus()\n",
        "    )\n",
        "    self.networkC = nn.Sequential(\n",
        "        nn.Linear(self.state_dim + 1, 100),\n",
        "        nn.Linear(100, 1),\n",
        "    )\n",
        "\n",
        "    # Define Network B\n",
        "    self.networkB1 = nn.Sequential(\n",
        "        nn.Linear(self.state_dim, 150),\n",
        "        nn.Linear(150, 1),\n",
        "    )\n",
        "    self.networkB2 = nn.Sequential(\n",
        "        nn.Linear(self.state_dim, 150),\n",
        "        nn.Linear(150, 1),\n",
        "    )\n",
        "    self.networkB3 = nn.Sequential(\n",
        "        nn.Linear(self.state_dim, 150),\n",
        "        nn.Linear(150, 1),\n",
        "    )\n",
        "\n",
        "  def forward(self, state, a, terminal):\n",
        "    \"\"\"\n",
        "    Q(s,a) = A(s)*(B(s)-a)^2 + C(s)\n",
        "    \"\"\"\n",
        "    if isinstance(state, dict):\n",
        "      state_tensor = torch.tensor(\n",
        "          [state[key] for key in ['vx', 'longitudinal_acceleration', 'x', 'y', 'thetha', 'lane_id', 'lane_width', 'self_curvature']],\n",
        "          dtype=torch.float32\n",
        "      ).unsqueeze(0)\n",
        "    else:\n",
        "        self.state = state\n",
        "    # Use a directly if it's already a tensor\n",
        "    self.a = a if isinstance(a, torch.Tensor) else torch.tensor([[a]], dtype=torch.float32)\n",
        "    self.terminal_condition = terminal if isinstance(terminal, torch.Tensor) else torch.tensor([[terminal]], dtype=torch.float32)\n",
        "\n",
        "    # Output of the Networks\n",
        "    self.A = -self.networkA(self.state)\n",
        "    self.C = self.networkC(torch.concat((self.state, self.terminal_condition), dim=1))\n",
        "    self.B = torch.max(self.networkB1(self.state) * self.networkB2(self.state),\n",
        "                         self.networkB3(self.state))\n",
        "\n",
        "    # Q-function Approximation\n",
        "    q_value = self.A * ((self.B - self.a) ** 2) + self.C\n",
        "\n",
        "    return q_value\n",
        "\n",
        "  def action(self, state):\n",
        "      # Convert it to a tensor\n",
        "      if isinstance(state, dict):\n",
        "          state_tensor = torch.tensor(\n",
        "              [state[key] for key in ['vx', 'longitudinal_acceleration', 'x', 'y', 'thetha', 'lane_id', 'lane_width', 'self_curvature']],\n",
        "              dtype=torch.float32\n",
        "          ).unsqueeze(0)\n",
        "      else:\n",
        "          state_tensor = state\n",
        "      self.state = state_tensor\n",
        "      self.B = torch.max(self.networkB1(self.state) * self.networkB2(self.state),\n",
        "                         self.networkB3(self.state))\n",
        "      return self.B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo-7aVU9J681"
      },
      "source": [
        "Experience Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7g2c9HRSJ8Xs"
      },
      "outputs": [],
      "source": [
        "# Buffer Class\n",
        "class Experience_Buffer():\n",
        "  \"\"\"\n",
        "  Define Experience Buffer\n",
        "  \"\"\"\n",
        "  def __init__(self, buffer_size):\n",
        "    self.buffer_size = buffer_size\n",
        "    # Define Buffer as Deque with Limit of Default Size buffer_size elements\n",
        "    self.buffer = deque(maxlen=self.buffer_size)\n",
        "\n",
        "  def add(self, state, action, reward, next_state, terminal_condition):\n",
        "    # Experience is the state transition tensor/vector\n",
        "    state_tensor = torch.tensor(\n",
        "        [state[key] for key in ['vx', 'longitudinal_acceleration', 'x', 'y', 'thetha', 'lane_id', 'lane_width', 'self_curvature']],\n",
        "        dtype=torch.float32\n",
        "    ).unsqueeze(0)\n",
        "    next_state_tensor = torch.tensor(\n",
        "        [next_state[key] for key in ['vx', 'longitudinal_acceleration', 'x', 'y', 'thetha', 'lane_id', 'lane_width', 'self_curvature']],\n",
        "        dtype=torch.float32\n",
        "    ).unsqueeze(0)\n",
        "    action_tensor = torch.tensor([[action]], dtype=torch.float32)\n",
        "    reward_tensor = torch.tensor([[reward]], dtype=torch.float32)\n",
        "    terminal_tensor = torch.tensor([[terminal_condition]], dtype=torch.float32)\n",
        "\n",
        "    self.transition = torch.concat((state_tensor, action_tensor, reward_tensor,\n",
        "                                    next_state_tensor, terminal_tensor), dim=1)\n",
        "    self.buffer.append(self.transition)\n",
        "\n",
        "  def sample_experience(self, batch_size):\n",
        "    # Randomly sample Experience from buffer\n",
        "    batch = random.sample(self.buffer, batch_size)\n",
        "    return batch\n",
        "\n",
        "  def size(self):\n",
        "    # Return the current number of elements in the buffer.\n",
        "    return len(self.buffer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enKF1jUjQxpL"
      },
      "source": [
        "Lateral Controller (Gap Checker)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Gap_Controller(env.__class__):\n",
        "    def __init__(self, obs=None):\n",
        "        if obs is not None:\n",
        "            self.obs = obs\n",
        "\n",
        "    def lane_checker(self, safety_threshold=10.0):\n",
        "        \"\"\"\n",
        "        Finds adjacent lane (A=Left, B=Right) where the closest vehicle\n",
        "        in that lane is furthest away (maximizing minimum distance),\n",
        "        and above a safety threshold. Returns the index of the best lane,\n",
        "        or current lane index if no suitable option.\n",
        "        \"\"\"\n",
        "        unwrapped_env = env.unwrapped\n",
        "        vehicle = unwrapped_env.vehicle\n",
        "        network = unwrapped_env.road.network\n",
        "        ego_pos = vehicle.position\n",
        "\n",
        "        env_config = unwrapped_env.config\n",
        "\n",
        "        try:\n",
        "            origin, dest, current_idx = network.get_closest_lane_index(ego_pos)\n",
        "        except Exception:\n",
        "            print(\"Error: Cannot determine current vehicle lane.\")\n",
        "            return None\n",
        "\n",
        "        lanes_to_check = {}\n",
        "        # Lane A (Left)\n",
        "        if current_idx < env_config[\"lanes_count\"] - 1:\n",
        "            lanes_to_check['left'] = (origin, dest, current_idx + 1)\n",
        "        # Lane B (Right)\n",
        "        if current_idx > 0:\n",
        "            lanes_to_check['right'] = (origin, dest, current_idx - 1)\n",
        "\n",
        "        if not lanes_to_check:\n",
        "            return current_idx\n",
        "\n",
        "        lane_min_distances = {}\n",
        "        for direction, lane_tuple in lanes_to_check.items():\n",
        "            min_dist_sq_in_lane = float('inf')\n",
        "            found_vehicle_in_lane = False\n",
        "            try:\n",
        "                target_lane = network.get_lane(lane_tuple)\n",
        "                for veh in unwrapped_env.road.vehicles:\n",
        "                    if veh is vehicle: continue\n",
        "                    try:\n",
        "                        if network.get_closest_lane_index(veh.position) == lane_tuple:\n",
        "                            found_vehicle_in_lane = True\n",
        "                            dist_sq = np.sum((veh.position - ego_pos)**2)\n",
        "                            min_dist_sq_in_lane = min(min_dist_sq_in_lane, dist_sq)\n",
        "                    except Exception: continue\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "            lane_min_distances[direction] = math.sqrt(min_dist_sq_in_lane) if found_vehicle_in_lane else float('inf')\n",
        "\n",
        "        best_lane_idx = current_idx\n",
        "        max_of_min_dist = -1.0\n",
        "\n",
        "        # Check Left Lane\n",
        "        if 'left' in lane_min_distances and lane_min_distances['left'] >= safety_threshold:\n",
        "            if lane_min_distances['left'] > max_of_min_dist :\n",
        "                max_of_min_dist = lane_min_distances['left']\n",
        "                best_lane_idx = lanes_to_check['left'][2]\n",
        "\n",
        "        # Check Right Lane\n",
        "        if 'right' in lane_min_distances and lane_min_distances['right'] >= safety_threshold:\n",
        "            if lane_min_distances['right'] > max_of_min_dist :\n",
        "                best_lane_idx = lanes_to_check['right'][2]\n",
        "\n",
        "        return best_lane_idx\n"
      ],
      "metadata": {
        "id": "ydU55ZY5eX4U"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDNPakyp7APE"
      },
      "source": [
        "Longitudinal Controller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jU_TDnfv7CzO"
      },
      "outputs": [],
      "source": [
        "class IDM():\n",
        "    ''' Intelligent Driving Model for Longitudinal Control\n",
        "\n",
        "    Control parameters:\n",
        "      a: maximum acceleration\n",
        "      b: comfortable deceleration\n",
        "      delta: acceleration exponent\n",
        "      T: safe time headway\n",
        "\n",
        "    Parameters:\n",
        "      s0: minimum gap\n",
        "      v0: desired speed\n",
        "\n",
        "      a (Maximum Acceleration): How fast the vehicle can speed up (m/s²).\n",
        "      b (Comfortable Deceleration): How smoothly the vehicle slows down (m/s²).\n",
        "      δ (Acceleration Exponent): The non-linearity factor in acceleration.\n",
        "      T (Safe Time Headway): The desired minimum following time gap (s).\n",
        "\n",
        "    Input variables:\n",
        "      s: current gap\n",
        "      v: current vehicle speed\n",
        "      delta_v: relative speed (difference between the vehicle's speed and the leading vehicle's speed)\n",
        "    '''\n",
        "\n",
        "    def __init__(self, desired_parameters, control_parameters):\n",
        "        # Unpack initial parameters: [s0, v0]\n",
        "        self.s0, self.v0 = desired_parameters\n",
        "        # Unpack control parameters: [a, b, delta, T]\n",
        "        self.a, self.b, self.delta, self.T = control_parameters\n",
        "\n",
        "    def longitudinal_controller(self, input_variables):\n",
        "        # Unpack input variables: [s, v, delta_v]\n",
        "        s, v, delta_v = input_variables\n",
        "\n",
        "        # Small epsilon to account for very small gaps and avoid division by zero\n",
        "        epsilon = 1e-6\n",
        "\n",
        "        # Desired gap: s* = s0 + v*T + (v * delta_v) / (2 * sqrt(a * b))\n",
        "        desired_gap = self.s0 + max(0, v * self.T + ((v * delta_v) / (2 * math.sqrt(self.a * self.b))))\n",
        "\n",
        "        # IDM acceleration: a_IDM = a * [ 1 - (v / v0)^delta - (s* / s)^2 ]\n",
        "        acceleration = self.a * (1 - (v / self.v0)**self.delta - (desired_gap / (s + epsilon))**2)\n",
        "\n",
        "        return acceleration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21gxd9D8au1O"
      },
      "source": [
        "Initialize Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gPl1lptqKdAy"
      },
      "outputs": [],
      "source": [
        "# Initialize Environment\n",
        "obs, _ = env.reset()\n",
        "\n",
        "# Initialize Environment Manager and Reward\n",
        "state_manager = ENV(obs,obs[0][2])\n",
        "\n",
        "# State Manager for Ego and Lead State\n",
        "ego_state_idm = state_manager.ego_state_idm()\n",
        "lead_state = state_manager.longitudinal_lead_state()\n",
        "\n",
        "# Initial Longitundinal Positions\n",
        "ego_position_idm = ego_state_idm['x']\n",
        "lead_position_idm = lead_state['x']\n",
        "\n",
        "# Initial Velocities (using vx for longitudinal control)\n",
        "ego_velocity_idm = ego_state_idm['vx']\n",
        "lead_velocity_idm = lead_state['vx']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZvAgaQfIyK6"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYA-H09sIw2_",
        "outputId": "c254e145-abb7-4a2a-e562-8a6df317210c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**** Target network updated at episode 0 ****\n",
            "Episode 0 finished after 57 steps. Total Reward: -91.67\n",
            "Episode 1 finished after 56 steps. Total Reward: -92.99\n",
            "Episode 2 finished after 86 steps. Total Reward: -142.55\n",
            "Episode 3 finished after 58 steps. Total Reward: -97.59\n",
            "Episode 4 finished after 73 steps. Total Reward: -117.21\n",
            "Episode 5 finished after 53 steps. Total Reward: -86.15\n",
            "Episode 6 finished after 57 steps. Total Reward: -91.72\n",
            "Episode 7 finished after 70 steps. Total Reward: -113.51\n",
            "Episode 8 finished after 94 steps. Total Reward: -152.69\n"
          ]
        }
      ],
      "source": [
        "################################################################################\n",
        "\"\"\"Initialization\"\"\"\n",
        "################################################################################\n",
        "# IDM Initialization\n",
        "control_parameters = [0.1, 5, 4, 4]  # a, b, δ, T\n",
        "desired_parameters = [20, 10.0]       # s0, v0\n",
        "\n",
        "# Set initial Input variables using the gap, current velocity, and relative lead velocity\n",
        "input_variables = [lead_position_idm, ego_velocity_idm, lead_velocity_idm]\n",
        "\n",
        "# Setup the IDM Model for Longitudinal control\n",
        "ego_vehicle_idm = IDM(desired_parameters, control_parameters)\n",
        "\n",
        "# Agent Initialization\n",
        "lr = 0.01\n",
        "gamma = 0.9\n",
        "buffer_size = 5000\n",
        "num_episodes = 10000\n",
        "batch_size = 1000\n",
        "timesteps = 40000\n",
        "update_target_frequency = 50\n",
        "target_id = env.unwrapped.vehicle.lane_index[-1]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Environment Information\n",
        "L = env.unwrapped.vehicle.LENGTH\n",
        "agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "state_dim = len(agent_state)\n",
        "epsilon = 0.5\n",
        "epsilon_decay_rate = 1000\n",
        "threshold = 30\n",
        "\n",
        "# Initialize Buffer\n",
        "buffer = Experience_Buffer(buffer_size)\n",
        "\n",
        "# Setup Lateral Control Agent\n",
        "target_network = Agent(state_dim).to(device)\n",
        "policy_network = Agent(state_dim).to(device)\n",
        "\n",
        "# Initialize target network\n",
        "target_network.load_state_dict(policy_network.state_dict())\n",
        "\n",
        "# Loss and Criterion Initialization\n",
        "policy_optimizer = torch.optim.Adam(policy_network.parameters(), lr=lr)\n",
        "policy_loss_fn = nn.MSELoss()\n",
        "\n",
        "# Tracking loss and Rewards values over training steps\n",
        "loss_history = []\n",
        "episode_rewards = []\n",
        "\n",
        "total_steps_taken = 0\n",
        "\n",
        "################################################################################\n",
        "\"\"\"Testing Loop\"\"\"\n",
        "################################################################################\n",
        "for episodes in range(num_episodes):\n",
        "    # Reset environment and state manager\n",
        "    obs, _ = env.reset()\n",
        "    state_manager = ENV(obs, 0.0)\n",
        "\n",
        "    # Initial state information\n",
        "    ego_state_idm = state_manager.ego_state_idm()\n",
        "    lead_state = state_manager.longitudinal_lead_state()\n",
        "    agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "    on_road_check = True\n",
        "    collision_check = False\n",
        "\n",
        "    # Agent activation flag and target lane storage\n",
        "    activated_target_lane = None\n",
        "\n",
        "    # Update IDM inputs based on initial state\n",
        "    gap = lead_state['x']\n",
        "    delta_velocity = lead_state['vx']\n",
        "    input_variables = [gap, ego_state_idm['vx'], delta_velocity]\n",
        "\n",
        "    steps = 0\n",
        "    terminal = 0\n",
        "    current_episode_reward = 0\n",
        "    episode_acce_reward = 0\n",
        "    episode_rate_reward = 0\n",
        "    episode_time_reward = 0\n",
        "    counter = 0\n",
        "\n",
        "    # Main loop for each episode\n",
        "    while steps < timesteps and counter < 50:\n",
        "        steps += 1\n",
        "\n",
        "        if not on_road_check or collision_check:\n",
        "          counter+=1\n",
        "\n",
        "        # Update epsilon using a decay rule\n",
        "        epsilon = max(0.05, 1.0 - (1.0 - 0.05) * (total_steps_taken / epsilon_decay_rate))\n",
        "\n",
        "        # Recompute gap\n",
        "        gap = lead_state['x']\n",
        "\n",
        "        ########################################################################\n",
        "        \"\"\"Action Preparation\"\"\"\n",
        "        ########################################################################\n",
        "        gap_control = Gap_Controller(obs)\n",
        "        activated_target_lane = gap_control.lane_checker(safety_threshold=threshold)\n",
        "\n",
        "        # Determine target lane\n",
        "        target_id = activated_target_lane\n",
        "        agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "\n",
        "        # Epsilon-greedy Action\n",
        "        if random.random() < epsilon:\n",
        "          agent_action = random.uniform(-1, 1)\n",
        "        else:\n",
        "          with torch.no_grad():\n",
        "              agent_action = policy_network.action(agent_state).item()\n",
        "\n",
        "        # Store prior state for buffer\n",
        "        old_state = agent_state\n",
        "        obs_old = obs\n",
        "\n",
        "        # IDM Longitudinal Control\n",
        "        idm_acceleration = ego_vehicle_idm.longitudinal_controller(input_variables)\n",
        "        longitudinal_control = idm_acceleration\n",
        "\n",
        "        # Transform agent action (acceleration prediction) to steering angle\n",
        "        agent_action = state_manager.steering_angle(agent_action, L)\n",
        "        lateral_control = agent_action\n",
        "\n",
        "        # Combine longitudinal and lateral actions\n",
        "        action = [longitudinal_control, lateral_control]\n",
        "\n",
        "        ########################################################################\n",
        "        \"\"\"Data Collection\"\"\"\n",
        "        ########################################################################\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "        # Check for off-road or collision events\n",
        "        on_road_check = env.unwrapped.vehicle.on_road\n",
        "        collision_check = info['crashed']\n",
        "\n",
        "        # Terminal condition check\n",
        "        if steps == timesteps or not on_road_check or collision_check:\n",
        "            terminal = 1\n",
        "        else:\n",
        "            terminal = 0\n",
        "\n",
        "        # Update state manager with new observation and applied longitudinal control\n",
        "        state_manager = ENV(obs, longitudinal_control)\n",
        "        # Compute reward based on the new state\n",
        "        reward_per_episode = state_manager.reward_function(agent_action, obs_old, obs, target_id)\n",
        "\n",
        "        # Reward Logging\n",
        "        reward = reward_per_episode[0]\n",
        "        current_episode_reward += reward\n",
        "        episode_acce_reward += reward_per_episode[1]\n",
        "        episode_rate_reward += reward_per_episode[2]\n",
        "        episode_time_reward += reward_per_episode[3]\n",
        "\n",
        "        ########################################################################\n",
        "        \"\"\"Update IDM Controller Inputs for next step\"\"\"\n",
        "        ########################################################################\n",
        "        ego_state_idm = state_manager.ego_state_idm()\n",
        "        lead_state = state_manager.longitudinal_lead_state()\n",
        "        agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "\n",
        "        gap = lead_state['x']\n",
        "        delta_velocity = lead_state['vx']\n",
        "        input_variables = [gap, ego_state_idm['vx'], delta_velocity]\n",
        "\n",
        "        # Update experience buffer\n",
        "        buffer.add(old_state, agent_action, reward, agent_state, terminal)\n",
        "\n",
        "        ########################################################################\n",
        "        \"\"\"Q-Learning Update\"\"\"\n",
        "        ########################################################################\n",
        "        if buffer.size() >= batch_size:\n",
        "            # Sample and unpack experiences.\n",
        "            rand_experience = buffer.sample_experience(batch_size=batch_size)\n",
        "            transitions = torch.cat([t for t in rand_experience if isinstance(t, torch.Tensor)], dim=0)\n",
        "            states, actions, rewards, next_states, terminals = torch.split(\n",
        "                transitions, [state_dim, 1, 1, state_dim, 1], dim=1)\n",
        "\n",
        "            # Compute current Q-values for the taken actions.\n",
        "            current_q_values = policy_network(states, actions, terminals)\n",
        "\n",
        "            with torch.no_grad():\n",
        "              target_network.eval()\n",
        "\n",
        "              # Compute Q-values for all possible next actions.\n",
        "              q_next_all = []\n",
        "              action_tensor = target_network.action(next_states)\n",
        "              q_val = target_network(next_states, action_tensor, terminals)\n",
        "              q_next_all.append(q_val)\n",
        "\n",
        "              # Concatenate along the action dimension to get a tensor of shape [batch_size, num_actions].\n",
        "              q_next_all = torch.cat(q_next_all, dim=1)\n",
        "\n",
        "              # Take the maximum Q-value across actions for each next state.\n",
        "              max_next_q_values, _ = q_next_all.max(dim=1, keepdim=True)\n",
        "\n",
        "            # Compute the target Q-values using the Bellman update.\n",
        "            target_q_values = rewards + gamma * max_next_q_values * (1 - terminals)\n",
        "\n",
        "            # Compute the loss.\n",
        "            loss = policy_loss_fn(current_q_values, target_q_values.detach())\n",
        "\n",
        "            loss_history.append(loss.item())\n",
        "            total_steps_taken += 1\n",
        "            policy_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            policy_optimizer.step()\n",
        "\n",
        "    ############################################################################\n",
        "    \"\"\"Update Target Policy Network\"\"\"\n",
        "    ############################################################################\n",
        "    if episodes % update_target_frequency == 0:\n",
        "        target_network.load_state_dict(policy_network.state_dict())\n",
        "        print(f\"**** Target network updated at episode {episodes} ****\")\n",
        "\n",
        "    # End of episode: store total reward\n",
        "    episode_rewards.append([current_episode_reward, episode_acce_reward, episode_rate_reward, episode_time_reward])\n",
        "    print(f\"Episode {episodes} finished after {steps} steps. Total Reward: {current_episode_reward:.2f}\")\n",
        "\n",
        "    if len(loss_history) > 0 and total_steps_taken > 0:\n",
        "        avg_loss_last_n = np.mean(loss_history[-steps:]) if steps > 0 else 0\n",
        "        print(f\"Avg Loss (last {steps} steps): {avg_loss_last_n:.4f}\")\n",
        "\n",
        "# End of training loop\n",
        "# Plot Total Reward per Episode\n",
        "episode_rewards_arr = np.array(episode_rewards)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(range(num_episodes), episode_rewards_arr[:, 0], label='Total Reward')\n",
        "plt.plot(range(num_episodes), episode_rewards_arr[:, 1], label='Acceleration Reward')\n",
        "plt.plot(range(num_episodes), episode_rewards_arr[:, 2], label='Rate Reward')\n",
        "plt.plot(range(num_episodes), episode_rewards_arr[:, 3], label='Time Reward')\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"Reward per Episode\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot Loss over Training Updates\n",
        "plt.figure()\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel(\"Q-Learning Update\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss over Training Updates\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}