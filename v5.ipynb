{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serciex/lane-change/blob/main/v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAqx0bsIEQbi"
      },
      "source": [
        "Installing Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AAc_-AeGEE5H",
        "outputId": "58ee338a-4e3b-4f95-daaf-443075c6440e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Collecting highway-env\n",
            "  Downloading highway_env-1.10.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a2 in /usr/local/lib/python3.11/dist-packages (from highway-env) (1.2.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from highway-env) (0.0.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from highway-env) (2.0.2)\n",
            "Requirement already satisfied: pygame>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from highway-env) (2.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from highway-env) (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from highway-env) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from highway-env) (1.15.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a2->highway-env) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a2->highway-env) (4.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->highway-env) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->highway-env) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->highway-env) (1.17.0)\n",
            "Downloading highway_env-1.10.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: highway-env\n",
            "Successfully installed highway-env-1.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pygame\n",
        "!pip install highway-env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Civ2P__KEUSq"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZwGd9eVEWGY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a2f41901-b310-4b2c-d387-507d448b65c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Import Libraries\n",
        "from os import truncate\n",
        "import math\n",
        "import gymnasium\n",
        "import highway_env\n",
        "from matplotlib import pyplot as plt\n",
        "import pygame\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "import gym\n",
        "from random import randint\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "# Workaround for gym compatibility\n",
        "if not hasattr(np, 'bool8'):\n",
        "    np.bool8 = np.bool_\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "def animate_image_list_efficient(image_list):\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Create initial image\n",
        "    im = ax.imshow(image_list[0])\n",
        "    ax.axis('off')\n",
        "    title = ax.set_title('Frame 0')\n",
        "\n",
        "    def animate(frame):\n",
        "        # Just update the image data - more efficient\n",
        "        im.set_array(image_list[frame])\n",
        "        title.set_text(f'Frame {frame}/{len(image_list)}')\n",
        "        return [im, title]\n",
        "\n",
        "    anim = FuncAnimation(fig, animate, frames=len(image_list),\n",
        "                        interval=100, blit=True, repeat=True)\n",
        "    plt.close()\n",
        "    return HTML(anim.to_jshtml())\n",
        "\n",
        "# Initialize plot outside the loop (before the training loop starts)\n",
        "plt.figure(figsize=(10, 6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF-2bRYTIjH6"
      },
      "source": [
        "Environment Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y922XK-cIewr",
        "outputId": "9f702cb4-1092-43b8-d4ac-f9ba3801c07f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<highway_env.envs.common.action.ContinuousAction at 0x7b2195905b90>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Configure Environment Conditions\n",
        "config = {\n",
        "    \"lanes_count\": 3,\n",
        "    \"lane_width\": 3.75,\n",
        "    \"observation\": {\n",
        "        \"type\": \"Kinematics\",\n",
        "        \"features\": [\"x\", \"y\", \"vx\", \"vy\", \"heading\", \"lat_off\"]\n",
        "    },\n",
        "    \"action\": {\"type\": \"ContinuousAction\"},\"ego_spawn_random\": True,\n",
        "    \"policy_frequency\": 10,\n",
        "}\n",
        "env = gymnasium.make('highway-v0', render_mode='rgb_array', config=config)\n",
        "frames = []\n",
        "\n",
        "# Action Setup\n",
        "highway_env.envs.common.action.ContinuousAction(env, lateral=True,\n",
        "                                                longitudinal=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Environment Wrapper"
      ],
      "metadata": {
        "id": "t2j5j5Q8Fh9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(env.unwrapped.vehicle.velocity[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7D0CQd05cGr",
        "outputId": "fb636398-8961-4b81-d52d-0d3372887c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(22.879332711210857)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ENVwrapper(env.wrapper):\n",
        "  def __init__(self,env, w1,w2,w3,parameters):\n",
        "    super().__init__(env)\n",
        "    self.w1 = w1\n",
        "    self.w2 = w2\n",
        "    self.w3 = w3\n",
        "    self.desired_parameters,self.control_parameters, self.reward_weights, self.following_gap = parameters\n",
        "\n",
        "    def lane_check(self):\n",
        "      \"\"\"\n",
        "      Determines which adjacent lane offers the largest front gap for a lane change.\n",
        "      \"\"\"\n",
        "      obs = self.env.observation\n",
        "      ego = env.unwrapped.vehicle\n",
        "      current_lane_id = env.unwrapped.vehicle.lane_index[2]\n",
        "\n",
        "      # Reset flag when lane changes\n",
        "      if self.last_lane is not None and current_lane_id != self.last_lane:\n",
        "          self.gap_controller_checked = False\n",
        "\n",
        "      # Only check if not already checked\n",
        "      if not self.gap_controller_checked:\n",
        "\n",
        "          # Get front gaps using existing longitudinal_lead_state\n",
        "          gap_current_front = obs[1][0]\n",
        "\n",
        "          # Get right lane front gap\n",
        "          current_lane = list(ego.lane_index)\n",
        "          lane_right = (current_lane[0], current_lane[1], current_lane[2] + 1) if current_lane[2] < 2 else ego.lane_index\n",
        "          neighbours_right = env.unwrapped.road.neighbour_vehicles(ego, lane_index=lane_right)\n",
        "          gap_right_front = neighbours_right[0].position[0] - ego.position[0] if neighbours_right and neighbours_right[0] else -float('inf')\n",
        "\n",
        "          # Get left lane front gap\n",
        "          lane_left = (current_lane[0], current_lane[1], current_lane[2] - 1) if current_lane[2] > 0 else ego.lane_index\n",
        "          neighbours_left = env.unwrapped.road.neighbour_vehicles(ego, lane_index=lane_left)\n",
        "          gap_left_front = neighbours_left[0].position[0] - ego.position[0] if neighbours_left and neighbours_left[0] else -float('inf')\n",
        "\n",
        "          # Find best lane\n",
        "          front_gaps = [gap_current_front, gap_right_front, gap_left_front]\n",
        "          best_index = np.argmax(front_gaps)\n",
        "\n",
        "          if best_index == 1:\n",
        "              direction = 1  # Right\n",
        "              gap_right_follower = ego.position[0] - neighbours_right[1].position[0] if len(neighbours_right) > 1 and neighbours_right[1] else float('inf')\n",
        "              self.target_id = direction if gap_right_follower >= self.following_gap_threshold else 0\n",
        "          elif best_index == 2:\n",
        "              direction = -1  # Left\n",
        "              gap_left_follower = ego.position[0] - neighbours_left[1].position[0] if len(neighbours_left) > 1 and neighbours_left[1] else float('inf')\n",
        "              self.target_id = direction if gap_left_follower >= self.following_gap_threshold else 0\n",
        "          else:\n",
        "              self.target_id = 0  # Stay in current lane\n",
        "\n",
        "          self.gap_controller_checked = True\n",
        "\n",
        "      self.last_lane = current_lane_id\n",
        "\n",
        "      return target_id\n",
        "\n",
        "    def idm_controller(self):\n",
        "      obs = self.env.observation\n",
        "      s0,v0,epsilon = self.desired_parameters\n",
        "      a,b,delta,T = self.control_parameters\n",
        "\n",
        "      v = obs[0][3]\n",
        "      delta_v = obs[0][4]\n",
        "      s = obs[0][5]\n",
        "\n",
        "      # Desired gap: s* = s0 + v*T + (v * delta_v) / (2 * sqrt(a * b))\n",
        "      desired_gap = self.s0 + max(0, v * self.T + ((v * delta_v) / (2 * math.sqrt(self.a * self.b))))\n",
        "\n",
        "      # IDM acceleration: a_IDM = a * [ 1 - (v / v0)^delta - (s* / s)^2 ]\n",
        "      acceleration = self.a * (1 - (v / self.v0)**self.delta - (desired_gap / (s + epsilon))**2)\n",
        "\n",
        "      return acceleration\n",
        "\n",
        "    def reward_function(self, obs_old, obs_new):\n",
        "      target_id = self.lateral_target_id()\n",
        "      w1,w2,w3 = self.reward_weights\n",
        "      obs_new = self.env.observation\n",
        "      obs_old = self.env.observation_old\n",
        "\n",
        "      \"\"\"\n",
        "      Reward Function:\n",
        "\n",
        "      Acceleration Reward: r_acce = w1*f_acce(a_yaw)\n",
        "      a_yaw = yaw acceleration (rate of change of yaw rate)\n",
        "\n",
        "      Rate Reward: r_rate = w2*f_rate(w_yaw)\n",
        "      w_yaw = yaw rate (rate of change of heading)\n",
        "\n",
        "      Time Reward: r_time = w3*f_time (delta_lat_deviation)\n",
        "      delta_lat_deviation = change in lateral deviation (self.lat_off)\n",
        "\n",
        "      Reward = Cummulative Sum of r_acce + Cummulative Sum of r_rate + Cummulative Sum of r_time\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "      # Convert direction command to actual target lane\n",
        "      current_lane_id = env.unwrapped.vehicle.lane_index[2]\n",
        "\n",
        "      if target_id == +1:  # Move right\n",
        "          actual_target_lane = min(2, current_lane_id + 1)  # Don't exceed rightmost lane\n",
        "      elif target_id == -1:  # Move left\n",
        "          actual_target_lane = max(0, current_lane_id - 1)  # Don't exceed leftmost lane\n",
        "      else:  # target_id == 0, stay in current lane\n",
        "          actual_target_lane = current_lane_id\n",
        "\n",
        "      # Use current vehicle lane structure and set actual target lane\n",
        "      current_lane_index = list(env.unwrapped.vehicle.lane_index)\n",
        "      current_lane_index[2] = actual_target_lane  # Only change the lane number to actual lane\n",
        "      self.target_lane = tuple(current_lane_index)\n",
        "\n",
        "      target_lane_object = env.unwrapped.road.network.get_lane(self.target_lane)\n",
        "      vehicle_s, _ = env.unwrapped.vehicle.lane.local_coordinates(env.unwrapped.vehicle.position)\n",
        "      _ , self.delta_lat_deviaton = target_lane_object.local_coordinates(env.unwrapped.vehicle.position)\n",
        "\n",
        "      obs = obs_new[0]\n",
        "      obs_old = obs_old[0]\n",
        "\n",
        "      # Extract lateral and longitudinal velocities from observations\n",
        "      vx_current = obs[2]  # longitudinal velocity\n",
        "      vy_current = obs[3]  # lateral velocity\n",
        "      vx_old = obs_old[2]\n",
        "      vy_old = obs_old[3]\n",
        "\n",
        "      dt = 1.0 / env.unwrapped.config['policy_frequency']\n",
        "      L = env.unwrapped.vehicle.LENGTH\n",
        "\n",
        "      # Calculate both yaw rates in parallel using bicycle model\n",
        "      vx_vals = np.array([vx_old, vx_current])\n",
        "      vy_vals = np.array([vy_old, vy_current])\n",
        "\n",
        "      # Vectorized calculations\n",
        "      total_velocities = np.sqrt(vx_vals**2 + vy_vals**2)\n",
        "      curvatures = np.divide(vy_vals, vx_vals * total_velocities + 1e-6,\n",
        "                            out=np.zeros_like(vy_vals), where=(abs(vx_vals) > 1e-6))\n",
        "      yaw_rates = total_velocities * curvatures\n",
        "\n",
        "      previous_yaw_rate, current_yaw_rate = yaw_rates[0], yaw_rates[1]\n",
        "\n",
        "      w_yaw = current_yaw_rate\n",
        "      w_yaw_old = previous_yaw_rate\n",
        "\n",
        "      self.w_acce = (w_yaw-w_yaw_old)*env.unwrapped.config['policy_frequency']\n",
        "\n",
        "      # Acceleration Reward\n",
        "      acce_reward = -w1*abs(self.w_acce)\n",
        "\n",
        "      # Rate Reward\n",
        "      rate_reward = -w2*abs(w_yaw)\n",
        "\n",
        "      # Time Reward\n",
        "      time_reward = -w3*((self.delta_lat_deviaton)**2)\n",
        "\n",
        "      # Off Road\n",
        "      if not env.unwrapped.vehicle.on_road:\n",
        "          off_road_penalty = -10\n",
        "\n",
        "      else:\n",
        "          off_road_penalty = 0\n",
        "\n",
        "      # Overall Reward\n",
        "      self.reward = acce_reward + rate_reward + time_reward + off_road_penalty\n",
        "\n",
        "      return [self.reward, acce_reward, rate_reward, time_reward, off_road_penalty]\n",
        "\n",
        "\n",
        "    def step(self, agent_action):\n",
        "      vehicle = env.unwrapped.vehicle\n",
        "      idm_action = self.idm_controller()\n",
        "      action = [idm_action, agent_action]\n",
        "\n",
        "      # Prior Observation\n",
        "      obs_old = env.observation[0]\n",
        "      v_prev = obs_old[0][2]\n",
        "\n",
        "      obs, reward, done, truncated, info = super().step(action)\n",
        "      custom_reward = self.reward_function(obs_old,obs)\n",
        "\n",
        "      done = done or truncated if vehicle.on_road and not info['crashed'] else True\n",
        "\n",
        "      x = obs[0][0]\n",
        "      y = obs[0][1]\n",
        "      vx = obs[0][2]\n",
        "      thetha = obs[0][4]\n",
        "      lane_width = vehicle.lane.width\n",
        "\n",
        "      lane_id = self.lane_check()\n",
        "\n",
        "      self_curvature = vehicle.lane.heading_at(np.clip(\n",
        "          vehicle.lane.local_coordinates(vehicle.position)[0],\n",
        "          0, vehicle.lane.length))\n",
        "\n",
        "      acceleration = (vx-v_prev)*env.unwrapped.config['policy_frequency']\n",
        "\n",
        "      agent_observation = [x,y,vx,acceleration,thetha,lane_id,lane_width,self_curvature]\n",
        "\n",
        "      return agent_observation, custom_reward, done, truncated, info\n",
        "\n",
        "      def\n"
      ],
      "metadata": {
        "id": "FWMYLohR0WZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEPNdVET6-mv"
      },
      "source": [
        "Agent Defintion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim=8, action_dim=1, hidden_size=128, max_angle=0.5):\n",
        "        super().__init__()\n",
        "        self.action_dim = action_dim\n",
        "        self.max_angle = max_angle\n",
        "\n",
        "        # Policy head (actor) - Output 2 values: mean and log_std\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.mu = nn.Sequential(\n",
        "            nn.Linear(hidden_size, action_dim)\n",
        "                                )\n",
        "        self.sigma = nn.Sequential(\n",
        "            nn.Linear(hidden_size, action_dim),\n",
        "                                   nn.Softplus()\n",
        "                                   )\n",
        "\n",
        "        # Value head (critic)\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Value head\n",
        "        value = self.critic(x)\n",
        "\n",
        "        # Policy head\n",
        "        actor_output = self.shared(x)\n",
        "\n",
        "        action_mean = (self.mu(actor_output))*self.max_angle\n",
        "\n",
        "        action_std = self.sigma(actor_output)\n",
        "        action_std = torch.clamp(action_std, -3, 3)\n",
        "\n",
        "        # Action Distribution\n",
        "        action_dist = torch.distributions.Normal(action_mean, action_std)\n",
        "\n",
        "        return action_dist, value\n",
        "\n",
        "    def action(self, state, deterministic = False):\n",
        "        action_dist, value = self.forward(state)\n",
        "\n",
        "        if deterministic:\n",
        "            action = action_dist.mean\n",
        "            log_prob = action_dist.log_prob(action)\n",
        "\n",
        "        else:\n",
        "          action = action_dist.sample()\n",
        "          log_prob = action_dist.log_prob(action)\n",
        "\n",
        "        return action, log_prob, value"
      ],
      "metadata": {
        "id": "MW5pZ2IxtbyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enKF1jUjQxpL"
      },
      "source": [
        "Lateral Controller (Gap Checker)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21gxd9D8au1O"
      },
      "source": [
        "Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYA-H09sIw2_"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "\"\"\"Initialization\"\"\"\n",
        "################################################################################\n",
        "# Environment\n",
        "# Initialize Environment\n",
        "obs, _ = env.reset()\n",
        "\n",
        "# Agent Initialization\n",
        "total_steps_taken = 0\n",
        "\n",
        "# Tracking for performance\n",
        "timestep_rewards = []  # Store rewards by timestep\n",
        "timestep_acce_rewards = []  # Store acceleration rewards\n",
        "timestep_rate_rewards = []  # Store rate rewards\n",
        "timestep_time_rewards = []  # Store time rewards\n",
        "global_returns = []  # Cumulative return tracking\n",
        "loss_history = []  # Loss values\n",
        "timesteps_list = []  # Track timesteps for plotting\n",
        "episode_lengths = []  # Track episode lengths\n",
        "episode_indices = []  # Track episode indices for plotting\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# IDM Parameters\n",
        "desired_parameters = [20, 10.0]       # s0, v0\n",
        "control_parameters = [0.1, 5, 4, 4]  # a, b, δ, T\n",
        "\n",
        "# Initialize Environment Manager\n",
        "state_manager = ENV(obs, desired_parameters, control_parameters)\n",
        "\n",
        "# Get initial target and state info\n",
        "target_id = state_manager.lane_checker()\n",
        "agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "state_dim = len(agent_state)\n",
        "action_dim = 1\n",
        "\n",
        "# Environment Information\n",
        "L = env.unwrapped.vehicle.LENGTH\n",
        "\n",
        "# Initialize reward and states\n",
        "state_manager.reward_function(obs, obs, target_id, w1=1, w2=1, w3=0.05)\n",
        "ego_state_idm = state_manager.ego_state_idm()\n",
        "lead_state = state_manager.longitudinal_lead_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "_r2sQPbixCmM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters"
      ],
      "metadata": {
        "id": "Rybk4w1dlqba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "max_timesteps_per_rollout = 500  # Max timesteps per rollout\n",
        "K = 10    # Number of policy update epochs\n",
        "test_rate = 5  # Test every 5 batches\n",
        "rollout_size = 5\n",
        "\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "vf_coeff_cl = 0.5\n",
        "ent_coef_c2 = 0.01\n",
        "epsilon = 0.2\n",
        "gae_lambda = 0.99\n",
        "actor_lr = 3e-4  # Reduced learning rate\n",
        "critic_lr = 1e-3\n",
        "hidden_size = 256\n",
        "\n",
        "# Episode counter\n",
        "max_episodes_per_rollout = 1\n",
        "nb_episodes = 1000\n",
        "nb_batches = round(nb_episodes/max_episodes_per_rollout)\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = ActorCritic(state_dim=state_dim, action_dim=action_dim,hidden_size=hidden_size).to(device)\n",
        "actor_params = list(model.shared.parameters()) + list(model.mu.parameters()) + list(model.sigma.parameters())\n",
        "critic_params = list(model.critic.parameters())\n",
        "\n",
        "actor_optimizer = torch.optim.Adam(actor_params, lr=actor_lr)\n",
        "critic_optimizer = torch.optim.Adam(critic_params, lr=critic_lr)\n",
        "\n",
        "# Tracking variables\n",
        "total_env_episodes = 0\n",
        "max_reward = float('-inf')\n",
        "batch_average_reward_history = []\n",
        "batch_episode_history = []\n",
        "individual_test_rewards = []\n",
        "individual_test_episodes = []"
      ],
      "metadata": {
        "id": "jbBscUzUlq5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "6lPXFTCN8eu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_reward_components_history = []\n",
        "batch_episode_history = []\n",
        "\n",
        "for batch in tqdm.tqdm(range(nb_batches)):\n",
        "\n",
        "    # PPO Initialization\n",
        "    max_possible_steps = max_timesteps_per_rollout * rollout_size\n",
        "\n",
        "    # Temporary storage lists\n",
        "    temp_states = []\n",
        "    temp_actions = []\n",
        "    temp_log_probs = []\n",
        "    temp_state_values = []\n",
        "    temp_rewards = []\n",
        "    temp_is_terminal = []\n",
        "\n",
        "    # Env Initialization\n",
        "    obs, _ = env.reset()\n",
        "    state_manager.update(obs)\n",
        "\n",
        "    # Initialize target_id\n",
        "    target_id = state_manager.lane_checker()\n",
        "    agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "\n",
        "    # Episode Reset\n",
        "    obs, _ = env.reset()\n",
        "    state_manager.update(obs)\n",
        "    done = False\n",
        "    obs_old = obs\n",
        "    episode_step = 0\n",
        "    episode_return = 0.0\n",
        "    timestep_counter = 0\n",
        "    rollout = 0\n",
        "\n",
        "    while not done and rollout < rollout_size:\n",
        "        # Check for lane change trigger\n",
        "        if obs[1][0] < state_manager.following_gap_threshold:\n",
        "            target_id = state_manager.lane_checker()\n",
        "\n",
        "        # Get agent state and create tensor\n",
        "        agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "        state_tensor = torch.tensor(\n",
        "            [agent_state[key] for key in ['vx', 'longitudinal_acceleration', 'x', 'y', 'thetha', 'lane_id', 'lane_width', 'self_curvature']],\n",
        "            dtype=torch.float32, device=device\n",
        "        ).unsqueeze(0)\n",
        "\n",
        "        # Get action and value from model\n",
        "        with torch.no_grad():\n",
        "            agent_action, log_prob, value = model.action(state_tensor)\n",
        "            action = state_manager.action(obs, agent_action)\n",
        "\n",
        "        # Take action in environment\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        done = done or truncated\n",
        "\n",
        "        # Terminal conditions\n",
        "        if info[\"crashed\"] or not env.unwrapped.vehicle.on_road or timestep_counter >= max_timesteps_per_rollout:\n",
        "            done = True\n",
        "\n",
        "        # Update state manager with new observation\n",
        "        state_manager.update(obs)\n",
        "\n",
        "        # Compute reward based on the new state\n",
        "        reward_components = state_manager.reward_function(obs_old, obs, target_id)\n",
        "        reward = reward_components[0]\n",
        "\n",
        "        # Store trajectory information in lists\n",
        "        temp_states.append(state_tensor.squeeze(0))\n",
        "        temp_actions.append(agent_action)\n",
        "        temp_log_probs.append(log_prob)\n",
        "        temp_state_values.append(value.squeeze())\n",
        "        temp_rewards.append(torch.tensor(reward, dtype=torch.float32, device=device))\n",
        "        temp_is_terminal.append(done)\n",
        "\n",
        "        timestep_counter += 1\n",
        "        obs_old = obs\n",
        "\n",
        "        # If episode is done, increment rollout and reset for next episode\n",
        "        if done:\n",
        "            rollout += 1\n",
        "            if rollout < rollout_size:\n",
        "                obs, _ = env.reset()\n",
        "                state_manager.update(obs)\n",
        "                done = False\n",
        "                obs_old = obs\n",
        "\n",
        "    actual_timesteps = len(temp_states)\n",
        "\n",
        "    buffer_states = torch.stack(temp_states)  # Shape: [actual_timesteps, state_dim]\n",
        "    buffer_actions = torch.stack(temp_actions)  # Shape: [actual_timesteps, action_dim]\n",
        "    buffer_log_probs = torch.stack(temp_log_probs)  # Shape: [actual_timesteps]\n",
        "    buffer_state_values = torch.stack(temp_state_values)  # Shape: [actual_timesteps]\n",
        "    buffer_rewards = torch.stack(temp_rewards)  # Shape: [actual_timesteps]\n",
        "    buffer_is_terminal = torch.tensor(temp_is_terminal, dtype=torch.float32, device=device)  # Shape: [actual_timesteps]\n",
        "\n",
        "    # Print rollout info\n",
        "    print(f\"Training Batch {batch+1}: Collected {actual_timesteps} timesteps Episode Count: {rollout}\")\n",
        "\n",
        "    # Initialize\n",
        "    advantages = torch.zeros(actual_timesteps, dtype=torch.float32, device=device)\n",
        "    gae = 0\n",
        "\n",
        "    for step_idx in range(actual_timesteps-1, -1, -1):\n",
        "        is_terminal = buffer_is_terminal[step_idx]\n",
        "\n",
        "        # Determine next step values\n",
        "        if step_idx == actual_timesteps - 1:\n",
        "            # Last step in buffer\n",
        "            next_value = 0\n",
        "            next_non_terminal = 0\n",
        "        else:\n",
        "            next_value = buffer_state_values[step_idx + 1]\n",
        "            next_non_terminal = 1 - buffer_is_terminal[step_idx]\n",
        "\n",
        "        # TD error\n",
        "        delta = buffer_rewards[step_idx] + gamma * next_value * next_non_terminal - buffer_state_values[step_idx]\n",
        "\n",
        "        # GAE calculation\n",
        "        gae = delta + gamma * gae_lambda * gae * next_non_terminal\n",
        "        advantages[step_idx] = gae\n",
        "\n",
        "        # Reset GAE AFTER processing if this step is terminal\n",
        "        if is_terminal:\n",
        "            gae = 0\n",
        "\n",
        "    # More robust global normalization\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "    # Calculate returns\n",
        "    returns = advantages + buffer_state_values\n",
        "\n",
        "    # Prepare data for policy update\n",
        "    advantages_data_loader = DataLoader(\n",
        "        TensorDataset(\n",
        "            advantages.detach(),\n",
        "            returns.detach(),              # Add pre-calculated returns\n",
        "            buffer_states.detach(),\n",
        "            buffer_actions.detach(),\n",
        "            buffer_log_probs.detach(),\n",
        "            buffer_state_values.detach()),\n",
        "        batch_size=min(batch_size, actual_timesteps),\n",
        "        shuffle=True)\n",
        "\n",
        "    # Policy update (K epochs)\n",
        "    for epoch in range(K):\n",
        "        for batch_data in advantages_data_loader:\n",
        "            b_adv, b_returns, obs_batch, action_batch, old_log_probs, old_state_values = batch_data\n",
        "\n",
        "            # Forward pass\n",
        "            dist, value = model(obs_batch)\n",
        "            log_probs = dist.log_prob(action_batch)\n",
        "\n",
        "            # Calculate ratio\n",
        "            ratio = torch.exp(log_probs - old_log_probs)\n",
        "\n",
        "            # Policy loss - use b_adv consistently\n",
        "            policy_loss_1 = b_adv * ratio\n",
        "            policy_loss_2 = b_adv * torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
        "            policy_loss = -torch.min(policy_loss_1, policy_loss_2).mean()\n",
        "\n",
        "            actor_loss = policy_loss - ent_coef_c2 * dist.entropy().mean()\n",
        "            critic_loss = F.mse_loss(value.squeeze(-1), b_returns)  # Use pre-calculated returns\n",
        "\n",
        "            # Optimize\n",
        "            actor_optimizer.zero_grad()\n",
        "            actor_loss.backward(retain_graph=True)\n",
        "            torch.nn.utils.clip_grad_norm_(actor_params, max_norm=0.5)\n",
        "            actor_optimizer.step()\n",
        "\n",
        "            critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(critic_params, max_norm=0.5)\n",
        "            critic_optimizer.step()\n",
        "\n",
        "    # Testing section\n",
        "    if batch % test_rate == 0 and batch > 0:\n",
        "        test_rewards = []\n",
        "        test_reward_components = []\n",
        "        frames = []\n",
        "\n",
        "        for test_ep in range(10):\n",
        "            obs, _ = env.reset()\n",
        "            obs_old = obs\n",
        "            episode_reward = 0\n",
        "            episode_reward_components = []\n",
        "            done = False\n",
        "            test_step_counter = 0\n",
        "            state_manager.update(obs)\n",
        "\n",
        "            while not done and test_step_counter < 500:\n",
        "                # Use state_manager lane checker for testing\n",
        "                target_id = state_manager.lane_checker()\n",
        "                agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "\n",
        "                state_tensor = torch.tensor(\n",
        "                    [agent_state[key] for key in ['vx', 'longitudinal_acceleration', 'x', 'y', 'thetha', 'lane_id', 'lane_width', 'self_curvature']],\n",
        "                    dtype=torch.float32, device=device\n",
        "                ).unsqueeze(0)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    agent_action, log_prob, value = model.action(state_tensor, deterministic=True)\n",
        "\n",
        "                action = state_manager.action(obs, agent_action)\n",
        "\n",
        "                obs, _, done, truncated, info = env.step(action)\n",
        "                done = done or truncated\n",
        "\n",
        "                if test_ep % 9 == 0:\n",
        "                    image = env.render()\n",
        "                    frames.append(image)\n",
        "\n",
        "                # Terminal conditions\n",
        "                if info[\"crashed\"] or not env.unwrapped.vehicle.on_road or test_step_counter >= 500:\n",
        "                    done = True\n",
        "\n",
        "                state_manager.update(obs)\n",
        "                reward_components = state_manager.reward_function(obs_old, obs, target_id)\n",
        "\n",
        "                # Store all reward components for this step\n",
        "                episode_reward_components.append(reward_components)\n",
        "\n",
        "                reward = reward_components[0]\n",
        "                episode_reward += reward\n",
        "                test_step_counter += 1\n",
        "                obs_old = obs\n",
        "\n",
        "            test_rewards.append(episode_reward)\n",
        "            test_reward_components.append(episode_reward_components)\n",
        "\n",
        "        # Animation code\n",
        "        display(animate_image_list_efficient(frames))\n",
        "\n",
        "        # Calculate average reward components PER TIMESTEP across all episodes\n",
        "        if test_reward_components:\n",
        "            # First, determine the number of reward components\n",
        "            num_components = len(test_reward_components[0][0]) if test_reward_components[0] else 0\n",
        "\n",
        "            # Calculate average per timestep for each component across all episodes\n",
        "            avg_reward_components = []\n",
        "            for component_idx in range(num_components):\n",
        "                # Collect all timestep values for this component across all episodes\n",
        "                all_timestep_values = []\n",
        "                for episode_components in test_reward_components:\n",
        "                    for step_components in episode_components:\n",
        "                        all_timestep_values.append(step_components[component_idx])\n",
        "\n",
        "                # Average across all timesteps from all episodes\n",
        "                avg_component = sum(all_timestep_values) / len(all_timestep_values) if all_timestep_values else 0\n",
        "                avg_reward_components.append(avg_component)\n",
        "\n",
        "            # Store the average components for plotting\n",
        "            if 'batch_reward_components_history' not in locals():\n",
        "                batch_reward_components_history = []\n",
        "\n",
        "            batch_reward_components_history.append(avg_reward_components)\n",
        "\n",
        "        avg_test_reward = sum(test_rewards) / len(test_rewards)\n",
        "        current_max = max(test_rewards)\n",
        "\n",
        "        if current_max > max_reward:\n",
        "            max_reward = current_max\n",
        "\n",
        "        # Store batch info for plotting\n",
        "        if 'batch_episode_history' not in locals():\n",
        "            batch_episode_history = []\n",
        "        batch_episode_history.append(batch)\n",
        "\n",
        "        # Print average reward components\n",
        "        if 'avg_reward_components' in locals() and avg_reward_components:\n",
        "            print(f\"Average Reward Components (per timestep): {[f'{comp:.3f}' for comp in avg_reward_components]}\")\n",
        "\n",
        "    # Plot every 10 batches\n",
        "    if batch % 10 == 0 and batch > 0 and 'batch_reward_components_history' in locals() and batch_reward_components_history:\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Convert to numpy array for easier indexing\n",
        "        components_array = np.array(batch_reward_components_history)\n",
        "\n",
        "        # Create x-axis values that match the length of components_array\n",
        "        x_values = []\n",
        "        for i, batch_num in enumerate(batch_episode_history):\n",
        "            if batch_num % test_rate == 0 and batch_num > 0:\n",
        "                x_values.append(batch_num)\n",
        "                if len(x_values) == len(batch_reward_components_history):\n",
        "                    break\n",
        "\n",
        "        # Ensure we have matching lengths\n",
        "        if len(x_values) != len(batch_reward_components_history):\n",
        "            x_values = list(range(len(batch_reward_components_history)))\n",
        "\n",
        "        # Define reward component names\n",
        "        component_names = [\n",
        "              \"Total Reward\",\n",
        "              \"Acceleration Reward (Smoothness)\",\n",
        "              \"Rate Reward (Turn Rate)\",\n",
        "              \"Time Reward (Lane Position)\",\n",
        "              \"Off Road Penalty\"\n",
        "          ]\n",
        "\n",
        "        # Plot each component with meaningful names\n",
        "        for i in range(components_array.shape[1]):\n",
        "            label = component_names[i] if i < len(component_names) else f'Component {i}'\n",
        "            plt.plot(x_values,\n",
        "                    components_array[:, i],\n",
        "                    marker='o', linewidth=2, label=label)\n",
        "\n",
        "        plt.title(f\"Average Reward Components Per Timestep Over Training (Batch {batch})\")\n",
        "        plt.xlabel(\"Batch Number\")\n",
        "        plt.ylabel(\"Average Component Value Per Timestep\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "print(\"Training completed!\")"
      ],
      "metadata": {
        "id": "gMY396Rx5F1J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}