{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serciex/lane-change/blob/main/replication_of_rl_lateral_controller_for_autonomous_vehicles_cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAqx0bsIEQbi"
      },
      "source": [
        "Installing Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AAc_-AeGEE5H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c68de364-3282-4683-802d-7dee58ed9c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: highway-env in /usr/local/lib/python3.11/dist-packages (1.10.1)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a2 in /usr/local/lib/python3.11/dist-packages (from highway-env) (1.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from highway-env) (0.0.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from highway-env) (2.0.2)\n",
            "Requirement already satisfied: pygame>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from highway-env) (2.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from highway-env) (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from highway-env) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from highway-env) (1.15.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a2->highway-env) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a2->highway-env) (4.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->highway-env) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->highway-env) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->highway-env) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->highway-env) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pygame\n",
        "!pip install highway-env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Civ2P__KEUSq"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZwGd9eVEWGY"
      },
      "outputs": [],
      "source": [
        "#Import Libraries\n",
        "from os import truncate\n",
        "import math\n",
        "import gymnasium\n",
        "import highway_env\n",
        "from matplotlib import pyplot as plt\n",
        "import pygame\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from IPython.display import Image, display\n",
        "from collections import deque\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF-2bRYTIjH6"
      },
      "source": [
        "Environment Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y922XK-cIewr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37b3d52e-23e3-44f3-9dd5-6ef930ecc704"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<highway_env.envs.common.action.ContinuousAction at 0x7c872f0bca50>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Configure Environment Conditions\n",
        "config = {\n",
        "    \"lanes_count\": 3,\n",
        "    \"lane_width\": 3.75,\n",
        "    \"observation\": {\n",
        "        \"type\": \"Kinematics\",\n",
        "        \"features\": [\"x\", \"y\", \"vx\", \"vy\", \"heading\", \"lat_off\"]\n",
        "    },\n",
        "    \"action\": {\"type\": \"ContinuousAction\"},\"ego_spawn_random\": True,\n",
        "    \"policy_frequency\": 10,\n",
        "}\n",
        "env = gymnasium.make('highway-v0', render_mode='rgb_array', config=config)\n",
        "frames = []\n",
        "\n",
        "# Action Setup\n",
        "highway_env.envs.common.action.ContinuousAction(env, lateral=True,\n",
        "                                                longitudinal=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HbWr4urTXQ8"
      },
      "outputs": [],
      "source": [
        "# Environment Manager\n",
        "class ENV(env.__class__):\n",
        "  \"\"\"\n",
        "  s = (v,a,x,y,thetha,id,w,c) ∈ S\n",
        "\n",
        "  Lateral Agent State:\n",
        "  Obs Data:\n",
        "  x = vehicle x position (x)\n",
        "  y = vehicle y position (y)\n",
        "  v = vehicle speed (vx)\n",
        "  thetha = yaw angle (heading)\n",
        "\n",
        "  Input:\n",
        "  a = longitudinal acceleration (longitudinal_control)\n",
        "  id = target lane id\n",
        "  w = lane width\n",
        "  c = road curvature\n",
        "\n",
        "  Extra Data:\n",
        "  vy = lateral rate (vy)\n",
        "  delta_lat_deviation = change in lateral deviation (lat_off)\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, obs,a):\n",
        "    self.obs = obs\n",
        "    self.a = a\n",
        "\n",
        "  def ego_state_idm(self):\n",
        "      ax, ay, avx, avy, athetha, _ = self.obs[0]\n",
        "      self._ego_state_idm = {\"x\": ax, \"y\": ay, \"vx\": avx,\"thetha\": athetha,\n",
        "                         \"longitudinal_acceleration\":self.a}\n",
        "\n",
        "      return self._ego_state_idm\n",
        "\n",
        "  def ego_state_agent(self,target_id):\n",
        "      ax, ay, avx, avy, athetha, _ = self.obs[0]\n",
        "      vehicle = env.unwrapped.vehicle\n",
        "\n",
        "      # Control Parameters\n",
        "      self.id = target_id\n",
        "\n",
        "      # Environment Parameters\n",
        "      self.s,_ = vehicle.lane.local_coordinates(vehicle.position)\n",
        "      self.w = vehicle.lane.width\n",
        "      self.c = vehicle.lane.heading_at(np.clip(\n",
        "          vehicle.lane.local_coordinates(vehicle.position)[0],\n",
        "          0, vehicle.lane.length))\n",
        "      self.v = math.sqrt(avx**2+avy**2)\n",
        "\n",
        "      self._ego_state_agent = {\"x\": ax, \"y\": ay, \"vx\": self.v,\"thetha\": athetha,\n",
        "                         \"lane_width\":self.w,\"lane_id\":self.id,\n",
        "                         \"self_curvature\":self.c,\n",
        "                         \"longitudinal_acceleration\":self.a}\n",
        "\n",
        "      return self._ego_state_agent\n",
        "\n",
        "  def longitudinal_lead_state(self):\n",
        "    # Lead State Parameters on the same Lane (5 columns: presence, x, y, vx, vy)\n",
        "    ego_vehicle = env.unwrapped.vehicle\n",
        "    lead_vehicle = env.unwrapped.road.neighbour_vehicles(ego_vehicle, lane_index=ego_vehicle.lane_index)[0]\n",
        "    if lead_vehicle:\n",
        "      gap = lead_vehicle.position[0] - ego_vehicle.position[0]\n",
        "      delta_velocity = ego_vehicle.velocity[0] - lead_vehicle.velocity[0]\n",
        "      self.longitudinal_lead_state = {\"x\": gap, \"vx\": delta_velocity}\n",
        "\n",
        "    else:\n",
        "      self.longitudinal_lead_state = {\"x\": 10, \"vx\": 0}\n",
        "\n",
        "    return self.longitudinal_lead_state\n",
        "\n",
        "  #Reward Function\n",
        "  def reward_function(self, obs_old, obs_new, target_id, w1=1, w2=1, w3=0.05):\n",
        "    \"\"\"\n",
        "    Reward Function:\n",
        "\n",
        "    Acceleration Reward: r_acce = w1*f_acce(a_yaw)\n",
        "    a_yaw = lateral acceleration (self.action)\n",
        "\n",
        "    Rate Reward: r_rate = w2*f_rate(w_yaw)\n",
        "    w_yaw = lateral rate (vy)\n",
        "\n",
        "    Time Reward: r_time = w3*f_time (delta_lat_deviation)\n",
        "    delta_lat_deviation = change in lateral deviation (self.lat_off)\n",
        "\n",
        "    Reward = Cummulative Sum of r_acce + Cummulative Sum of r_rate + Cummulative Sum of r_time\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    self.target_id = (\"0\",\"1\",target_id)\n",
        "    target_lane_object = env.unwrapped.road.network.get_lane(self.target_id)\n",
        "    vehicle_s, _ = env.unwrapped.vehicle.lane.local_coordinates(env.unwrapped.vehicle.position)\n",
        "    _ , self.delta_lat_deviaton = target_lane_object.local_coordinates(env.unwrapped.vehicle.position)\n",
        "\n",
        "    obs = obs_new[0]\n",
        "    obs_old = obs_old[0]\n",
        "\n",
        "    w_yaw = (obs[0] * obs[3] - obs[1] * obs[2]) / (obs[0]**2 + obs[1]**2+1e-8)\n",
        "    w_yaw_old = (obs_old[0] * obs_old[3] - obs_old[1] * obs_old[2]) / (obs_old[0]**2 + obs_old[1])\n",
        "\n",
        "    self.w_acce = (w_yaw-w_yaw_old)*env.unwrapped.config['policy_frequency']\n",
        "\n",
        "    # Acceleration Reward\n",
        "    acce_reward = -1*abs(self.w_acce)\n",
        "\n",
        "    # Rate Reward\n",
        "    rate_reward = -1*abs(w_yaw)\n",
        "\n",
        "    # Time Reward\n",
        "    time_reward = -0.05 * abs(self.delta_lat_deviaton)\n",
        "\n",
        "    # Overall Reward\n",
        "    self.reward = w1*acce_reward + w2*rate_reward + w3*time_reward\n",
        "\n",
        "    return [self.reward, acce_reward, rate_reward, time_reward]\n",
        "\n",
        "  #Acceleration to Steering angle\n",
        "  def steering_angle(self, agent_action,L=1):\n",
        "    \"\"\"\n",
        "    Steering Angle: theta = atan(a_yaw/v^2)\n",
        "    a_yaw = lateral acceleration (agent_action)\n",
        "    v = vehicle speed (vx)\n",
        "    \"\"\"\n",
        "    self.angle = math.atan(L*agent_action/self.ego_state_idm()['vx']**2)\n",
        "\n",
        "    return self.angle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEPNdVET6-mv"
      },
      "source": [
        "Agent Defintion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJ68SiyJTSx7"
      },
      "outputs": [],
      "source": [
        "class Agent(nn.Module):\n",
        "  \"\"\"\n",
        "  s = (x, y, vx, vy, thetha, lane_width, lane_id, self_curvature, longitudinal_acceleration) ∈ S\n",
        "\n",
        "  Lateral Agent State:\n",
        "  x = vehicle x position\n",
        "  y = vehicle y position\n",
        "  vx = vehicle speed (longitudinal)\n",
        "  thetha = yaw angle (heading)\n",
        "  lane_width = width of the lane\n",
        "  lane_id = target lane id\n",
        "  self_curvature = road curvature at the vehicle's position\n",
        "  longitudinal_acceleration = vehicle longitudinal acceleration\n",
        "  \"\"\"\n",
        "  def __init__(self, state_dim):\n",
        "    super(Agent, self).__init__()\n",
        "    self.state_dim = state_dim\n",
        "    # Define Network A & C\n",
        "    self.networkA = nn.Sequential(\n",
        "        nn.Linear(self.state_dim, 100),\n",
        "        nn.Linear(100, 1),\n",
        "        nn.Softplus()\n",
        "    )\n",
        "    self.networkC = nn.Sequential(\n",
        "        nn.Linear(self.state_dim + 1, 100),\n",
        "        nn.Linear(100, 1),\n",
        "    )\n",
        "\n",
        "    # Define Network B\n",
        "    self.networkB1 = nn.Sequential(\n",
        "        nn.Linear(self.state_dim, 150),\n",
        "        nn.Linear(150, 1),\n",
        "    )\n",
        "    self.networkB2 = nn.Sequential(\n",
        "        nn.Linear(self.state_dim, 150),\n",
        "        nn.Linear(150, 1),\n",
        "    )\n",
        "    self.networkB3 = nn.Sequential(\n",
        "        nn.Linear(self.state_dim, 150),\n",
        "        nn.Linear(150, 1),\n",
        "    )\n",
        "\n",
        "  def forward(self, state, a, terminal):\n",
        "    \"\"\"\n",
        "    Q(s,a) = A(s)*(B(s)-a)^2 + C(s)\n",
        "    \"\"\"\n",
        "    if isinstance(state, dict):\n",
        "      state_tensor = torch.tensor(\n",
        "          [state[key] for key in ['vx', 'longitudinal_acceleration', 'x', 'y', 'thetha', 'lane_id', 'lane_width', 'self_curvature']],\n",
        "          dtype=torch.float32\n",
        "      ).unsqueeze(0)\n",
        "    else:\n",
        "        self.state = state\n",
        "    # Use a directly if it's already a tensor\n",
        "    self.a = a if isinstance(a, torch.Tensor) else torch.tensor([[a]], dtype=torch.float32)\n",
        "    self.terminal_condition = terminal if isinstance(terminal, torch.Tensor) else torch.tensor([[terminal]], dtype=torch.float32)\n",
        "\n",
        "    # Output of the Networks\n",
        "    self.A = self.networkA(self.state)\n",
        "    self.C = self.networkC(torch.concat((self.state, self.terminal_condition), dim=1))\n",
        "    self.B = torch.max(self.networkB1(self.state) * self.networkB2(self.state),\n",
        "                         self.networkB3(self.state))\n",
        "\n",
        "    # Q-function Approximation\n",
        "    q_value =self.A * ((self.B - self.a) ** 2) + self.C\n",
        "\n",
        "    return q_value\n",
        "\n",
        "  def action(self, state):\n",
        "      # Convert it to a tensor\n",
        "      if isinstance(state, dict):\n",
        "          state_tensor = torch.tensor(\n",
        "              [state[key] for key in ['vx', 'longitudinal_acceleration', 'x', 'y', 'thetha', 'lane_id', 'lane_width', 'self_curvature']],\n",
        "              dtype=torch.float32\n",
        "          ).unsqueeze(0)\n",
        "      else:\n",
        "          state_tensor = state\n",
        "      self.state = state_tensor\n",
        "      self.B = torch.max(self.networkB1(self.state) * self.networkB2(self.state),\n",
        "                         self.networkB3(self.state))\n",
        "      return self.B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo-7aVU9J681"
      },
      "source": [
        "Experience Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7g2c9HRSJ8Xs"
      },
      "outputs": [],
      "source": [
        "# Buffer Class\n",
        "class Experience_Buffer():\n",
        "  \"\"\"\n",
        "  Define Experience Buffer\n",
        "  \"\"\"\n",
        "  def __init__(self, buffer_size):\n",
        "    self.buffer_size = buffer_size\n",
        "    # Define Buffer as Deque with Limit of Default Size buffer_size elements\n",
        "    self.buffer = deque(maxlen=self.buffer_size)\n",
        "\n",
        "  def add(self, state, action, reward, next_state, terminal_condition):\n",
        "    # Experience is the state transition tensor/vector\n",
        "    state_tensor = torch.tensor(\n",
        "        [state[key] for key in ['vx', 'longitudinal_acceleration', 'x', 'y', 'thetha', 'lane_id', 'lane_width', 'self_curvature']],\n",
        "        dtype=torch.float32\n",
        "    ).unsqueeze(0)\n",
        "    next_state_tensor = torch.tensor(\n",
        "        [next_state[key] for key in ['vx', 'longitudinal_acceleration', 'x', 'y', 'thetha', 'lane_id', 'lane_width', 'self_curvature']],\n",
        "        dtype=torch.float32\n",
        "    ).unsqueeze(0)\n",
        "    action_tensor = torch.tensor([[action]], dtype=torch.float32)\n",
        "    reward_tensor = torch.tensor([[reward]], dtype=torch.float32)\n",
        "    terminal_tensor = torch.tensor([[terminal_condition]], dtype=torch.float32)\n",
        "\n",
        "    self.transition = torch.concat((state_tensor, action_tensor, reward_tensor,\n",
        "                                    next_state_tensor, terminal_tensor), dim=1)\n",
        "    self.buffer.append(self.transition)\n",
        "\n",
        "  def sample_experience(self, batch_size):\n",
        "    # Randomly sample Experience from buffer\n",
        "    batch = random.sample(self.buffer, batch_size)\n",
        "    return batch\n",
        "\n",
        "  def size(self):\n",
        "    # Return the current number of elements in the buffer.\n",
        "    return len(self.buffer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enKF1jUjQxpL"
      },
      "source": [
        "Lateral Controller (Gap Checker)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Gap_Controller(env.__class__):\n",
        "    def __init__(self, obs=None, following_gap_threshold=10.0):\n",
        "        # Optionally store an initial observation if provided.\n",
        "        if obs is not None:\n",
        "            self.obs = obs\n",
        "        # Threshold for safety gap with the following vehicle\n",
        "        self.following_gap_threshold = following_gap_threshold\n",
        "\n",
        "    def lane_checker(self):\n",
        "        \"\"\"\n",
        "        Determines which adjacent lane offers the safest gap for a lane change.\n",
        "        It examines the current lane, the lane to the left, and the lane to the right,\n",
        "        returning the lane index that has the largest safe front gap available.\n",
        "        In addition, if the gap with the following vehicle in the target lane is below the safety\n",
        "        threshold, the lane change is aborted and the current lane is chosen.\n",
        "\n",
        "        Returns:\n",
        "            int: The target lane index (the lane with the largest safe gap, or the current lane)\n",
        "                 if the candidate lane's following vehicle gap is unsafe.\n",
        "        \"\"\"\n",
        "        lane_number = len(env.unwrapped.road.network.lanes_list())\n",
        "        ego = env.unwrapped.vehicle\n",
        "\n",
        "        # Initialize front gap values with a default that indicates an unsafe or non-existent gap.\n",
        "        gap_current_front = -float('inf')\n",
        "        gap_right_front = -float('inf')\n",
        "        gap_left_front = -float('inf')\n",
        "\n",
        "        # Initialize following gap values (gap between ego and the vehicle behind)\n",
        "        gap_current_follower = float('inf')\n",
        "        gap_right_follower = float('inf')\n",
        "        gap_left_follower = float('inf')\n",
        "\n",
        "        # Get vehicle neighbours in current lane:\n",
        "        neighbours_current = env.unwrapped.road.neighbour_vehicles(ego, ego.lane_index)\n",
        "        # neighbours_current[0] is the vehicle ahead and [1] is the following vehicle.\n",
        "        if neighbours_current:\n",
        "            if neighbours_current[0]:\n",
        "                gap_current_front = neighbours_current[0].position[0] - ego.position[0]\n",
        "            if len(neighbours_current) > 1 and neighbours_current[1]:\n",
        "                gap_current_follower = ego.position[0] - neighbours_current[1].position[0]\n",
        "\n",
        "        # Compute the left and right lane indices.\n",
        "        current_lane = list(ego.lane_index)\n",
        "        if current_lane[2] > 0:\n",
        "            lane_left = (current_lane[0], current_lane[1], current_lane[2] - 1)\n",
        "        else:\n",
        "            lane_left = ego.lane_index\n",
        "\n",
        "        if current_lane[2] < (lane_number - 1):\n",
        "            lane_right = (current_lane[0], current_lane[1], current_lane[2] + 1)\n",
        "        else:\n",
        "            lane_right = ego.lane_index\n",
        "\n",
        "        # Retrieve neighbour vehicles for the right lane.\n",
        "        neighbours_right = env.unwrapped.road.neighbour_vehicles(ego, lane_index=lane_right)\n",
        "        if neighbours_right:\n",
        "            if neighbours_right[0]:\n",
        "                gap_right_front = neighbours_right[0].position[0] - ego.position[0]\n",
        "            if len(neighbours_right) > 1 and neighbours_right[1]:\n",
        "                gap_right_follower = ego.position[0] - neighbours_right[1].position[0]\n",
        "\n",
        "        # Retrieve neighbour vehicles for the left lane.\n",
        "        neighbours_left = env.unwrapped.road.neighbour_vehicles(ego, lane_index=lane_left)\n",
        "        if neighbours_left:\n",
        "            if neighbours_left[0]:\n",
        "                gap_left_front = neighbours_left[0].position[0] - ego.position[0]\n",
        "            if len(neighbours_left) > 1 and neighbours_left[1]:\n",
        "                gap_left_follower = ego.position[0] - neighbours_left[1].position[0]\n",
        "\n",
        "        # Compare the front gaps: current, right, and left.\n",
        "        # The candidate lane is chosen based on the largest front gap.\n",
        "        front_gaps = [gap_current_front, gap_right_front, gap_left_front]\n",
        "        best_index = np.argmax(front_gaps)\n",
        "\n",
        "        # Determine the target lane index based on the best candidate.\n",
        "        # best_index: 0 => current lane, 1 => right lane, 2 => left lane.\n",
        "        if best_index == 1:\n",
        "            candidate_lane = lane_right\n",
        "            candidate_follower_gap = gap_right_follower\n",
        "        elif best_index == 2:\n",
        "            candidate_lane = lane_left\n",
        "            candidate_follower_gap = gap_left_follower\n",
        "        else:\n",
        "            candidate_lane = ego.lane_index\n",
        "            candidate_follower_gap = gap_current_follower  # in current lane, we don't enforce follower gap condition\n",
        "\n",
        "        # Check if the candidate lane (if different from the current lane)\n",
        "        # meets the follower gap condition.\n",
        "        if candidate_lane != ego.lane_index:\n",
        "            if candidate_follower_gap < self.following_gap_threshold:\n",
        "                # The follower gap is too small; do not change lanes.\n",
        "                target_lane_id = ego.lane_index[2]\n",
        "            else:\n",
        "                target_lane_id = candidate_lane[2]\n",
        "        else:\n",
        "            target_lane_id = ego.lane_index[2]\n",
        "\n",
        "        return target_lane_id\n"
      ],
      "metadata": {
        "id": "ydU55ZY5eX4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDNPakyp7APE"
      },
      "source": [
        "Longitudinal Controller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU_TDnfv7CzO"
      },
      "outputs": [],
      "source": [
        "class IDM():\n",
        "    ''' Intelligent Driving Model for Longitudinal Control\n",
        "\n",
        "    Control parameters:\n",
        "      a: maximum acceleration\n",
        "      b: comfortable deceleration\n",
        "      delta: acceleration exponent\n",
        "      T: safe time headway\n",
        "\n",
        "    Parameters:\n",
        "      s0: minimum gap\n",
        "      v0: desired speed\n",
        "\n",
        "      a (Maximum Acceleration): How fast the vehicle can speed up (m/s²).\n",
        "      b (Comfortable Deceleration): How smoothly the vehicle slows down (m/s²).\n",
        "      δ (Acceleration Exponent): The non-linearity factor in acceleration.\n",
        "      T (Safe Time Headway): The desired minimum following time gap (s).\n",
        "\n",
        "    Input variables:\n",
        "      s: current gap\n",
        "      v: current vehicle speed\n",
        "      delta_v: relative speed (difference between the vehicle's speed and the leading vehicle's speed)\n",
        "    '''\n",
        "\n",
        "    def __init__(self, desired_parameters, control_parameters):\n",
        "        # Unpack initial parameters: [s0, v0]\n",
        "        self.s0, self.v0 = desired_parameters\n",
        "        # Unpack control parameters: [a, b, delta, T]\n",
        "        self.a, self.b, self.delta, self.T = control_parameters\n",
        "\n",
        "    def longitudinal_controller(self, input_variables):\n",
        "        # Unpack input variables: [s, v, delta_v]\n",
        "        s, v, delta_v = input_variables\n",
        "\n",
        "        # Small epsilon to account for very small gaps and avoid division by zero\n",
        "        epsilon = 1e-6\n",
        "\n",
        "        # Desired gap: s* = s0 + v*T + (v * delta_v) / (2 * sqrt(a * b))\n",
        "        desired_gap = self.s0 + max(0, v * self.T + ((v * delta_v) / (2 * math.sqrt(self.a * self.b))))\n",
        "\n",
        "        # IDM acceleration: a_IDM = a * [ 1 - (v / v0)^delta - (s* / s)^2 ]\n",
        "        acceleration = self.a * (1 - (v / self.v0)**self.delta - (desired_gap / (s + epsilon))**2)\n",
        "\n",
        "        return acceleration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21gxd9D8au1O"
      },
      "source": [
        "Initialize Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPl1lptqKdAy"
      },
      "outputs": [],
      "source": [
        "# Initialize Environment\n",
        "obs, _ = env.reset()\n",
        "\n",
        "# Initialize Environment Manager and Reward\n",
        "state_manager = ENV(obs,obs[0][2])\n",
        "\n",
        "# State Manager for Ego and Lead State\n",
        "ego_state_idm = state_manager.ego_state_idm()\n",
        "lead_state = state_manager.longitudinal_lead_state()\n",
        "\n",
        "# Initial Longitundinal Positions\n",
        "ego_position_idm = ego_state_idm['x']\n",
        "lead_position_idm = lead_state['x']\n",
        "\n",
        "# Initial Velocities (using vx for longitudinal control)\n",
        "ego_velocity_idm = ego_state_idm['vx']\n",
        "lead_velocity_idm = lead_state['vx']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZvAgaQfIyK6"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CartPole (Test)"
      ],
      "metadata": {
        "id": "v-t_GJwAlEFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ────────────────────────────────  ENV  ───────────────────────────────────────\n",
        "import gymnasium\n",
        "env_name          = \"CartPole-v1\"\n",
        "env               = gymnasium.make(env_name, render_mode=None)\n",
        "state_dim         = env.observation_space.shape[0]      # 2  (pos, vel)\n",
        "act_low, act_high = 0, env.action_space.n - 1\n",
        "device            = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ─────────────────────────────  HYPERPARAMETERS  ───────────────────────────────────────\n",
        "gamma                 = 0.95\n",
        "batch_size            = 10000\n",
        "update_per_n_steps    = 10000\n",
        "num_episodes          = 10000\n",
        "max_steps_per_episode = 999\n",
        "lr                    = 1e-2\n",
        "replay_buffer_size    = batch_size*5\n",
        "min_replay_size       = batch_size*2\n",
        "plot_every            = 10\n",
        "\n",
        "# --- Epsilon-Greedy Parameters ---\n",
        "epsilon_start         = 0.99\n",
        "epsilon_end           = 0.05\n",
        "epsilon_decay_steps   = 7500\n",
        "epsilon_decay_rate    = (epsilon_start - epsilon_end) / epsilon_decay_steps\n",
        "epsilon               = epsilon_start\n",
        "\n",
        "episode_returns = []\n",
        "loss_history    = []\n",
        "global_step     = 0\n",
        "\n",
        "# ─────────────────────────────  NETWORKS  ─────────────────────────────────────\n",
        "policy_net = Agent(state_dim).to(device)\n",
        "target_net = Agent(state_dim).to(device)\n",
        "\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer  = optim.Adam(policy_net.parameters(), lr=lr)\n",
        "loss_fn    = nn.MSELoss()\n",
        "\n",
        "# ───────────────────────────  REPLAY BUFFER  ─────────────────────────────────\n",
        "class ReplayBuf:\n",
        "    def __init__(self, capacity):\n",
        "        self.buf = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, s, a, r, s2, d):\n",
        "        s_tensor = s.squeeze(0).cpu()\n",
        "        s2_tensor = s2.squeeze(0).cpu()\n",
        "        self.buf.append((s_tensor, a, r, s2_tensor, d))\n",
        "\n",
        "    def sample(self, n):\n",
        "        if len(self.buf) < n: return None\n",
        "        batch = random.sample(self.buf, n)\n",
        "\n",
        "        s, a, r, s2, d = zip(*batch)\n",
        "        s_batch = torch.stack(s).to(device)\n",
        "        a_batch = torch.tensor(a, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "        r_batch = torch.tensor(r, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "        s2_batch = torch.stack(s2).to(device)\n",
        "        d_batch = torch.tensor(d, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "        return s_batch, a_batch, r_batch, s2_batch, d_batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buf)\n",
        "\n",
        "replay = ReplayBuf(replay_buffer_size)\n",
        "\n",
        "# ─────────────────────────── TRAINING LOOP ────────────────────────────────────\n",
        "for ep in range(1, num_episodes + 1):\n",
        "    obs, _ = env.reset()\n",
        "    state  = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "    done   = False\n",
        "    steps  = 0\n",
        "    ep_ret = 0.0\n",
        "    last_step_loss = None\n",
        "    policy_net.train()\n",
        "\n",
        "    while not done and steps < max_steps_per_episode:\n",
        "        steps       += 1\n",
        "        global_step += 1\n",
        "\n",
        "        # Epsilon-greedy action selection\n",
        "        if random.random() < epsilon:\n",
        "            action_val = random.randint(0, 1)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                terminal_tensor = torch.tensor([[0.0]], device=device)\n",
        "                q_value_0 = policy_net(state, torch.tensor([[0.0]], device=device), terminal_tensor)\n",
        "                q_value_1 = policy_net(state, torch.tensor([[1.0]], device=device), terminal_tensor)\n",
        "                q_values = torch.cat((q_value_0, q_value_1), dim=1)\n",
        "            action_val = torch.argmax(q_values).item()\n",
        "\n",
        "        # Step environment\n",
        "        obs2, reward, terminated, truncated, _ = env.step(action_val)\n",
        "        done       = terminated or truncated\n",
        "        ep_ret    += reward\n",
        "\n",
        "        next_state = torch.tensor(obs2, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "        replay.add(state, float(action_val), float(reward), next_state, float(done))\n",
        "        state = next_state\n",
        "\n",
        "        # Learning step\n",
        "        if len(replay) >= min_replay_size:\n",
        "            sample = replay.sample(batch_size)\n",
        "            if sample is not None:\n",
        "                s_b, a_b, r_b, s2_b, d_b = sample\n",
        "                q_curr_b = policy_net(s_b, a_b, d_b)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    q_next_val = target_net.networkC(torch.concat((s2_b, d_b), dim=1))\n",
        "                    q_target_b = r_b + gamma * q_next_val * (1 - d_b)\n",
        "\n",
        "                loss = loss_fn(q_curr_b, q_target_b)\n",
        "                last_step_loss = loss.item()\n",
        "                loss_history.append(loss.item())\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Decay epsilon after training update\n",
        "                epsilon = max(epsilon_end, epsilon - epsilon_decay_rate)\n",
        "\n",
        "        # Update target network\n",
        "        if global_step % update_per_n_steps == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Track returns and print\n",
        "    episode_returns.append(ep_ret)\n",
        "    avg_return = np.mean(episode_returns[-100:])\n",
        "    loss_str_repr = f\"{last_step_loss:.4f}\" if last_step_loss is not None else \"N/A\"\n",
        "\n",
        "    print(f\"Ep {ep:4}/{num_episodes} │ Steps:{steps:4} │ Ep Return:{ep_ret:8.2f} \"\n",
        "          f\"│ Avg Return (100 ep):{avg_return:8.2f} │ Epsilon:{epsilon:.3f} \"\n",
        "          f\"│ Buffer:{len(replay):6} │ Loss:{loss_str_repr}\")\n",
        "\n",
        "# Plot results\n",
        "plt.plot(episode_returns)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Return\")\n",
        "plt.title(\"Episode Returns over Time\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss over Time\")\n",
        "plt.show()\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "prVGADshVUaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Always Active (Option 1)"
      ],
      "metadata": {
        "id": "IhoITfrZ_nOt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYA-H09sIw2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91b77b2c-48ef-4c2f-e495-ce4139e95a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**** Target network updated at episode 0 ****\n",
            "Ep    0 │ Global Steps:     8/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -5.68 │ Epsilon:1.000 │ Buffer:     8 │ Loss:N/A\n",
            "Ep    1 │ Global Steps:    11/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.65 │ Epsilon:1.000 │ Buffer:    11 │ Loss:N/A\n",
            "Ep    2 │ Global Steps:    16/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:    16 │ Loss:N/A\n",
            "Ep    3 │ Global Steps:    19/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.03 │ Epsilon:1.000 │ Buffer:    19 │ Loss:N/A\n",
            "Ep    4 │ Global Steps:    24/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.07 │ Epsilon:1.000 │ Buffer:    24 │ Loss:N/A\n",
            "Ep    5 │ Global Steps:    32/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.51 │ Epsilon:1.000 │ Buffer:    32 │ Loss:N/A\n",
            "Ep    6 │ Global Steps:    35/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:    35 │ Loss:N/A\n",
            "Ep    7 │ Global Steps:    38/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.03 │ Epsilon:1.000 │ Buffer:    38 │ Loss:N/A\n",
            "Ep    8 │ Global Steps:    41/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -2.87 │ Epsilon:1.000 │ Buffer:    41 │ Loss:N/A\n",
            "Ep    9 │ Global Steps:    46/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -2.91 │ Epsilon:1.000 │ Buffer:    46 │ Loss:N/A\n",
            "Ep   10 │ Global Steps:    51/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -2.94 │ Epsilon:1.000 │ Buffer:    51 │ Loss:N/A\n",
            "Ep   11 │ Global Steps:    59/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.16 │ Epsilon:1.000 │ Buffer:    59 │ Loss:N/A\n",
            "Ep   12 │ Global Steps:    62/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.04 │ Epsilon:1.000 │ Buffer:    62 │ Loss:N/A\n",
            "Ep   13 │ Global Steps:    70/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:    70 │ Loss:N/A\n",
            "Ep   14 │ Global Steps:    78/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:    78 │ Loss:N/A\n",
            "Ep   15 │ Global Steps:    83/10000000 │ Ep Return:   -3.22 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:    83 │ Loss:N/A\n",
            "Ep   16 │ Global Steps:    91/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:    91 │ Loss:N/A\n",
            "Ep   17 │ Global Steps:    96/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:    96 │ Loss:N/A\n",
            "Ep   18 │ Global Steps:   104/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:   104 │ Loss:N/A\n",
            "Ep   19 │ Global Steps:   107/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:   107 │ Loss:N/A\n",
            "Ep   20 │ Global Steps:   110/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:   110 │ Loss:N/A\n",
            "Ep   21 │ Global Steps:   118/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.53 │ Epsilon:1.000 │ Buffer:   118 │ Loss:N/A\n",
            "Ep   22 │ Global Steps:   123/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:   123 │ Loss:N/A\n",
            "Ep   23 │ Global Steps:   128/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:   128 │ Loss:N/A\n",
            "Ep   24 │ Global Steps:   133/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.49 │ Epsilon:1.000 │ Buffer:   133 │ Loss:N/A\n",
            "Ep   25 │ Global Steps:   138/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:   138 │ Loss:N/A\n",
            "Ep   26 │ Global Steps:   146/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:   146 │ Loss:N/A\n",
            "Ep   27 │ Global Steps:   151/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:   151 │ Loss:N/A\n",
            "Ep   28 │ Global Steps:   154/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:   154 │ Loss:N/A\n",
            "Ep   29 │ Global Steps:   162/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:   162 │ Loss:N/A\n",
            "Ep   30 │ Global Steps:   170/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.63 │ Epsilon:1.000 │ Buffer:   170 │ Loss:N/A\n",
            "Ep   31 │ Global Steps:   175/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.61 │ Epsilon:1.000 │ Buffer:   175 │ Loss:N/A\n",
            "Ep   32 │ Global Steps:   178/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:   178 │ Loss:N/A\n",
            "Ep   33 │ Global Steps:   181/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:   181 │ Loss:N/A\n",
            "Ep   34 │ Global Steps:   184/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.44 │ Epsilon:1.000 │ Buffer:   184 │ Loss:N/A\n",
            "Ep   35 │ Global Steps:   187/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:   187 │ Loss:N/A\n",
            "Ep   36 │ Global Steps:   195/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:   195 │ Loss:N/A\n",
            "Ep   37 │ Global Steps:   203/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.51 │ Epsilon:1.000 │ Buffer:   203 │ Loss:N/A\n",
            "Ep   38 │ Global Steps:   208/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:   208 │ Loss:N/A\n",
            "Ep   39 │ Global Steps:   216/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:   216 │ Loss:N/A\n",
            "Ep   40 │ Global Steps:   224/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.61 │ Epsilon:1.000 │ Buffer:   224 │ Loss:N/A\n",
            "Ep   41 │ Global Steps:   229/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.60 │ Epsilon:1.000 │ Buffer:   229 │ Loss:N/A\n",
            "Ep   42 │ Global Steps:   234/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:   234 │ Loss:N/A\n",
            "Ep   43 │ Global Steps:   242/10000000 │ Ep Return:   -5.65 │ Avg Return (100 ep):   -3.64 │ Epsilon:1.000 │ Buffer:   242 │ Loss:N/A\n",
            "Ep   44 │ Global Steps:   247/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.63 │ Epsilon:1.000 │ Buffer:   247 │ Loss:N/A\n",
            "Ep   45 │ Global Steps:   255/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.67 │ Epsilon:1.000 │ Buffer:   255 │ Loss:N/A\n",
            "Ep   46 │ Global Steps:   260/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.66 │ Epsilon:1.000 │ Buffer:   260 │ Loss:N/A\n",
            "Ep   47 │ Global Steps:   265/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.65 │ Epsilon:1.000 │ Buffer:   265 │ Loss:N/A\n",
            "Ep   48 │ Global Steps:   268/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.61 │ Epsilon:1.000 │ Buffer:   268 │ Loss:N/A\n",
            "Ep   49 │ Global Steps:   276/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.65 │ Epsilon:1.000 │ Buffer:   276 │ Loss:N/A\n",
            "Ep   50 │ Global Steps:   281/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.64 │ Epsilon:1.000 │ Buffer:   281 │ Loss:N/A\n",
            "Ep   51 │ Global Steps:   289/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:   289 │ Loss:N/A\n",
            "Ep   52 │ Global Steps:   294/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.67 │ Epsilon:1.000 │ Buffer:   294 │ Loss:N/A\n",
            "Ep   53 │ Global Steps:   299/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.67 │ Epsilon:1.000 │ Buffer:   299 │ Loss:N/A\n",
            "Ep   54 │ Global Steps:   304/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.66 │ Epsilon:1.000 │ Buffer:   304 │ Loss:N/A\n",
            "Ep   55 │ Global Steps:   312/10000000 │ Ep Return:   -5.65 │ Avg Return (100 ep):   -3.69 │ Epsilon:1.000 │ Buffer:   312 │ Loss:N/A\n",
            "Ep   56 │ Global Steps:   317/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:   317 │ Loss:N/A\n",
            "Ep   57 │ Global Steps:   320/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.65 │ Epsilon:1.000 │ Buffer:   320 │ Loss:N/A\n",
            "Ep   58 │ Global Steps:   325/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.64 │ Epsilon:1.000 │ Buffer:   325 │ Loss:N/A\n",
            "Ep   59 │ Global Steps:   330/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.63 │ Epsilon:1.000 │ Buffer:   330 │ Loss:N/A\n",
            "Ep   60 │ Global Steps:   335/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.63 │ Epsilon:1.000 │ Buffer:   335 │ Loss:N/A\n",
            "Ep   61 │ Global Steps:   340/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:   340 │ Loss:N/A\n",
            "Ep   62 │ Global Steps:   348/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.65 │ Epsilon:1.000 │ Buffer:   348 │ Loss:N/A\n",
            "Ep   63 │ Global Steps:   356/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:   356 │ Loss:N/A\n",
            "Ep   64 │ Global Steps:   359/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.65 │ Epsilon:1.000 │ Buffer:   359 │ Loss:N/A\n",
            "Ep   65 │ Global Steps:   367/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:   367 │ Loss:N/A\n",
            "Ep   66 │ Global Steps:   372/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:   372 │ Loss:N/A\n",
            "Ep   67 │ Global Steps:   377/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.67 │ Epsilon:1.000 │ Buffer:   377 │ Loss:N/A\n",
            "Ep   68 │ Global Steps:   380/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.64 │ Epsilon:1.000 │ Buffer:   380 │ Loss:N/A\n",
            "Ep   69 │ Global Steps:   388/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.67 │ Epsilon:1.000 │ Buffer:   388 │ Loss:N/A\n",
            "Ep   70 │ Global Steps:   396/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:   396 │ Loss:N/A\n",
            "Ep   71 │ Global Steps:   401/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.69 │ Epsilon:1.000 │ Buffer:   401 │ Loss:N/A\n",
            "Ep   72 │ Global Steps:   409/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:   409 │ Loss:N/A\n",
            "Ep   73 │ Global Steps:   412/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.69 │ Epsilon:1.000 │ Buffer:   412 │ Loss:N/A\n",
            "Ep   74 │ Global Steps:   417/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:   417 │ Loss:N/A\n",
            "Ep   75 │ Global Steps:   420/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.66 │ Epsilon:1.000 │ Buffer:   420 │ Loss:N/A\n",
            "Ep   76 │ Global Steps:   425/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.65 │ Epsilon:1.000 │ Buffer:   425 │ Loss:N/A\n",
            "Ep   77 │ Global Steps:   433/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:   433 │ Loss:N/A\n",
            "Ep   78 │ Global Steps:   441/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:   441 │ Loss:N/A\n",
            "Ep   79 │ Global Steps:   444/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:   444 │ Loss:N/A\n",
            "Ep   80 │ Global Steps:   449/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.67 │ Epsilon:1.000 │ Buffer:   449 │ Loss:N/A\n",
            "Ep   81 │ Global Steps:   454/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.66 │ Epsilon:1.000 │ Buffer:   454 │ Loss:N/A\n",
            "Ep   82 │ Global Steps:   457/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.64 │ Epsilon:1.000 │ Buffer:   457 │ Loss:N/A\n",
            "Ep   83 │ Global Steps:   460/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:   460 │ Loss:N/A\n",
            "Ep   84 │ Global Steps:   465/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.61 │ Epsilon:1.000 │ Buffer:   465 │ Loss:N/A\n",
            "Ep   85 │ Global Steps:   468/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:   468 │ Loss:N/A\n",
            "Ep   86 │ Global Steps:   471/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:   471 │ Loss:N/A\n",
            "Ep   87 │ Global Steps:   474/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.54 │ Epsilon:1.000 │ Buffer:   474 │ Loss:N/A\n",
            "Ep   88 │ Global Steps:   482/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.57 │ Epsilon:1.000 │ Buffer:   482 │ Loss:N/A\n",
            "Ep   89 │ Global Steps:   490/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:   490 │ Loss:N/A\n",
            "Ep   90 │ Global Steps:   493/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.57 │ Epsilon:1.000 │ Buffer:   493 │ Loss:N/A\n",
            "Ep   91 │ Global Steps:   501/10000000 │ Ep Return:   -5.72 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:   501 │ Loss:N/A\n",
            "Ep   92 │ Global Steps:   506/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:   506 │ Loss:N/A\n",
            "Ep   93 │ Global Steps:   511/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.58 │ Epsilon:1.000 │ Buffer:   511 │ Loss:N/A\n",
            "Ep   94 │ Global Steps:   514/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:   514 │ Loss:N/A\n",
            "Ep   95 │ Global Steps:   519/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:   519 │ Loss:N/A\n",
            "Ep   96 │ Global Steps:   524/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:   524 │ Loss:N/A\n",
            "Ep   97 │ Global Steps:   529/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:   529 │ Loss:N/A\n",
            "Ep   98 │ Global Steps:   534/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:   534 │ Loss:N/A\n",
            "Ep   99 │ Global Steps:   537/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.53 │ Epsilon:1.000 │ Buffer:   537 │ Loss:N/A\n",
            "Ep  100 │ Global Steps:   542/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:   542 │ Loss:N/A\n",
            "Ep  101 │ Global Steps:   545/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:   545 │ Loss:N/A\n",
            "Ep  102 │ Global Steps:   548/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.49 │ Epsilon:1.000 │ Buffer:   548 │ Loss:N/A\n",
            "Ep  103 │ Global Steps:   553/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:   553 │ Loss:N/A\n",
            "Ep  104 │ Global Steps:   558/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:   558 │ Loss:N/A\n",
            "Ep  105 │ Global Steps:   561/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:   561 │ Loss:N/A\n",
            "Ep  106 │ Global Steps:   566/10000000 │ Ep Return:   -3.22 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:   566 │ Loss:N/A\n",
            "Ep  107 │ Global Steps:   571/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.49 │ Epsilon:1.000 │ Buffer:   571 │ Loss:N/A\n",
            "Ep  108 │ Global Steps:   574/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.49 │ Epsilon:1.000 │ Buffer:   574 │ Loss:N/A\n",
            "Ep  109 │ Global Steps:   582/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:   582 │ Loss:N/A\n",
            "Ep  110 │ Global Steps:   585/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:   585 │ Loss:N/A\n",
            "Ep  111 │ Global Steps:   588/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:   588 │ Loss:N/A\n",
            "Ep  112 │ Global Steps:   591/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:   591 │ Loss:N/A\n",
            "Ep  113 │ Global Steps:   594/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:   594 │ Loss:N/A\n",
            "Ep  114 │ Global Steps:   602/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:   602 │ Loss:N/A\n",
            "Ep  115 │ Global Steps:   607/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:   607 │ Loss:N/A\n",
            "Ep  116 │ Global Steps:   612/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:   612 │ Loss:N/A\n",
            "Ep  117 │ Global Steps:   615/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:   615 │ Loss:N/A\n",
            "Ep  118 │ Global Steps:   618/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:   618 │ Loss:N/A\n",
            "Ep  119 │ Global Steps:   626/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:   626 │ Loss:N/A\n",
            "Ep  120 │ Global Steps:   629/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:   629 │ Loss:N/A\n",
            "Ep  121 │ Global Steps:   632/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:   632 │ Loss:N/A\n",
            "Ep  122 │ Global Steps:   635/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:   635 │ Loss:N/A\n",
            "Ep  123 │ Global Steps:   643/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:   643 │ Loss:N/A\n",
            "Ep  124 │ Global Steps:   651/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:   651 │ Loss:N/A\n",
            "Ep  125 │ Global Steps:   659/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:   659 │ Loss:N/A\n",
            "Ep  126 │ Global Steps:   664/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:   664 │ Loss:N/A\n",
            "Ep  127 │ Global Steps:   667/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:   667 │ Loss:N/A\n",
            "Ep  128 │ Global Steps:   670/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:   670 │ Loss:N/A\n",
            "Ep  129 │ Global Steps:   675/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:   675 │ Loss:N/A\n",
            "Ep  130 │ Global Steps:   680/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:   680 │ Loss:N/A\n",
            "Ep  131 │ Global Steps:   685/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:   685 │ Loss:N/A\n",
            "Ep  132 │ Global Steps:   693/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:   693 │ Loss:N/A\n",
            "Ep  133 │ Global Steps:   696/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:   696 │ Loss:N/A\n",
            "Ep  134 │ Global Steps:   699/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:   699 │ Loss:N/A\n",
            "Ep  135 │ Global Steps:   702/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:   702 │ Loss:N/A\n",
            "Ep  136 │ Global Steps:   707/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:   707 │ Loss:N/A\n",
            "Ep  137 │ Global Steps:   712/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:   712 │ Loss:N/A\n",
            "Ep  138 │ Global Steps:   720/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:   720 │ Loss:N/A\n",
            "Ep  139 │ Global Steps:   723/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:   723 │ Loss:N/A\n",
            "Ep  140 │ Global Steps:   726/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:   726 │ Loss:N/A\n",
            "Ep  141 │ Global Steps:   729/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:   729 │ Loss:N/A\n",
            "Ep  142 │ Global Steps:   732/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:   732 │ Loss:N/A\n",
            "Ep  143 │ Global Steps:   737/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.19 │ Epsilon:1.000 │ Buffer:   737 │ Loss:N/A\n",
            "Ep  144 │ Global Steps:   742/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.19 │ Epsilon:1.000 │ Buffer:   742 │ Loss:N/A\n",
            "Ep  145 │ Global Steps:   747/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.16 │ Epsilon:1.000 │ Buffer:   747 │ Loss:N/A\n",
            "Ep  146 │ Global Steps:   750/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.15 │ Epsilon:1.000 │ Buffer:   750 │ Loss:N/A\n",
            "Ep  147 │ Global Steps:   755/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.15 │ Epsilon:1.000 │ Buffer:   755 │ Loss:N/A\n",
            "Ep  148 │ Global Steps:   758/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.15 │ Epsilon:1.000 │ Buffer:   758 │ Loss:N/A\n",
            "Ep  149 │ Global Steps:   766/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.15 │ Epsilon:1.000 │ Buffer:   766 │ Loss:N/A\n",
            "Ep  150 │ Global Steps:   771/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.15 │ Epsilon:1.000 │ Buffer:   771 │ Loss:N/A\n",
            "Ep  151 │ Global Steps:   776/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.12 │ Epsilon:1.000 │ Buffer:   776 │ Loss:N/A\n",
            "Ep  152 │ Global Steps:   779/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.11 │ Epsilon:1.000 │ Buffer:   779 │ Loss:N/A\n",
            "Ep  153 │ Global Steps:   784/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.10 │ Epsilon:1.000 │ Buffer:   784 │ Loss:N/A\n",
            "Ep  154 │ Global Steps:   792/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.13 │ Epsilon:1.000 │ Buffer:   792 │ Loss:N/A\n",
            "Ep  155 │ Global Steps:   800/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.13 │ Epsilon:1.000 │ Buffer:   800 │ Loss:N/A\n",
            "Ep  156 │ Global Steps:   803/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.11 │ Epsilon:1.000 │ Buffer:   803 │ Loss:N/A\n",
            "Ep  157 │ Global Steps:   806/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.11 │ Epsilon:1.000 │ Buffer:   806 │ Loss:N/A\n",
            "Ep  158 │ Global Steps:   811/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.11 │ Epsilon:1.000 │ Buffer:   811 │ Loss:N/A\n",
            "Ep  159 │ Global Steps:   816/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.11 │ Epsilon:1.000 │ Buffer:   816 │ Loss:N/A\n",
            "Ep  160 │ Global Steps:   821/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.11 │ Epsilon:1.000 │ Buffer:   821 │ Loss:N/A\n",
            "Ep  161 │ Global Steps:   826/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.11 │ Epsilon:1.000 │ Buffer:   826 │ Loss:N/A\n",
            "Ep  162 │ Global Steps:   834/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.11 │ Epsilon:1.000 │ Buffer:   834 │ Loss:N/A\n",
            "Ep  163 │ Global Steps:   839/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.09 │ Epsilon:1.000 │ Buffer:   839 │ Loss:N/A\n",
            "Ep  164 │ Global Steps:   847/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.13 │ Epsilon:1.000 │ Buffer:   847 │ Loss:N/A\n",
            "Ep  165 │ Global Steps:   855/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.13 │ Epsilon:1.000 │ Buffer:   855 │ Loss:N/A\n",
            "Ep  166 │ Global Steps:   863/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.15 │ Epsilon:1.000 │ Buffer:   863 │ Loss:N/A\n",
            "Ep  167 │ Global Steps:   868/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.15 │ Epsilon:1.000 │ Buffer:   868 │ Loss:N/A\n",
            "Ep  168 │ Global Steps:   871/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.15 │ Epsilon:1.000 │ Buffer:   871 │ Loss:N/A\n",
            "Ep  169 │ Global Steps:   879/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.15 │ Epsilon:1.000 │ Buffer:   879 │ Loss:N/A\n",
            "Ep  170 │ Global Steps:   882/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.11 │ Epsilon:1.000 │ Buffer:   882 │ Loss:N/A\n",
            "Ep  171 │ Global Steps:   890/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.14 │ Epsilon:1.000 │ Buffer:   890 │ Loss:N/A\n",
            "Ep  172 │ Global Steps:   895/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.11 │ Epsilon:1.000 │ Buffer:   895 │ Loss:N/A\n",
            "Ep  173 │ Global Steps:   898/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.11 │ Epsilon:1.000 │ Buffer:   898 │ Loss:N/A\n",
            "Ep  174 │ Global Steps:   901/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.10 │ Epsilon:1.000 │ Buffer:   901 │ Loss:N/A\n",
            "Ep  175 │ Global Steps:   909/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.14 │ Epsilon:1.000 │ Buffer:   909 │ Loss:N/A\n",
            "Ep  176 │ Global Steps:   914/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.14 │ Epsilon:1.000 │ Buffer:   914 │ Loss:N/A\n",
            "Ep  177 │ Global Steps:   919/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.11 │ Epsilon:1.000 │ Buffer:   919 │ Loss:N/A\n",
            "Ep  178 │ Global Steps:   924/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.09 │ Epsilon:1.000 │ Buffer:   924 │ Loss:N/A\n",
            "Ep  179 │ Global Steps:   929/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.10 │ Epsilon:1.000 │ Buffer:   929 │ Loss:N/A\n",
            "Ep  180 │ Global Steps:   934/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.10 │ Epsilon:1.000 │ Buffer:   934 │ Loss:N/A\n",
            "Ep  181 │ Global Steps:   939/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.10 │ Epsilon:1.000 │ Buffer:   939 │ Loss:N/A\n",
            "Ep  182 │ Global Steps:   942/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.10 │ Epsilon:1.000 │ Buffer:   942 │ Loss:N/A\n",
            "Ep  183 │ Global Steps:   950/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.14 │ Epsilon:1.000 │ Buffer:   950 │ Loss:N/A\n",
            "Ep  184 │ Global Steps:   955/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.14 │ Epsilon:1.000 │ Buffer:   955 │ Loss:N/A\n",
            "Ep  185 │ Global Steps:   963/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.18 │ Epsilon:1.000 │ Buffer:   963 │ Loss:N/A\n",
            "Ep  186 │ Global Steps:   966/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.18 │ Epsilon:1.000 │ Buffer:   966 │ Loss:N/A\n",
            "Ep  187 │ Global Steps:   971/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:   971 │ Loss:N/A\n",
            "Ep  188 │ Global Steps:   979/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:   979 │ Loss:N/A\n",
            "Ep  189 │ Global Steps:   984/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.18 │ Epsilon:1.000 │ Buffer:   984 │ Loss:N/A\n",
            "Ep  190 │ Global Steps:   992/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:   992 │ Loss:N/A\n",
            "Ep  191 │ Global Steps:   995/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.18 │ Epsilon:1.000 │ Buffer:   995 │ Loss:N/A\n",
            "Ep  192 │ Global Steps:   998/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.16 │ Epsilon:1.000 │ Buffer:   998 │ Loss:N/A\n",
            "Ep  193 │ Global Steps:  1001/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.14 │ Epsilon:1.000 │ Buffer:  1001 │ Loss:N/A\n",
            "Ep  194 │ Global Steps:  1006/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.16 │ Epsilon:1.000 │ Buffer:  1006 │ Loss:N/A\n",
            "Ep  195 │ Global Steps:  1014/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.18 │ Epsilon:1.000 │ Buffer:  1014 │ Loss:N/A\n",
            "Ep  196 │ Global Steps:  1017/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.17 │ Epsilon:1.000 │ Buffer:  1017 │ Loss:N/A\n",
            "Ep  197 │ Global Steps:  1025/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.19 │ Epsilon:1.000 │ Buffer:  1025 │ Loss:N/A\n",
            "Ep  198 │ Global Steps:  1028/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.18 │ Epsilon:1.000 │ Buffer:  1028 │ Loss:N/A\n",
            "Ep  199 │ Global Steps:  1033/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.19 │ Epsilon:1.000 │ Buffer:  1033 │ Loss:N/A\n",
            "Ep  200 │ Global Steps:  1041/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  1041 │ Loss:N/A\n",
            "Ep  201 │ Global Steps:  1044/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  1044 │ Loss:N/A\n",
            "Ep  202 │ Global Steps:  1047/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  1047 │ Loss:N/A\n",
            "Ep  203 │ Global Steps:  1052/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  1052 │ Loss:N/A\n",
            "Ep  204 │ Global Steps:  1055/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:  1055 │ Loss:N/A\n",
            "Ep  205 │ Global Steps:  1060/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  1060 │ Loss:N/A\n",
            "Ep  206 │ Global Steps:  1065/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  1065 │ Loss:N/A\n",
            "Ep  207 │ Global Steps:  1073/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  1073 │ Loss:N/A\n",
            "Ep  208 │ Global Steps:  1078/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  1078 │ Loss:N/A\n",
            "Ep  209 │ Global Steps:  1083/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  1083 │ Loss:N/A\n",
            "Ep  210 │ Global Steps:  1091/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  1091 │ Loss:N/A\n",
            "Ep  211 │ Global Steps:  1099/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  1099 │ Loss:N/A\n",
            "Ep  212 │ Global Steps:  1107/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  1107 │ Loss:N/A\n",
            "Ep  213 │ Global Steps:  1115/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  1115 │ Loss:N/A\n",
            "Ep  214 │ Global Steps:  1118/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  1118 │ Loss:N/A\n",
            "Ep  215 │ Global Steps:  1126/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  1126 │ Loss:N/A\n",
            "Ep  216 │ Global Steps:  1129/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  1129 │ Loss:N/A\n",
            "Ep  217 │ Global Steps:  1134/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  1134 │ Loss:N/A\n",
            "Ep  218 │ Global Steps:  1137/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  1137 │ Loss:N/A\n",
            "Ep  219 │ Global Steps:  1145/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  1145 │ Loss:N/A\n",
            "Ep  220 │ Global Steps:  1148/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  1148 │ Loss:N/A\n",
            "Ep  221 │ Global Steps:  1151/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  1151 │ Loss:N/A\n",
            "Ep  222 │ Global Steps:  1159/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  1159 │ Loss:N/A\n",
            "Ep  223 │ Global Steps:  1164/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  1164 │ Loss:N/A\n",
            "Ep  224 │ Global Steps:  1172/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  1172 │ Loss:N/A\n",
            "Ep  225 │ Global Steps:  1177/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  1177 │ Loss:N/A\n",
            "Ep  226 │ Global Steps:  1182/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  1182 │ Loss:N/A\n",
            "Ep  227 │ Global Steps:  1185/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  1185 │ Loss:N/A\n",
            "Ep  228 │ Global Steps:  1193/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  1193 │ Loss:N/A\n",
            "Ep  229 │ Global Steps:  1198/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  1198 │ Loss:N/A\n",
            "Ep  230 │ Global Steps:  1206/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.44 │ Epsilon:1.000 │ Buffer:  1206 │ Loss:N/A\n",
            "Ep  231 │ Global Steps:  1211/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.44 │ Epsilon:1.000 │ Buffer:  1211 │ Loss:N/A\n",
            "Ep  232 │ Global Steps:  1214/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  1214 │ Loss:N/A\n",
            "Ep  233 │ Global Steps:  1219/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  1219 │ Loss:N/A\n",
            "Ep  234 │ Global Steps:  1227/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  1227 │ Loss:N/A\n",
            "Ep  235 │ Global Steps:  1235/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.49 │ Epsilon:1.000 │ Buffer:  1235 │ Loss:N/A\n",
            "Ep  236 │ Global Steps:  1238/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  1238 │ Loss:N/A\n",
            "Ep  237 │ Global Steps:  1246/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  1246 │ Loss:N/A\n",
            "Ep  238 │ Global Steps:  1254/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  1254 │ Loss:N/A\n",
            "Ep  239 │ Global Steps:  1259/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:  1259 │ Loss:N/A\n",
            "Ep  240 │ Global Steps:  1264/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.53 │ Epsilon:1.000 │ Buffer:  1264 │ Loss:N/A\n",
            "Ep  241 │ Global Steps:  1272/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.57 │ Epsilon:1.000 │ Buffer:  1272 │ Loss:N/A\n",
            "Ep  242 │ Global Steps:  1277/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  1277 │ Loss:N/A\n",
            "Ep  243 │ Global Steps:  1282/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  1282 │ Loss:N/A\n",
            "Ep  244 │ Global Steps:  1287/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  1287 │ Loss:N/A\n",
            "Ep  245 │ Global Steps:  1295/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  1295 │ Loss:N/A\n",
            "Ep  246 │ Global Steps:  1303/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.66 │ Epsilon:1.000 │ Buffer:  1303 │ Loss:N/A\n",
            "Ep  247 │ Global Steps:  1308/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.66 │ Epsilon:1.000 │ Buffer:  1308 │ Loss:N/A\n",
            "Ep  248 │ Global Steps:  1316/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  1316 │ Loss:N/A\n",
            "Ep  249 │ Global Steps:  1319/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.66 │ Epsilon:1.000 │ Buffer:  1319 │ Loss:N/A\n",
            "Ep  250 │ Global Steps:  1324/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.66 │ Epsilon:1.000 │ Buffer:  1324 │ Loss:N/A\n",
            "Ep  251 │ Global Steps:  1327/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.64 │ Epsilon:1.000 │ Buffer:  1327 │ Loss:N/A\n",
            "Ep  252 │ Global Steps:  1335/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:  1335 │ Loss:N/A\n",
            "Ep  253 │ Global Steps:  1338/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.67 │ Epsilon:1.000 │ Buffer:  1338 │ Loss:N/A\n",
            "Ep  254 │ Global Steps:  1346/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.66 │ Epsilon:1.000 │ Buffer:  1346 │ Loss:N/A\n",
            "Ep  255 │ Global Steps:  1349/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  1349 │ Loss:N/A\n",
            "Ep  256 │ Global Steps:  1357/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.66 │ Epsilon:1.000 │ Buffer:  1357 │ Loss:N/A\n",
            "Ep  257 │ Global Steps:  1365/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  1365 │ Loss:N/A\n",
            "Ep  258 │ Global Steps:  1370/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  1370 │ Loss:N/A\n",
            "Ep  259 │ Global Steps:  1375/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  1375 │ Loss:N/A\n",
            "Ep  260 │ Global Steps:  1380/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  1380 │ Loss:N/A\n",
            "Ep  261 │ Global Steps:  1383/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.69 │ Epsilon:1.000 │ Buffer:  1383 │ Loss:N/A\n",
            "Ep  262 │ Global Steps:  1388/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.67 │ Epsilon:1.000 │ Buffer:  1388 │ Loss:N/A\n",
            "Ep  263 │ Global Steps:  1393/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.67 │ Epsilon:1.000 │ Buffer:  1393 │ Loss:N/A\n",
            "Ep  264 │ Global Steps:  1398/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.64 │ Epsilon:1.000 │ Buffer:  1398 │ Loss:N/A\n",
            "Ep  265 │ Global Steps:  1403/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  1403 │ Loss:N/A\n",
            "Ep  266 │ Global Steps:  1406/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.58 │ Epsilon:1.000 │ Buffer:  1406 │ Loss:N/A\n",
            "Ep  267 │ Global Steps:  1414/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.60 │ Epsilon:1.000 │ Buffer:  1414 │ Loss:N/A\n",
            "Ep  268 │ Global Steps:  1422/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.64 │ Epsilon:1.000 │ Buffer:  1422 │ Loss:N/A\n",
            "Ep  269 │ Global Steps:  1427/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  1427 │ Loss:N/A\n",
            "Ep  270 │ Global Steps:  1435/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.66 │ Epsilon:1.000 │ Buffer:  1435 │ Loss:N/A\n",
            "Ep  271 │ Global Steps:  1438/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  1438 │ Loss:N/A\n",
            "Ep  272 │ Global Steps:  1441/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.60 │ Epsilon:1.000 │ Buffer:  1441 │ Loss:N/A\n",
            "Ep  273 │ Global Steps:  1444/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.60 │ Epsilon:1.000 │ Buffer:  1444 │ Loss:N/A\n",
            "Ep  274 │ Global Steps:  1449/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  1449 │ Loss:N/A\n",
            "Ep  275 │ Global Steps:  1452/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.58 │ Epsilon:1.000 │ Buffer:  1452 │ Loss:N/A\n",
            "Ep  276 │ Global Steps:  1457/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.58 │ Epsilon:1.000 │ Buffer:  1457 │ Loss:N/A\n",
            "Ep  277 │ Global Steps:  1460/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:  1460 │ Loss:N/A\n",
            "Ep  278 │ Global Steps:  1463/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.54 │ Epsilon:1.000 │ Buffer:  1463 │ Loss:N/A\n",
            "Ep  279 │ Global Steps:  1471/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.57 │ Epsilon:1.000 │ Buffer:  1471 │ Loss:N/A\n",
            "Ep  280 │ Global Steps:  1476/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.57 │ Epsilon:1.000 │ Buffer:  1476 │ Loss:N/A\n",
            "Ep  281 │ Global Steps:  1481/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.57 │ Epsilon:1.000 │ Buffer:  1481 │ Loss:N/A\n",
            "Ep  282 │ Global Steps:  1489/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.61 │ Epsilon:1.000 │ Buffer:  1489 │ Loss:N/A\n",
            "Ep  283 │ Global Steps:  1494/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  1494 │ Loss:N/A\n",
            "Ep  284 │ Global Steps:  1499/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  1499 │ Loss:N/A\n",
            "Ep  285 │ Global Steps:  1507/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  1507 │ Loss:N/A\n",
            "Ep  286 │ Global Steps:  1515/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.63 │ Epsilon:1.000 │ Buffer:  1515 │ Loss:N/A\n",
            "Ep  287 │ Global Steps:  1518/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.61 │ Epsilon:1.000 │ Buffer:  1518 │ Loss:N/A\n",
            "Ep  288 │ Global Steps:  1523/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  1523 │ Loss:N/A\n",
            "Ep  289 │ Global Steps:  1528/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  1528 │ Loss:N/A\n",
            "Ep  290 │ Global Steps:  1536/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  1536 │ Loss:N/A\n",
            "Ep  291 │ Global Steps:  1539/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  1539 │ Loss:N/A\n",
            "Ep  292 │ Global Steps:  1547/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.63 │ Epsilon:1.000 │ Buffer:  1547 │ Loss:N/A\n",
            "Ep  293 │ Global Steps:  1550/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.63 │ Epsilon:1.000 │ Buffer:  1550 │ Loss:N/A\n",
            "Ep  294 │ Global Steps:  1558/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.65 │ Epsilon:1.000 │ Buffer:  1558 │ Loss:N/A\n",
            "Ep  295 │ Global Steps:  1561/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.61 │ Epsilon:1.000 │ Buffer:  1561 │ Loss:N/A\n",
            "Ep  296 │ Global Steps:  1566/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.63 │ Epsilon:1.000 │ Buffer:  1566 │ Loss:N/A\n",
            "Ep  297 │ Global Steps:  1571/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.60 │ Epsilon:1.000 │ Buffer:  1571 │ Loss:N/A\n",
            "Ep  298 │ Global Steps:  1574/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.60 │ Epsilon:1.000 │ Buffer:  1574 │ Loss:N/A\n",
            "Ep  299 │ Global Steps:  1579/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.60 │ Epsilon:1.000 │ Buffer:  1579 │ Loss:N/A\n",
            "Ep  300 │ Global Steps:  1582/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:  1582 │ Loss:N/A\n",
            "Ep  301 │ Global Steps:  1590/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.60 │ Epsilon:1.000 │ Buffer:  1590 │ Loss:N/A\n",
            "Ep  302 │ Global Steps:  1595/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  1595 │ Loss:N/A\n",
            "Ep  303 │ Global Steps:  1603/10000000 │ Ep Return:   -5.70 │ Avg Return (100 ep):   -3.64 │ Epsilon:1.000 │ Buffer:  1603 │ Loss:N/A\n",
            "Ep  304 │ Global Steps:  1611/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:  1611 │ Loss:N/A\n",
            "Ep  305 │ Global Steps:  1619/10000000 │ Ep Return:   -5.72 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  1619 │ Loss:N/A\n",
            "Ep  306 │ Global Steps:  1622/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.69 │ Epsilon:1.000 │ Buffer:  1622 │ Loss:N/A\n",
            "Ep  307 │ Global Steps:  1627/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.67 │ Epsilon:1.000 │ Buffer:  1627 │ Loss:N/A\n",
            "Ep  308 │ Global Steps:  1632/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.67 │ Epsilon:1.000 │ Buffer:  1632 │ Loss:N/A\n",
            "Ep  309 │ Global Steps:  1640/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.69 │ Epsilon:1.000 │ Buffer:  1640 │ Loss:N/A\n",
            "Ep  310 │ Global Steps:  1648/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.69 │ Epsilon:1.000 │ Buffer:  1648 │ Loss:N/A\n",
            "Ep  311 │ Global Steps:  1653/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.67 │ Epsilon:1.000 │ Buffer:  1653 │ Loss:N/A\n",
            "Ep  312 │ Global Steps:  1661/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.67 │ Epsilon:1.000 │ Buffer:  1661 │ Loss:N/A\n",
            "Ep  313 │ Global Steps:  1666/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.64 │ Epsilon:1.000 │ Buffer:  1666 │ Loss:N/A\n",
            "Ep  314 │ Global Steps:  1669/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.64 │ Epsilon:1.000 │ Buffer:  1669 │ Loss:N/A\n",
            "Ep  315 │ Global Steps:  1674/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  1674 │ Loss:N/A\n",
            "Ep  316 │ Global Steps:  1682/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.66 │ Epsilon:1.000 │ Buffer:  1682 │ Loss:N/A\n",
            "Ep  317 │ Global Steps:  1690/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.69 │ Epsilon:1.000 │ Buffer:  1690 │ Loss:N/A\n",
            "Ep  318 │ Global Steps:  1695/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  1695 │ Loss:N/A\n",
            "Ep  319 │ Global Steps:  1703/10000000 │ Ep Return:   -5.65 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  1703 │ Loss:N/A\n",
            "Ep  320 │ Global Steps:  1711/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  1711 │ Loss:N/A\n",
            "Ep  321 │ Global Steps:  1719/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.78 │ Epsilon:1.000 │ Buffer:  1719 │ Loss:N/A\n",
            "Ep  322 │ Global Steps:  1727/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.78 │ Epsilon:1.000 │ Buffer:  1727 │ Loss:N/A\n",
            "Ep  323 │ Global Steps:  1735/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.81 │ Epsilon:1.000 │ Buffer:  1735 │ Loss:N/A\n",
            "Ep  324 │ Global Steps:  1738/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.77 │ Epsilon:1.000 │ Buffer:  1738 │ Loss:N/A\n",
            "Ep  325 │ Global Steps:  1741/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.75 │ Epsilon:1.000 │ Buffer:  1741 │ Loss:N/A\n",
            "Ep  326 │ Global Steps:  1749/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.78 │ Epsilon:1.000 │ Buffer:  1749 │ Loss:N/A\n",
            "Ep  327 │ Global Steps:  1752/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.78 │ Epsilon:1.000 │ Buffer:  1752 │ Loss:N/A\n",
            "Ep  328 │ Global Steps:  1760/10000000 │ Ep Return:   -5.70 │ Avg Return (100 ep):   -3.78 │ Epsilon:1.000 │ Buffer:  1760 │ Loss:N/A\n",
            "Ep  329 │ Global Steps:  1765/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.77 │ Epsilon:1.000 │ Buffer:  1765 │ Loss:N/A\n",
            "Ep  330 │ Global Steps:  1770/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.75 │ Epsilon:1.000 │ Buffer:  1770 │ Loss:N/A\n",
            "Ep  331 │ Global Steps:  1773/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.73 │ Epsilon:1.000 │ Buffer:  1773 │ Loss:N/A\n",
            "Ep  332 │ Global Steps:  1781/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.78 │ Epsilon:1.000 │ Buffer:  1781 │ Loss:N/A\n",
            "Ep  333 │ Global Steps:  1789/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.80 │ Epsilon:1.000 │ Buffer:  1789 │ Loss:N/A\n",
            "Ep  334 │ Global Steps:  1792/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  1792 │ Loss:N/A\n",
            "Ep  335 │ Global Steps:  1800/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  1800 │ Loss:N/A\n",
            "Ep  336 │ Global Steps:  1803/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  1803 │ Loss:N/A\n",
            "Ep  337 │ Global Steps:  1808/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.73 │ Epsilon:1.000 │ Buffer:  1808 │ Loss:N/A\n",
            "Ep  338 │ Global Steps:  1816/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.73 │ Epsilon:1.000 │ Buffer:  1816 │ Loss:N/A\n",
            "Ep  339 │ Global Steps:  1824/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  1824 │ Loss:N/A\n",
            "Ep  340 │ Global Steps:  1829/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  1829 │ Loss:N/A\n",
            "Ep  341 │ Global Steps:  1837/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  1837 │ Loss:N/A\n",
            "Ep  342 │ Global Steps:  1842/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  1842 │ Loss:N/A\n",
            "Ep  343 │ Global Steps:  1850/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.78 │ Epsilon:1.000 │ Buffer:  1850 │ Loss:N/A\n",
            "Ep  344 │ Global Steps:  1855/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.78 │ Epsilon:1.000 │ Buffer:  1855 │ Loss:N/A\n",
            "Ep  345 │ Global Steps:  1863/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.78 │ Epsilon:1.000 │ Buffer:  1863 │ Loss:N/A\n",
            "Ep  346 │ Global Steps:  1866/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  1866 │ Loss:N/A\n",
            "Ep  347 │ Global Steps:  1869/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.73 │ Epsilon:1.000 │ Buffer:  1869 │ Loss:N/A\n",
            "Ep  348 │ Global Steps:  1874/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  1874 │ Loss:N/A\n",
            "Ep  349 │ Global Steps:  1882/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  1882 │ Loss:N/A\n",
            "Ep  350 │ Global Steps:  1885/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.73 │ Epsilon:1.000 │ Buffer:  1885 │ Loss:N/A\n",
            "Ep  351 │ Global Steps:  1890/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  1890 │ Loss:N/A\n",
            "Ep  352 │ Global Steps:  1898/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  1898 │ Loss:N/A\n",
            "Ep  353 │ Global Steps:  1901/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  1901 │ Loss:N/A\n",
            "Ep  354 │ Global Steps:  1904/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  1904 │ Loss:N/A\n",
            "Ep  355 │ Global Steps:  1909/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  1909 │ Loss:N/A\n",
            "Ep  356 │ Global Steps:  1917/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  1917 │ Loss:N/A\n",
            "Ep  357 │ Global Steps:  1922/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.69 │ Epsilon:1.000 │ Buffer:  1922 │ Loss:N/A\n",
            "Ep  358 │ Global Steps:  1927/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  1927 │ Loss:N/A\n",
            "Ep  359 │ Global Steps:  1932/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.69 │ Epsilon:1.000 │ Buffer:  1932 │ Loss:N/A\n",
            "Ep  360 │ Global Steps:  1937/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.69 │ Epsilon:1.000 │ Buffer:  1937 │ Loss:N/A\n",
            "Ep  361 │ Global Steps:  1942/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  1942 │ Loss:N/A\n",
            "Ep  362 │ Global Steps:  1947/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  1947 │ Loss:N/A\n",
            "Ep  363 │ Global Steps:  1955/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.73 │ Epsilon:1.000 │ Buffer:  1955 │ Loss:N/A\n",
            "Ep  364 │ Global Steps:  1958/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  1958 │ Loss:N/A\n",
            "Ep  365 │ Global Steps:  1966/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  1966 │ Loss:N/A\n",
            "Ep  366 │ Global Steps:  1969/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  1969 │ Loss:N/A\n",
            "Ep  367 │ Global Steps:  1972/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  1972 │ Loss:N/A\n",
            "Ep  368 │ Global Steps:  1977/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:  1977 │ Loss:N/A\n",
            "Ep  369 │ Global Steps:  1985/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  1985 │ Loss:N/A\n",
            "Ep  370 │ Global Steps:  1993/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  1993 │ Loss:N/A\n",
            "Ep  371 │ Global Steps:  1996/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  1996 │ Loss:N/A\n",
            "Ep  372 │ Global Steps:  1999/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  1999 │ Loss:N/A\n",
            "Ep  373 │ Global Steps:  2007/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  2007 │ Loss:N/A\n",
            "Ep  374 │ Global Steps:  2015/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.77 │ Epsilon:1.000 │ Buffer:  2015 │ Loss:N/A\n",
            "Ep  375 │ Global Steps:  2018/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.77 │ Epsilon:1.000 │ Buffer:  2018 │ Loss:N/A\n",
            "Ep  376 │ Global Steps:  2023/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.77 │ Epsilon:1.000 │ Buffer:  2023 │ Loss:N/A\n",
            "Ep  377 │ Global Steps:  2031/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.81 │ Epsilon:1.000 │ Buffer:  2031 │ Loss:N/A\n",
            "Ep  378 │ Global Steps:  2036/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.83 │ Epsilon:1.000 │ Buffer:  2036 │ Loss:N/A\n",
            "Ep  379 │ Global Steps:  2041/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.80 │ Epsilon:1.000 │ Buffer:  2041 │ Loss:N/A\n",
            "Ep  380 │ Global Steps:  2049/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.83 │ Epsilon:1.000 │ Buffer:  2049 │ Loss:N/A\n",
            "Ep  381 │ Global Steps:  2057/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.85 │ Epsilon:1.000 │ Buffer:  2057 │ Loss:N/A\n",
            "Ep  382 │ Global Steps:  2062/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.83 │ Epsilon:1.000 │ Buffer:  2062 │ Loss:N/A\n",
            "Ep  383 │ Global Steps:  2065/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.81 │ Epsilon:1.000 │ Buffer:  2065 │ Loss:N/A\n",
            "Ep  384 │ Global Steps:  2070/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.81 │ Epsilon:1.000 │ Buffer:  2070 │ Loss:N/A\n",
            "Ep  385 │ Global Steps:  2078/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.81 │ Epsilon:1.000 │ Buffer:  2078 │ Loss:N/A\n",
            "Ep  386 │ Global Steps:  2086/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.81 │ Epsilon:1.000 │ Buffer:  2086 │ Loss:N/A\n",
            "Ep  387 │ Global Steps:  2091/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.82 │ Epsilon:1.000 │ Buffer:  2091 │ Loss:N/A\n",
            "Ep  388 │ Global Steps:  2096/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.82 │ Epsilon:1.000 │ Buffer:  2096 │ Loss:N/A\n",
            "Ep  389 │ Global Steps:  2101/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.82 │ Epsilon:1.000 │ Buffer:  2101 │ Loss:N/A\n",
            "Ep  390 │ Global Steps:  2104/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.78 │ Epsilon:1.000 │ Buffer:  2104 │ Loss:N/A\n",
            "Ep  391 │ Global Steps:  2107/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.78 │ Epsilon:1.000 │ Buffer:  2107 │ Loss:N/A\n",
            "Ep  392 │ Global Steps:  2115/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.78 │ Epsilon:1.000 │ Buffer:  2115 │ Loss:N/A\n",
            "Ep  393 │ Global Steps:  2118/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.78 │ Epsilon:1.000 │ Buffer:  2118 │ Loss:N/A\n",
            "Ep  394 │ Global Steps:  2123/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  2123 │ Loss:N/A\n",
            "Ep  395 │ Global Steps:  2126/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  2126 │ Loss:N/A\n",
            "Ep  396 │ Global Steps:  2131/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  2131 │ Loss:N/A\n",
            "Ep  397 │ Global Steps:  2139/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.78 │ Epsilon:1.000 │ Buffer:  2139 │ Loss:N/A\n",
            "Ep  398 │ Global Steps:  2144/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.80 │ Epsilon:1.000 │ Buffer:  2144 │ Loss:N/A\n",
            "Ep  399 │ Global Steps:  2152/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.82 │ Epsilon:1.000 │ Buffer:  2152 │ Loss:N/A\n",
            "Ep  400 │ Global Steps:  2155/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.82 │ Epsilon:1.000 │ Buffer:  2155 │ Loss:N/A\n",
            "Ep  401 │ Global Steps:  2160/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.80 │ Epsilon:1.000 │ Buffer:  2160 │ Loss:N/A\n",
            "Ep  402 │ Global Steps:  2163/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.78 │ Epsilon:1.000 │ Buffer:  2163 │ Loss:N/A\n",
            "Ep  403 │ Global Steps:  2168/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  2168 │ Loss:N/A\n",
            "Ep  404 │ Global Steps:  2171/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  2171 │ Loss:N/A\n",
            "Ep  405 │ Global Steps:  2174/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:  2174 │ Loss:N/A\n",
            "Ep  406 │ Global Steps:  2177/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:  2177 │ Loss:N/A\n",
            "Ep  407 │ Global Steps:  2185/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  2185 │ Loss:N/A\n",
            "Ep  408 │ Global Steps:  2188/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.69 │ Epsilon:1.000 │ Buffer:  2188 │ Loss:N/A\n",
            "Ep  409 │ Global Steps:  2193/10000000 │ Ep Return:   -3.22 │ Avg Return (100 ep):   -3.66 │ Epsilon:1.000 │ Buffer:  2193 │ Loss:N/A\n",
            "Ep  410 │ Global Steps:  2198/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.64 │ Epsilon:1.000 │ Buffer:  2198 │ Loss:N/A\n",
            "Ep  411 │ Global Steps:  2201/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  2201 │ Loss:N/A\n",
            "Ep  412 │ Global Steps:  2206/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.60 │ Epsilon:1.000 │ Buffer:  2206 │ Loss:N/A\n",
            "Ep  413 │ Global Steps:  2214/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  2214 │ Loss:N/A\n",
            "Ep  414 │ Global Steps:  2219/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.64 │ Epsilon:1.000 │ Buffer:  2219 │ Loss:N/A\n",
            "Ep  415 │ Global Steps:  2222/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  2222 │ Loss:N/A\n",
            "Ep  416 │ Global Steps:  2227/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.60 │ Epsilon:1.000 │ Buffer:  2227 │ Loss:N/A\n",
            "Ep  417 │ Global Steps:  2230/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:  2230 │ Loss:N/A\n",
            "Ep  418 │ Global Steps:  2235/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:  2235 │ Loss:N/A\n",
            "Ep  419 │ Global Steps:  2243/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:  2243 │ Loss:N/A\n",
            "Ep  420 │ Global Steps:  2251/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:  2251 │ Loss:N/A\n",
            "Ep  421 │ Global Steps:  2256/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.53 │ Epsilon:1.000 │ Buffer:  2256 │ Loss:N/A\n",
            "Ep  422 │ Global Steps:  2264/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.53 │ Epsilon:1.000 │ Buffer:  2264 │ Loss:N/A\n",
            "Ep  423 │ Global Steps:  2269/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.51 │ Epsilon:1.000 │ Buffer:  2269 │ Loss:N/A\n",
            "Ep  424 │ Global Steps:  2277/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:  2277 │ Loss:N/A\n",
            "Ep  425 │ Global Steps:  2280/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:  2280 │ Loss:N/A\n",
            "Ep  426 │ Global Steps:  2285/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:  2285 │ Loss:N/A\n",
            "Ep  427 │ Global Steps:  2288/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:  2288 │ Loss:N/A\n",
            "Ep  428 │ Global Steps:  2291/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  2291 │ Loss:N/A\n",
            "Ep  429 │ Global Steps:  2296/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  2296 │ Loss:N/A\n",
            "Ep  430 │ Global Steps:  2301/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  2301 │ Loss:N/A\n",
            "Ep  431 │ Global Steps:  2306/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  2306 │ Loss:N/A\n",
            "Ep  432 │ Global Steps:  2309/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  2309 │ Loss:N/A\n",
            "Ep  433 │ Global Steps:  2312/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  2312 │ Loss:N/A\n",
            "Ep  434 │ Global Steps:  2315/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  2315 │ Loss:N/A\n",
            "Ep  435 │ Global Steps:  2318/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  2318 │ Loss:N/A\n",
            "Ep  436 │ Global Steps:  2323/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  2323 │ Loss:N/A\n",
            "Ep  437 │ Global Steps:  2326/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  2326 │ Loss:N/A\n",
            "Ep  438 │ Global Steps:  2329/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  2329 │ Loss:N/A\n",
            "Ep  439 │ Global Steps:  2332/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  2332 │ Loss:N/A\n",
            "Ep  440 │ Global Steps:  2340/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  2340 │ Loss:N/A\n",
            "Ep  441 │ Global Steps:  2345/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  2345 │ Loss:N/A\n",
            "Ep  442 │ Global Steps:  2348/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  2348 │ Loss:N/A\n",
            "Ep  443 │ Global Steps:  2353/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  2353 │ Loss:N/A\n",
            "Ep  444 │ Global Steps:  2356/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  2356 │ Loss:N/A\n",
            "Ep  445 │ Global Steps:  2364/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  2364 │ Loss:N/A\n",
            "Ep  446 │ Global Steps:  2372/10000000 │ Ep Return:   -5.65 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  2372 │ Loss:N/A\n",
            "Ep  447 │ Global Steps:  2380/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  2380 │ Loss:N/A\n",
            "Ep  448 │ Global Steps:  2383/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  2383 │ Loss:N/A\n",
            "Ep  449 │ Global Steps:  2388/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  2388 │ Loss:N/A\n",
            "Ep  450 │ Global Steps:  2391/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  2391 │ Loss:N/A\n",
            "Ep  451 │ Global Steps:  2399/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  2399 │ Loss:N/A\n",
            "Ep  452 │ Global Steps:  2402/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  2402 │ Loss:N/A\n",
            "Ep  453 │ Global Steps:  2405/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  2405 │ Loss:N/A\n",
            "Ep  454 │ Global Steps:  2410/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  2410 │ Loss:N/A\n",
            "Ep  455 │ Global Steps:  2418/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  2418 │ Loss:N/A\n",
            "Ep  456 │ Global Steps:  2421/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  2421 │ Loss:N/A\n",
            "Ep  457 │ Global Steps:  2429/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  2429 │ Loss:N/A\n",
            "Ep  458 │ Global Steps:  2437/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  2437 │ Loss:N/A\n",
            "Ep  459 │ Global Steps:  2445/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  2445 │ Loss:N/A\n",
            "Ep  460 │ Global Steps:  2448/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  2448 │ Loss:N/A\n",
            "Ep  461 │ Global Steps:  2453/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  2453 │ Loss:N/A\n",
            "Ep  462 │ Global Steps:  2458/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  2458 │ Loss:N/A\n",
            "Ep  463 │ Global Steps:  2463/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  2463 │ Loss:N/A\n",
            "Ep  464 │ Global Steps:  2471/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  2471 │ Loss:N/A\n",
            "Ep  465 │ Global Steps:  2474/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  2474 │ Loss:N/A\n",
            "Ep  466 │ Global Steps:  2479/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  2479 │ Loss:N/A\n",
            "Ep  467 │ Global Steps:  2487/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  2487 │ Loss:N/A\n",
            "Ep  468 │ Global Steps:  2495/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  2495 │ Loss:N/A\n",
            "Ep  469 │ Global Steps:  2500/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  2500 │ Loss:N/A\n",
            "Ep  470 │ Global Steps:  2503/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  2503 │ Loss:N/A\n",
            "Ep  471 │ Global Steps:  2511/10000000 │ Ep Return:   -5.65 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  2511 │ Loss:N/A\n",
            "Ep  472 │ Global Steps:  2516/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  2516 │ Loss:N/A\n",
            "Ep  473 │ Global Steps:  2524/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  2524 │ Loss:N/A\n",
            "Ep  474 │ Global Steps:  2529/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  2529 │ Loss:N/A\n",
            "Ep  475 │ Global Steps:  2534/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  2534 │ Loss:N/A\n",
            "Ep  476 │ Global Steps:  2537/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  2537 │ Loss:N/A\n",
            "Ep  477 │ Global Steps:  2540/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  2540 │ Loss:N/A\n",
            "Ep  478 │ Global Steps:  2548/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  2548 │ Loss:N/A\n",
            "Ep  479 │ Global Steps:  2553/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  2553 │ Loss:N/A\n",
            "Ep  480 │ Global Steps:  2558/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  2558 │ Loss:N/A\n",
            "Ep  481 │ Global Steps:  2566/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  2566 │ Loss:N/A\n",
            "Ep  482 │ Global Steps:  2571/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  2571 │ Loss:N/A\n",
            "Ep  483 │ Global Steps:  2574/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  2574 │ Loss:N/A\n",
            "Ep  484 │ Global Steps:  2577/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  2577 │ Loss:N/A\n",
            "Ep  485 │ Global Steps:  2582/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  2582 │ Loss:N/A\n",
            "Ep  486 │ Global Steps:  2587/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  2587 │ Loss:N/A\n",
            "Ep  487 │ Global Steps:  2590/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  2590 │ Loss:N/A\n",
            "Ep  488 │ Global Steps:  2595/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  2595 │ Loss:N/A\n",
            "Ep  489 │ Global Steps:  2603/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  2603 │ Loss:N/A\n",
            "Ep  490 │ Global Steps:  2606/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  2606 │ Loss:N/A\n",
            "Ep  491 │ Global Steps:  2609/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  2609 │ Loss:N/A\n",
            "Ep  492 │ Global Steps:  2617/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  2617 │ Loss:N/A\n",
            "Ep  493 │ Global Steps:  2625/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  2625 │ Loss:N/A\n",
            "Ep  494 │ Global Steps:  2630/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  2630 │ Loss:N/A\n",
            "Ep  495 │ Global Steps:  2635/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  2635 │ Loss:N/A\n",
            "Ep  496 │ Global Steps:  2640/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  2640 │ Loss:N/A\n",
            "Ep  497 │ Global Steps:  2643/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  2643 │ Loss:N/A\n",
            "Ep  498 │ Global Steps:  2648/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  2648 │ Loss:N/A\n",
            "Ep  499 │ Global Steps:  2656/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  2656 │ Loss:N/A\n",
            "Ep  500 │ Global Steps:  2664/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  2664 │ Loss:N/A\n",
            "Ep  501 │ Global Steps:  2667/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  2667 │ Loss:N/A\n",
            "Ep  502 │ Global Steps:  2670/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  2670 │ Loss:N/A\n",
            "Ep  503 │ Global Steps:  2673/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  2673 │ Loss:N/A\n",
            "Ep  504 │ Global Steps:  2678/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  2678 │ Loss:N/A\n",
            "Ep  505 │ Global Steps:  2683/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  2683 │ Loss:N/A\n",
            "Ep  506 │ Global Steps:  2688/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  2688 │ Loss:N/A\n",
            "Ep  507 │ Global Steps:  2691/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  2691 │ Loss:N/A\n",
            "Ep  508 │ Global Steps:  2694/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  2694 │ Loss:N/A\n",
            "Ep  509 │ Global Steps:  2697/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  2697 │ Loss:N/A\n",
            "Ep  510 │ Global Steps:  2705/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  2705 │ Loss:N/A\n",
            "Ep  511 │ Global Steps:  2708/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  2708 │ Loss:N/A\n",
            "Ep  512 │ Global Steps:  2711/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  2711 │ Loss:N/A\n",
            "Ep  513 │ Global Steps:  2716/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  2716 │ Loss:N/A\n",
            "Ep  514 │ Global Steps:  2719/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  2719 │ Loss:N/A\n",
            "Ep  515 │ Global Steps:  2722/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  2722 │ Loss:N/A\n",
            "Ep  516 │ Global Steps:  2725/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  2725 │ Loss:N/A\n",
            "Ep  517 │ Global Steps:  2728/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  2728 │ Loss:N/A\n",
            "Ep  518 │ Global Steps:  2736/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  2736 │ Loss:N/A\n",
            "Ep  519 │ Global Steps:  2744/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  2744 │ Loss:N/A\n",
            "Ep  520 │ Global Steps:  2749/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  2749 │ Loss:N/A\n",
            "Ep  521 │ Global Steps:  2752/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:  2752 │ Loss:N/A\n",
            "Ep  522 │ Global Steps:  2760/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:  2760 │ Loss:N/A\n",
            "Ep  523 │ Global Steps:  2765/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:  2765 │ Loss:N/A\n",
            "Ep  524 │ Global Steps:  2770/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.17 │ Epsilon:1.000 │ Buffer:  2770 │ Loss:N/A\n",
            "Ep  525 │ Global Steps:  2773/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.17 │ Epsilon:1.000 │ Buffer:  2773 │ Loss:N/A\n",
            "Ep  526 │ Global Steps:  2781/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:  2781 │ Loss:N/A\n",
            "Ep  527 │ Global Steps:  2786/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  2786 │ Loss:N/A\n",
            "Ep  528 │ Global Steps:  2789/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  2789 │ Loss:N/A\n",
            "Ep  529 │ Global Steps:  2797/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  2797 │ Loss:N/A\n",
            "Ep  530 │ Global Steps:  2800/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  2800 │ Loss:N/A\n",
            "Ep  531 │ Global Steps:  2803/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  2803 │ Loss:N/A\n",
            "Ep  532 │ Global Steps:  2811/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  2811 │ Loss:N/A\n",
            "Ep  533 │ Global Steps:  2819/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  2819 │ Loss:N/A\n",
            "Ep  534 │ Global Steps:  2822/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  2822 │ Loss:N/A\n",
            "Ep  535 │ Global Steps:  2825/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  2825 │ Loss:N/A\n",
            "Ep  536 │ Global Steps:  2833/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  2833 │ Loss:N/A\n",
            "Ep  537 │ Global Steps:  2838/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  2838 │ Loss:N/A\n",
            "Ep  538 │ Global Steps:  2841/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  2841 │ Loss:N/A\n",
            "Ep  539 │ Global Steps:  2846/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  2846 │ Loss:N/A\n",
            "Ep  540 │ Global Steps:  2849/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  2849 │ Loss:N/A\n",
            "Ep  541 │ Global Steps:  2857/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  2857 │ Loss:N/A\n",
            "Ep  542 │ Global Steps:  2860/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  2860 │ Loss:N/A\n",
            "Ep  543 │ Global Steps:  2863/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  2863 │ Loss:N/A\n",
            "Ep  544 │ Global Steps:  2868/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  2868 │ Loss:N/A\n",
            "Ep  545 │ Global Steps:  2873/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  2873 │ Loss:N/A\n",
            "Ep  546 │ Global Steps:  2881/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  2881 │ Loss:N/A\n",
            "Ep  547 │ Global Steps:  2884/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  2884 │ Loss:N/A\n",
            "Ep  548 │ Global Steps:  2892/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  2892 │ Loss:N/A\n",
            "Ep  549 │ Global Steps:  2900/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  2900 │ Loss:N/A\n",
            "Ep  550 │ Global Steps:  2903/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  2903 │ Loss:N/A\n",
            "Ep  551 │ Global Steps:  2906/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  2906 │ Loss:N/A\n",
            "Ep  552 │ Global Steps:  2914/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  2914 │ Loss:N/A\n",
            "Ep  553 │ Global Steps:  2917/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  2917 │ Loss:N/A\n",
            "Ep  554 │ Global Steps:  2922/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  2922 │ Loss:N/A\n",
            "Ep  555 │ Global Steps:  2927/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  2927 │ Loss:N/A\n",
            "Ep  556 │ Global Steps:  2935/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  2935 │ Loss:N/A\n",
            "Ep  557 │ Global Steps:  2938/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  2938 │ Loss:N/A\n",
            "Ep  558 │ Global Steps:  2943/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  2943 │ Loss:N/A\n",
            "Ep  559 │ Global Steps:  2948/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  2948 │ Loss:N/A\n",
            "Ep  560 │ Global Steps:  2953/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  2953 │ Loss:N/A\n",
            "Ep  561 │ Global Steps:  2961/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  2961 │ Loss:N/A\n",
            "Ep  562 │ Global Steps:  2966/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  2966 │ Loss:N/A\n",
            "Ep  563 │ Global Steps:  2971/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  2971 │ Loss:N/A\n",
            "Ep  564 │ Global Steps:  2974/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  2974 │ Loss:N/A\n",
            "Ep  565 │ Global Steps:  2982/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  2982 │ Loss:N/A\n",
            "Ep  566 │ Global Steps:  2987/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  2987 │ Loss:N/A\n",
            "Ep  567 │ Global Steps:  2990/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  2990 │ Loss:N/A\n",
            "Ep  568 │ Global Steps:  2993/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  2993 │ Loss:N/A\n",
            "Ep  569 │ Global Steps:  2998/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  2998 │ Loss:N/A\n",
            "Ep  570 │ Global Steps:  3003/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  3003 │ Loss:N/A\n",
            "Ep  571 │ Global Steps:  3006/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.19 │ Epsilon:1.000 │ Buffer:  3006 │ Loss:N/A\n",
            "Ep  572 │ Global Steps:  3014/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  3014 │ Loss:N/A\n",
            "Ep  573 │ Global Steps:  3022/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  3022 │ Loss:N/A\n",
            "Ep  574 │ Global Steps:  3030/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  3030 │ Loss:N/A\n",
            "Ep  575 │ Global Steps:  3035/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  3035 │ Loss:N/A\n",
            "Ep  576 │ Global Steps:  3038/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  3038 │ Loss:N/A\n",
            "Ep  577 │ Global Steps:  3041/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  3041 │ Loss:N/A\n",
            "Ep  578 │ Global Steps:  3044/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:  3044 │ Loss:N/A\n",
            "Ep  579 │ Global Steps:  3049/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:  3049 │ Loss:N/A\n",
            "Ep  580 │ Global Steps:  3057/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  3057 │ Loss:N/A\n",
            "Ep  581 │ Global Steps:  3065/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  3065 │ Loss:N/A\n",
            "Ep  582 │ Global Steps:  3068/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:  3068 │ Loss:N/A\n",
            "Ep  583 │ Global Steps:  3071/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:  3071 │ Loss:N/A\n",
            "Ep  584 │ Global Steps:  3079/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  3079 │ Loss:N/A\n",
            "Ep  585 │ Global Steps:  3082/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  3082 │ Loss:N/A\n",
            "Ep  586 │ Global Steps:  3090/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  3090 │ Loss:N/A\n",
            "Ep  587 │ Global Steps:  3095/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  3095 │ Loss:N/A\n",
            "Ep  588 │ Global Steps:  3100/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  3100 │ Loss:N/A\n",
            "Ep  589 │ Global Steps:  3103/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  3103 │ Loss:N/A\n",
            "Ep  590 │ Global Steps:  3106/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  3106 │ Loss:N/A\n",
            "Ep  591 │ Global Steps:  3114/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  3114 │ Loss:N/A\n",
            "Ep  592 │ Global Steps:  3122/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  3122 │ Loss:N/A\n",
            "Ep  593 │ Global Steps:  3130/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  3130 │ Loss:N/A\n",
            "Ep  594 │ Global Steps:  3135/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  3135 │ Loss:N/A\n",
            "Ep  595 │ Global Steps:  3138/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  3138 │ Loss:N/A\n",
            "Ep  596 │ Global Steps:  3141/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  3141 │ Loss:N/A\n",
            "Ep  597 │ Global Steps:  3144/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  3144 │ Loss:N/A\n",
            "Ep  598 │ Global Steps:  3152/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  3152 │ Loss:N/A\n",
            "Ep  599 │ Global Steps:  3155/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  3155 │ Loss:N/A\n",
            "Ep  600 │ Global Steps:  3160/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:  3160 │ Loss:N/A\n",
            "Ep  601 │ Global Steps:  3168/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  3168 │ Loss:N/A\n",
            "Ep  602 │ Global Steps:  3176/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  3176 │ Loss:N/A\n",
            "Ep  603 │ Global Steps:  3181/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  3181 │ Loss:N/A\n",
            "Ep  604 │ Global Steps:  3184/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  3184 │ Loss:N/A\n",
            "Ep  605 │ Global Steps:  3192/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  3192 │ Loss:N/A\n",
            "Ep  606 │ Global Steps:  3195/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  3195 │ Loss:N/A\n",
            "Ep  607 │ Global Steps:  3198/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  3198 │ Loss:N/A\n",
            "Ep  608 │ Global Steps:  3203/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  3203 │ Loss:N/A\n",
            "Ep  609 │ Global Steps:  3206/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  3206 │ Loss:N/A\n",
            "Ep  610 │ Global Steps:  3211/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  3211 │ Loss:N/A\n",
            "Ep  611 │ Global Steps:  3216/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  3216 │ Loss:N/A\n",
            "Ep  612 │ Global Steps:  3219/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  3219 │ Loss:N/A\n",
            "Ep  613 │ Global Steps:  3222/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  3222 │ Loss:N/A\n",
            "Ep  614 │ Global Steps:  3227/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  3227 │ Loss:N/A\n",
            "Ep  615 │ Global Steps:  3232/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  3232 │ Loss:N/A\n",
            "Ep  616 │ Global Steps:  3237/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  3237 │ Loss:N/A\n",
            "Ep  617 │ Global Steps:  3245/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  3245 │ Loss:N/A\n",
            "Ep  618 │ Global Steps:  3248/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  3248 │ Loss:N/A\n",
            "Ep  619 │ Global Steps:  3253/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  3253 │ Loss:N/A\n",
            "Ep  620 │ Global Steps:  3258/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  3258 │ Loss:N/A\n",
            "Ep  621 │ Global Steps:  3266/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  3266 │ Loss:N/A\n",
            "Ep  622 │ Global Steps:  3269/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  3269 │ Loss:N/A\n",
            "Ep  623 │ Global Steps:  3277/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  3277 │ Loss:N/A\n",
            "Ep  624 │ Global Steps:  3280/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  3280 │ Loss:N/A\n",
            "Ep  625 │ Global Steps:  3288/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  3288 │ Loss:N/A\n",
            "Ep  626 │ Global Steps:  3296/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  3296 │ Loss:N/A\n",
            "Ep  627 │ Global Steps:  3299/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  3299 │ Loss:N/A\n",
            "Ep  628 │ Global Steps:  3307/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  3307 │ Loss:N/A\n",
            "Ep  629 │ Global Steps:  3310/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  3310 │ Loss:N/A\n",
            "Ep  630 │ Global Steps:  3313/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  3313 │ Loss:N/A\n",
            "Ep  631 │ Global Steps:  3316/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  3316 │ Loss:N/A\n",
            "Ep  632 │ Global Steps:  3319/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  3319 │ Loss:N/A\n",
            "Ep  633 │ Global Steps:  3324/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  3324 │ Loss:N/A\n",
            "Ep  634 │ Global Steps:  3327/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  3327 │ Loss:N/A\n",
            "Ep  635 │ Global Steps:  3332/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  3332 │ Loss:N/A\n",
            "Ep  636 │ Global Steps:  3335/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  3335 │ Loss:N/A\n",
            "Ep  637 │ Global Steps:  3340/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  3340 │ Loss:N/A\n",
            "Ep  638 │ Global Steps:  3348/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  3348 │ Loss:N/A\n",
            "Ep  639 │ Global Steps:  3356/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  3356 │ Loss:N/A\n",
            "Ep  640 │ Global Steps:  3359/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  3359 │ Loss:N/A\n",
            "Ep  641 │ Global Steps:  3362/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  3362 │ Loss:N/A\n",
            "Ep  642 │ Global Steps:  3365/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  3365 │ Loss:N/A\n",
            "Ep  643 │ Global Steps:  3370/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  3370 │ Loss:N/A\n",
            "Ep  644 │ Global Steps:  3378/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  3378 │ Loss:N/A\n",
            "Ep  645 │ Global Steps:  3386/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  3386 │ Loss:N/A\n",
            "Ep  646 │ Global Steps:  3389/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  3389 │ Loss:N/A\n",
            "Ep  647 │ Global Steps:  3397/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  3397 │ Loss:N/A\n",
            "Ep  648 │ Global Steps:  3400/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  3400 │ Loss:N/A\n",
            "Ep  649 │ Global Steps:  3403/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  3403 │ Loss:N/A\n",
            "Ep  650 │ Global Steps:  3406/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  3406 │ Loss:N/A\n",
            "Ep  651 │ Global Steps:  3414/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  3414 │ Loss:N/A\n",
            "Ep  652 │ Global Steps:  3417/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  3417 │ Loss:N/A\n",
            "Ep  653 │ Global Steps:  3420/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  3420 │ Loss:N/A\n",
            "Ep  654 │ Global Steps:  3423/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  3423 │ Loss:N/A\n",
            "Ep  655 │ Global Steps:  3426/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  3426 │ Loss:N/A\n",
            "Ep  656 │ Global Steps:  3431/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:  3431 │ Loss:N/A\n",
            "Ep  657 │ Global Steps:  3436/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  3436 │ Loss:N/A\n",
            "Ep  658 │ Global Steps:  3444/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  3444 │ Loss:N/A\n",
            "Ep  659 │ Global Steps:  3452/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  3452 │ Loss:N/A\n",
            "Ep  660 │ Global Steps:  3455/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  3455 │ Loss:N/A\n",
            "Ep  661 │ Global Steps:  3458/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  3458 │ Loss:N/A\n",
            "Ep  662 │ Global Steps:  3463/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  3463 │ Loss:N/A\n",
            "Ep  663 │ Global Steps:  3468/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  3468 │ Loss:N/A\n",
            "Ep  664 │ Global Steps:  3471/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  3471 │ Loss:N/A\n",
            "Ep  665 │ Global Steps:  3479/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  3479 │ Loss:N/A\n",
            "Ep  666 │ Global Steps:  3482/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.19 │ Epsilon:1.000 │ Buffer:  3482 │ Loss:N/A\n",
            "Ep  667 │ Global Steps:  3487/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  3487 │ Loss:N/A\n",
            "Ep  668 │ Global Steps:  3495/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  3495 │ Loss:N/A\n",
            "Ep  669 │ Global Steps:  3498/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  3498 │ Loss:N/A\n",
            "Ep  670 │ Global Steps:  3503/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  3503 │ Loss:N/A\n",
            "Ep  671 │ Global Steps:  3511/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  3511 │ Loss:N/A\n",
            "Ep  672 │ Global Steps:  3514/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  3514 │ Loss:N/A\n",
            "Ep  673 │ Global Steps:  3517/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.19 │ Epsilon:1.000 │ Buffer:  3517 │ Loss:N/A\n",
            "Ep  674 │ Global Steps:  3525/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.19 │ Epsilon:1.000 │ Buffer:  3525 │ Loss:N/A\n",
            "Ep  675 │ Global Steps:  3533/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  3533 │ Loss:N/A\n",
            "Ep  676 │ Global Steps:  3541/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  3541 │ Loss:N/A\n",
            "Ep  677 │ Global Steps:  3544/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  3544 │ Loss:N/A\n",
            "Ep  678 │ Global Steps:  3549/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  3549 │ Loss:N/A\n",
            "Ep  679 │ Global Steps:  3557/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  3557 │ Loss:N/A\n",
            "Ep  680 │ Global Steps:  3562/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  3562 │ Loss:N/A\n",
            "Ep  681 │ Global Steps:  3570/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  3570 │ Loss:N/A\n",
            "Ep  682 │ Global Steps:  3573/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  3573 │ Loss:N/A\n",
            "Ep  683 │ Global Steps:  3581/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  3581 │ Loss:N/A\n",
            "Ep  684 │ Global Steps:  3586/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  3586 │ Loss:N/A\n",
            "Ep  685 │ Global Steps:  3591/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  3591 │ Loss:N/A\n",
            "Ep  686 │ Global Steps:  3599/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  3599 │ Loss:N/A\n",
            "Ep  687 │ Global Steps:  3607/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  3607 │ Loss:N/A\n",
            "Ep  688 │ Global Steps:  3610/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  3610 │ Loss:N/A\n",
            "Ep  689 │ Global Steps:  3615/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  3615 │ Loss:N/A\n",
            "Ep  690 │ Global Steps:  3623/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  3623 │ Loss:N/A\n",
            "Ep  691 │ Global Steps:  3626/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  3626 │ Loss:N/A\n",
            "Ep  692 │ Global Steps:  3631/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  3631 │ Loss:N/A\n",
            "Ep  693 │ Global Steps:  3634/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  3634 │ Loss:N/A\n",
            "Ep  694 │ Global Steps:  3639/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  3639 │ Loss:N/A\n",
            "Ep  695 │ Global Steps:  3647/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  3647 │ Loss:N/A\n",
            "Ep  696 │ Global Steps:  3650/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  3650 │ Loss:N/A\n",
            "Ep  697 │ Global Steps:  3655/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  3655 │ Loss:N/A\n",
            "Ep  698 │ Global Steps:  3660/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  3660 │ Loss:N/A\n",
            "Ep  699 │ Global Steps:  3665/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  3665 │ Loss:N/A\n",
            "Ep  700 │ Global Steps:  3668/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  3668 │ Loss:N/A\n",
            "Ep  701 │ Global Steps:  3673/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  3673 │ Loss:N/A\n",
            "Ep  702 │ Global Steps:  3678/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  3678 │ Loss:N/A\n",
            "Ep  703 │ Global Steps:  3683/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  3683 │ Loss:N/A\n",
            "Ep  704 │ Global Steps:  3686/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  3686 │ Loss:N/A\n",
            "Ep  705 │ Global Steps:  3694/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  3694 │ Loss:N/A\n",
            "Ep  706 │ Global Steps:  3699/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  3699 │ Loss:N/A\n",
            "Ep  707 │ Global Steps:  3702/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  3702 │ Loss:N/A\n",
            "Ep  708 │ Global Steps:  3707/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  3707 │ Loss:N/A\n",
            "Ep  709 │ Global Steps:  3712/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  3712 │ Loss:N/A\n",
            "Ep  710 │ Global Steps:  3720/10000000 │ Ep Return:   -5.65 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  3720 │ Loss:N/A\n",
            "Ep  711 │ Global Steps:  3725/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  3725 │ Loss:N/A\n",
            "Ep  712 │ Global Steps:  3728/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  3728 │ Loss:N/A\n",
            "Ep  713 │ Global Steps:  3733/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  3733 │ Loss:N/A\n",
            "Ep  714 │ Global Steps:  3736/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  3736 │ Loss:N/A\n",
            "Ep  715 │ Global Steps:  3739/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  3739 │ Loss:N/A\n",
            "Ep  716 │ Global Steps:  3744/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  3744 │ Loss:N/A\n",
            "Ep  717 │ Global Steps:  3749/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  3749 │ Loss:N/A\n",
            "Ep  718 │ Global Steps:  3757/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  3757 │ Loss:N/A\n",
            "Ep  719 │ Global Steps:  3762/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  3762 │ Loss:N/A\n",
            "Ep  720 │ Global Steps:  3770/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  3770 │ Loss:N/A\n",
            "Ep  721 │ Global Steps:  3775/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  3775 │ Loss:N/A\n",
            "Ep  722 │ Global Steps:  3778/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  3778 │ Loss:N/A\n",
            "Ep  723 │ Global Steps:  3781/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  3781 │ Loss:N/A\n",
            "Ep  724 │ Global Steps:  3784/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  3784 │ Loss:N/A\n",
            "Ep  725 │ Global Steps:  3787/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  3787 │ Loss:N/A\n",
            "Ep  726 │ Global Steps:  3792/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:  3792 │ Loss:N/A\n",
            "Ep  727 │ Global Steps:  3797/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  3797 │ Loss:N/A\n",
            "Ep  728 │ Global Steps:  3802/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.19 │ Epsilon:1.000 │ Buffer:  3802 │ Loss:N/A\n",
            "Ep  729 │ Global Steps:  3807/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:  3807 │ Loss:N/A\n",
            "Ep  730 │ Global Steps:  3815/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  3815 │ Loss:N/A\n",
            "Ep  731 │ Global Steps:  3823/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  3823 │ Loss:N/A\n",
            "Ep  732 │ Global Steps:  3828/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  3828 │ Loss:N/A\n",
            "Ep  733 │ Global Steps:  3836/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  3836 │ Loss:N/A\n",
            "Ep  734 │ Global Steps:  3844/10000000 │ Ep Return:   -5.65 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  3844 │ Loss:N/A\n",
            "Ep  735 │ Global Steps:  3849/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  3849 │ Loss:N/A\n",
            "Ep  736 │ Global Steps:  3852/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  3852 │ Loss:N/A\n",
            "Ep  737 │ Global Steps:  3860/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  3860 │ Loss:N/A\n",
            "Ep  738 │ Global Steps:  3863/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  3863 │ Loss:N/A\n",
            "Ep  739 │ Global Steps:  3871/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  3871 │ Loss:N/A\n",
            "Ep  740 │ Global Steps:  3874/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  3874 │ Loss:N/A\n",
            "Ep  741 │ Global Steps:  3882/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  3882 │ Loss:N/A\n",
            "Ep  742 │ Global Steps:  3890/10000000 │ Ep Return:   -5.72 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  3890 │ Loss:N/A\n",
            "Ep  743 │ Global Steps:  3898/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  3898 │ Loss:N/A\n",
            "Ep  744 │ Global Steps:  3906/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  3906 │ Loss:N/A\n",
            "Ep  745 │ Global Steps:  3909/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  3909 │ Loss:N/A\n",
            "Ep  746 │ Global Steps:  3912/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  3912 │ Loss:N/A\n",
            "Ep  747 │ Global Steps:  3915/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  3915 │ Loss:N/A\n",
            "Ep  748 │ Global Steps:  3920/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  3920 │ Loss:N/A\n",
            "Ep  749 │ Global Steps:  3928/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  3928 │ Loss:N/A\n",
            "Ep  750 │ Global Steps:  3931/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  3931 │ Loss:N/A\n",
            "Ep  751 │ Global Steps:  3936/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  3936 │ Loss:N/A\n",
            "Ep  752 │ Global Steps:  3941/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  3941 │ Loss:N/A\n",
            "Ep  753 │ Global Steps:  3944/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  3944 │ Loss:N/A\n",
            "Ep  754 │ Global Steps:  3947/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  3947 │ Loss:N/A\n",
            "Ep  755 │ Global Steps:  3950/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  3950 │ Loss:N/A\n",
            "Ep  756 │ Global Steps:  3953/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  3953 │ Loss:N/A\n",
            "Ep  757 │ Global Steps:  3961/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  3961 │ Loss:N/A\n",
            "Ep  758 │ Global Steps:  3969/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  3969 │ Loss:N/A\n",
            "Ep  759 │ Global Steps:  3972/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  3972 │ Loss:N/A\n",
            "Ep  760 │ Global Steps:  3975/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  3975 │ Loss:N/A\n",
            "Ep  761 │ Global Steps:  3978/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  3978 │ Loss:N/A\n",
            "Ep  762 │ Global Steps:  3981/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  3981 │ Loss:N/A\n",
            "Ep  763 │ Global Steps:  3986/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  3986 │ Loss:N/A\n",
            "Ep  764 │ Global Steps:  3989/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  3989 │ Loss:N/A\n",
            "Ep  765 │ Global Steps:  3994/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  3994 │ Loss:N/A\n",
            "Ep  766 │ Global Steps:  3997/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  3997 │ Loss:N/A\n",
            "Ep  767 │ Global Steps:  4002/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  4002 │ Loss:N/A\n",
            "Ep  768 │ Global Steps:  4005/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  4005 │ Loss:N/A\n",
            "Ep  769 │ Global Steps:  4013/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  4013 │ Loss:N/A\n",
            "Ep  770 │ Global Steps:  4021/10000000 │ Ep Return:   -5.72 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  4021 │ Loss:N/A\n",
            "Ep  771 │ Global Steps:  4026/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  4026 │ Loss:N/A\n",
            "Ep  772 │ Global Steps:  4029/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  4029 │ Loss:N/A\n",
            "Ep  773 │ Global Steps:  4037/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  4037 │ Loss:N/A\n",
            "Ep  774 │ Global Steps:  4042/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  4042 │ Loss:N/A\n",
            "Ep  775 │ Global Steps:  4047/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  4047 │ Loss:N/A\n",
            "Ep  776 │ Global Steps:  4050/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  4050 │ Loss:N/A\n",
            "Ep  777 │ Global Steps:  4055/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  4055 │ Loss:N/A\n",
            "Ep  778 │ Global Steps:  4063/10000000 │ Ep Return:   -5.65 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  4063 │ Loss:N/A\n",
            "Ep  779 │ Global Steps:  4071/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  4071 │ Loss:N/A\n",
            "Ep  780 │ Global Steps:  4076/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  4076 │ Loss:N/A\n",
            "Ep  781 │ Global Steps:  4081/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  4081 │ Loss:N/A\n",
            "Ep  782 │ Global Steps:  4089/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  4089 │ Loss:N/A\n",
            "Ep  783 │ Global Steps:  4092/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  4092 │ Loss:N/A\n",
            "Ep  784 │ Global Steps:  4097/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  4097 │ Loss:N/A\n",
            "Ep  785 │ Global Steps:  4105/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  4105 │ Loss:N/A\n",
            "Ep  786 │ Global Steps:  4110/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  4110 │ Loss:N/A\n",
            "Ep  787 │ Global Steps:  4115/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  4115 │ Loss:N/A\n",
            "Ep  788 │ Global Steps:  4120/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  4120 │ Loss:N/A\n",
            "Ep  789 │ Global Steps:  4123/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  4123 │ Loss:N/A\n",
            "Ep  790 │ Global Steps:  4128/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  4128 │ Loss:N/A\n",
            "Ep  791 │ Global Steps:  4131/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  4131 │ Loss:N/A\n",
            "Ep  792 │ Global Steps:  4134/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  4134 │ Loss:N/A\n",
            "Ep  793 │ Global Steps:  4139/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  4139 │ Loss:N/A\n",
            "Ep  794 │ Global Steps:  4147/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  4147 │ Loss:N/A\n",
            "Ep  795 │ Global Steps:  4152/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  4152 │ Loss:N/A\n",
            "Ep  796 │ Global Steps:  4157/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  4157 │ Loss:N/A\n",
            "Ep  797 │ Global Steps:  4162/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  4162 │ Loss:N/A\n",
            "Ep  798 │ Global Steps:  4165/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  4165 │ Loss:N/A\n",
            "Ep  799 │ Global Steps:  4173/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  4173 │ Loss:N/A\n",
            "Ep  800 │ Global Steps:  4178/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  4178 │ Loss:N/A\n",
            "Ep  801 │ Global Steps:  4183/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  4183 │ Loss:N/A\n",
            "Ep  802 │ Global Steps:  4186/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  4186 │ Loss:N/A\n",
            "Ep  803 │ Global Steps:  4191/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  4191 │ Loss:N/A\n",
            "Ep  804 │ Global Steps:  4194/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  4194 │ Loss:N/A\n",
            "Ep  805 │ Global Steps:  4202/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  4202 │ Loss:N/A\n",
            "Ep  806 │ Global Steps:  4210/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  4210 │ Loss:N/A\n",
            "Ep  807 │ Global Steps:  4213/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  4213 │ Loss:N/A\n",
            "Ep  808 │ Global Steps:  4221/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  4221 │ Loss:N/A\n",
            "Ep  809 │ Global Steps:  4226/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  4226 │ Loss:N/A\n",
            "Ep  810 │ Global Steps:  4229/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  4229 │ Loss:N/A\n",
            "Ep  811 │ Global Steps:  4237/10000000 │ Ep Return:   -5.65 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  4237 │ Loss:N/A\n",
            "Ep  812 │ Global Steps:  4245/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  4245 │ Loss:N/A\n",
            "Ep  813 │ Global Steps:  4248/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  4248 │ Loss:N/A\n",
            "Ep  814 │ Global Steps:  4253/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  4253 │ Loss:N/A\n",
            "Ep  815 │ Global Steps:  4256/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  4256 │ Loss:N/A\n",
            "Ep  816 │ Global Steps:  4259/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  4259 │ Loss:N/A\n",
            "Ep  817 │ Global Steps:  4262/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  4262 │ Loss:N/A\n",
            "Ep  818 │ Global Steps:  4265/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  4265 │ Loss:N/A\n",
            "Ep  819 │ Global Steps:  4270/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  4270 │ Loss:N/A\n",
            "Ep  820 │ Global Steps:  4275/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  4275 │ Loss:N/A\n",
            "Ep  821 │ Global Steps:  4283/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  4283 │ Loss:N/A\n",
            "Ep  822 │ Global Steps:  4291/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  4291 │ Loss:N/A\n",
            "Ep  823 │ Global Steps:  4296/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  4296 │ Loss:N/A\n",
            "Ep  824 │ Global Steps:  4304/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  4304 │ Loss:N/A\n",
            "Ep  825 │ Global Steps:  4307/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  4307 │ Loss:N/A\n",
            "Ep  826 │ Global Steps:  4315/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  4315 │ Loss:N/A\n",
            "Ep  827 │ Global Steps:  4318/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  4318 │ Loss:N/A\n",
            "Ep  828 │ Global Steps:  4321/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  4321 │ Loss:N/A\n",
            "Ep  829 │ Global Steps:  4326/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  4326 │ Loss:N/A\n",
            "Ep  830 │ Global Steps:  4331/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  4331 │ Loss:N/A\n",
            "Ep  831 │ Global Steps:  4334/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  4334 │ Loss:N/A\n",
            "Ep  832 │ Global Steps:  4342/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  4342 │ Loss:N/A\n",
            "Ep  833 │ Global Steps:  4345/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  4345 │ Loss:N/A\n",
            "Ep  834 │ Global Steps:  4353/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  4353 │ Loss:N/A\n",
            "Ep  835 │ Global Steps:  4358/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  4358 │ Loss:N/A\n",
            "Ep  836 │ Global Steps:  4363/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  4363 │ Loss:N/A\n",
            "Ep  837 │ Global Steps:  4368/10000000 │ Ep Return:   -3.22 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  4368 │ Loss:N/A\n",
            "Ep  838 │ Global Steps:  4373/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  4373 │ Loss:N/A\n",
            "Ep  839 │ Global Steps:  4376/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  4376 │ Loss:N/A\n",
            "Ep  840 │ Global Steps:  4379/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  4379 │ Loss:N/A\n",
            "Ep  841 │ Global Steps:  4382/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  4382 │ Loss:N/A\n",
            "Ep  842 │ Global Steps:  4385/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.19 │ Epsilon:1.000 │ Buffer:  4385 │ Loss:N/A\n",
            "Ep  843 │ Global Steps:  4388/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.15 │ Epsilon:1.000 │ Buffer:  4388 │ Loss:N/A\n",
            "Ep  844 │ Global Steps:  4393/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.13 │ Epsilon:1.000 │ Buffer:  4393 │ Loss:N/A\n",
            "Ep  845 │ Global Steps:  4401/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.17 │ Epsilon:1.000 │ Buffer:  4401 │ Loss:N/A\n",
            "Ep  846 │ Global Steps:  4409/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  4409 │ Loss:N/A\n",
            "Ep  847 │ Global Steps:  4414/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  4414 │ Loss:N/A\n",
            "Ep  848 │ Global Steps:  4417/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  4417 │ Loss:N/A\n",
            "Ep  849 │ Global Steps:  4422/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.18 │ Epsilon:1.000 │ Buffer:  4422 │ Loss:N/A\n",
            "Ep  850 │ Global Steps:  4430/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  4430 │ Loss:N/A\n",
            "Ep  851 │ Global Steps:  4433/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  4433 │ Loss:N/A\n",
            "Ep  852 │ Global Steps:  4441/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  4441 │ Loss:N/A\n",
            "Ep  853 │ Global Steps:  4446/10000000 │ Ep Return:   -3.22 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  4446 │ Loss:N/A\n",
            "Ep  854 │ Global Steps:  4449/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  4449 │ Loss:N/A\n",
            "Ep  855 │ Global Steps:  4454/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  4454 │ Loss:N/A\n",
            "Ep  856 │ Global Steps:  4457/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  4457 │ Loss:N/A\n",
            "Ep  857 │ Global Steps:  4462/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  4462 │ Loss:N/A\n",
            "Ep  858 │ Global Steps:  4467/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  4467 │ Loss:N/A\n",
            "Ep  859 │ Global Steps:  4472/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  4472 │ Loss:N/A\n",
            "Ep  860 │ Global Steps:  4475/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  4475 │ Loss:N/A\n",
            "Ep  861 │ Global Steps:  4480/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  4480 │ Loss:N/A\n",
            "Ep  862 │ Global Steps:  4485/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  4485 │ Loss:N/A\n",
            "Ep  863 │ Global Steps:  4493/10000000 │ Ep Return:   -5.70 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  4493 │ Loss:N/A\n",
            "Ep  864 │ Global Steps:  4498/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  4498 │ Loss:N/A\n",
            "Ep  865 │ Global Steps:  4503/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  4503 │ Loss:N/A\n",
            "Ep  866 │ Global Steps:  4506/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  4506 │ Loss:N/A\n",
            "Ep  867 │ Global Steps:  4511/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  4511 │ Loss:N/A\n",
            "Ep  868 │ Global Steps:  4519/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  4519 │ Loss:N/A\n",
            "Ep  869 │ Global Steps:  4524/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  4524 │ Loss:N/A\n",
            "Ep  870 │ Global Steps:  4529/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  4529 │ Loss:N/A\n",
            "Ep  871 │ Global Steps:  4534/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  4534 │ Loss:N/A\n",
            "Ep  872 │ Global Steps:  4537/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  4537 │ Loss:N/A\n",
            "Ep  873 │ Global Steps:  4545/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  4545 │ Loss:N/A\n",
            "Ep  874 │ Global Steps:  4548/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  4548 │ Loss:N/A\n",
            "Ep  875 │ Global Steps:  4553/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  4553 │ Loss:N/A\n",
            "Ep  876 │ Global Steps:  4558/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  4558 │ Loss:N/A\n",
            "Ep  877 │ Global Steps:  4563/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  4563 │ Loss:N/A\n",
            "Ep  878 │ Global Steps:  4566/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  4566 │ Loss:N/A\n",
            "Ep  879 │ Global Steps:  4569/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  4569 │ Loss:N/A\n",
            "Ep  880 │ Global Steps:  4577/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  4577 │ Loss:N/A\n",
            "Ep  881 │ Global Steps:  4580/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  4580 │ Loss:N/A\n",
            "Ep  882 │ Global Steps:  4585/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:  4585 │ Loss:N/A\n",
            "Ep  883 │ Global Steps:  4588/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:  4588 │ Loss:N/A\n",
            "Ep  884 │ Global Steps:  4591/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.18 │ Epsilon:1.000 │ Buffer:  4591 │ Loss:N/A\n",
            "Ep  885 │ Global Steps:  4599/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.18 │ Epsilon:1.000 │ Buffer:  4599 │ Loss:N/A\n",
            "Ep  886 │ Global Steps:  4607/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:  4607 │ Loss:N/A\n",
            "Ep  887 │ Global Steps:  4615/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  4615 │ Loss:N/A\n",
            "Ep  888 │ Global Steps:  4623/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  4623 │ Loss:N/A\n",
            "Ep  889 │ Global Steps:  4628/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  4628 │ Loss:N/A\n",
            "Ep  890 │ Global Steps:  4631/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  4631 │ Loss:N/A\n",
            "Ep  891 │ Global Steps:  4634/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  4634 │ Loss:N/A\n",
            "Ep  892 │ Global Steps:  4637/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  4637 │ Loss:N/A\n",
            "Ep  893 │ Global Steps:  4645/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  4645 │ Loss:N/A\n",
            "Ep  894 │ Global Steps:  4648/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  4648 │ Loss:N/A\n",
            "Ep  895 │ Global Steps:  4653/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  4653 │ Loss:N/A\n",
            "Ep  896 │ Global Steps:  4661/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  4661 │ Loss:N/A\n",
            "Ep  897 │ Global Steps:  4669/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  4669 │ Loss:N/A\n",
            "Ep  898 │ Global Steps:  4672/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  4672 │ Loss:N/A\n",
            "Ep  899 │ Global Steps:  4680/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  4680 │ Loss:N/A\n",
            "Ep  900 │ Global Steps:  4685/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  4685 │ Loss:N/A\n",
            "Ep  901 │ Global Steps:  4693/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  4693 │ Loss:N/A\n",
            "Ep  902 │ Global Steps:  4698/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  4698 │ Loss:N/A\n",
            "Ep  903 │ Global Steps:  4701/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  4701 │ Loss:N/A\n",
            "Ep  904 │ Global Steps:  4704/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  4704 │ Loss:N/A\n",
            "Ep  905 │ Global Steps:  4712/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  4712 │ Loss:N/A\n",
            "Ep  906 │ Global Steps:  4715/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  4715 │ Loss:N/A\n",
            "Ep  907 │ Global Steps:  4723/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  4723 │ Loss:N/A\n",
            "Ep  908 │ Global Steps:  4731/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  4731 │ Loss:N/A\n",
            "Ep  909 │ Global Steps:  4739/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  4739 │ Loss:N/A\n",
            "Ep  910 │ Global Steps:  4742/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  4742 │ Loss:N/A\n",
            "Ep  911 │ Global Steps:  4750/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  4750 │ Loss:N/A\n",
            "Ep  912 │ Global Steps:  4753/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  4753 │ Loss:N/A\n",
            "Ep  913 │ Global Steps:  4756/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  4756 │ Loss:N/A\n",
            "Ep  914 │ Global Steps:  4759/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  4759 │ Loss:N/A\n",
            "Ep  915 │ Global Steps:  4767/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  4767 │ Loss:N/A\n",
            "Ep  916 │ Global Steps:  4775/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  4775 │ Loss:N/A\n",
            "Ep  917 │ Global Steps:  4778/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  4778 │ Loss:N/A\n",
            "Ep  918 │ Global Steps:  4786/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  4786 │ Loss:N/A\n",
            "Ep  919 │ Global Steps:  4789/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  4789 │ Loss:N/A\n",
            "Ep  920 │ Global Steps:  4794/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  4794 │ Loss:N/A\n",
            "Ep  921 │ Global Steps:  4797/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  4797 │ Loss:N/A\n",
            "Ep  922 │ Global Steps:  4805/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  4805 │ Loss:N/A\n",
            "Ep  923 │ Global Steps:  4813/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  4813 │ Loss:N/A\n",
            "Ep  924 │ Global Steps:  4821/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  4821 │ Loss:N/A\n",
            "Ep  925 │ Global Steps:  4824/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  4824 │ Loss:N/A\n",
            "Ep  926 │ Global Steps:  4829/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  4829 │ Loss:N/A\n",
            "Ep  927 │ Global Steps:  4832/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  4832 │ Loss:N/A\n",
            "Ep  928 │ Global Steps:  4840/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  4840 │ Loss:N/A\n",
            "Ep  929 │ Global Steps:  4843/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  4843 │ Loss:N/A\n",
            "Ep  930 │ Global Steps:  4851/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  4851 │ Loss:N/A\n",
            "Ep  931 │ Global Steps:  4859/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  4859 │ Loss:N/A\n",
            "Ep  932 │ Global Steps:  4862/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  4862 │ Loss:N/A\n",
            "Ep  933 │ Global Steps:  4867/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  4867 │ Loss:N/A\n",
            "Ep  934 │ Global Steps:  4875/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  4875 │ Loss:N/A\n",
            "Ep  935 │ Global Steps:  4883/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  4883 │ Loss:N/A\n",
            "Ep  936 │ Global Steps:  4888/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  4888 │ Loss:N/A\n",
            "Ep  937 │ Global Steps:  4896/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  4896 │ Loss:N/A\n",
            "Ep  938 │ Global Steps:  4899/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.44 │ Epsilon:1.000 │ Buffer:  4899 │ Loss:N/A\n",
            "Ep  939 │ Global Steps:  4904/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  4904 │ Loss:N/A\n",
            "Ep  940 │ Global Steps:  4909/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  4909 │ Loss:N/A\n",
            "Ep  941 │ Global Steps:  4914/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.49 │ Epsilon:1.000 │ Buffer:  4914 │ Loss:N/A\n",
            "Ep  942 │ Global Steps:  4922/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.53 │ Epsilon:1.000 │ Buffer:  4922 │ Loss:N/A\n",
            "Ep  943 │ Global Steps:  4925/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.53 │ Epsilon:1.000 │ Buffer:  4925 │ Loss:N/A\n",
            "Ep  944 │ Global Steps:  4933/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:  4933 │ Loss:N/A\n",
            "Ep  945 │ Global Steps:  4938/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.53 │ Epsilon:1.000 │ Buffer:  4938 │ Loss:N/A\n",
            "Ep  946 │ Global Steps:  4946/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.53 │ Epsilon:1.000 │ Buffer:  4946 │ Loss:N/A\n",
            "Ep  947 │ Global Steps:  4954/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:  4954 │ Loss:N/A\n",
            "Ep  948 │ Global Steps:  4959/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.57 │ Epsilon:1.000 │ Buffer:  4959 │ Loss:N/A\n",
            "Ep  949 │ Global Steps:  4967/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.60 │ Epsilon:1.000 │ Buffer:  4967 │ Loss:N/A\n",
            "Ep  950 │ Global Steps:  4970/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:  4970 │ Loss:N/A\n",
            "Ep  951 │ Global Steps:  4978/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.60 │ Epsilon:1.000 │ Buffer:  4978 │ Loss:N/A\n",
            "Ep  952 │ Global Steps:  4986/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.60 │ Epsilon:1.000 │ Buffer:  4986 │ Loss:N/A\n",
            "Ep  953 │ Global Steps:  4989/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.58 │ Epsilon:1.000 │ Buffer:  4989 │ Loss:N/A\n",
            "Ep  954 │ Global Steps:  4997/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  4997 │ Loss:N/A\n",
            "Ep  955 │ Global Steps:  5005/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.65 │ Epsilon:1.000 │ Buffer:  5005 │ Loss:N/A\n",
            "Ep  956 │ Global Steps:  5010/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.66 │ Epsilon:1.000 │ Buffer:  5010 │ Loss:N/A\n",
            "Ep  957 │ Global Steps:  5015/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.66 │ Epsilon:1.000 │ Buffer:  5015 │ Loss:N/A\n",
            "Ep  958 │ Global Steps:  5018/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.65 │ Epsilon:1.000 │ Buffer:  5018 │ Loss:N/A\n",
            "Ep  959 │ Global Steps:  5026/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.67 │ Epsilon:1.000 │ Buffer:  5026 │ Loss:N/A\n",
            "Ep  960 │ Global Steps:  5034/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  5034 │ Loss:N/A\n",
            "Ep  961 │ Global Steps:  5039/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  5039 │ Loss:N/A\n",
            "Ep  962 │ Global Steps:  5044/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  5044 │ Loss:N/A\n",
            "Ep  963 │ Global Steps:  5049/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.69 │ Epsilon:1.000 │ Buffer:  5049 │ Loss:N/A\n",
            "Ep  964 │ Global Steps:  5057/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  5057 │ Loss:N/A\n",
            "Ep  965 │ Global Steps:  5062/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  5062 │ Loss:N/A\n",
            "Ep  966 │ Global Steps:  5065/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  5065 │ Loss:N/A\n",
            "Ep  967 │ Global Steps:  5073/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  5073 │ Loss:N/A\n",
            "Ep  968 │ Global Steps:  5078/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  5078 │ Loss:N/A\n",
            "Ep  969 │ Global Steps:  5083/10000000 │ Ep Return:   -3.22 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  5083 │ Loss:N/A\n",
            "Ep  970 │ Global Steps:  5088/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  5088 │ Loss:N/A\n",
            "Ep  971 │ Global Steps:  5096/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  5096 │ Loss:N/A\n",
            "Ep  972 │ Global Steps:  5099/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  5099 │ Loss:N/A\n",
            "Ep  973 │ Global Steps:  5102/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  5102 │ Loss:N/A\n",
            "Ep  974 │ Global Steps:  5110/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  5110 │ Loss:N/A\n",
            "Ep  975 │ Global Steps:  5115/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  5115 │ Loss:N/A\n",
            "Ep  976 │ Global Steps:  5123/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  5123 │ Loss:N/A\n",
            "Ep  977 │ Global Steps:  5126/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.75 │ Epsilon:1.000 │ Buffer:  5126 │ Loss:N/A\n",
            "Ep  978 │ Global Steps:  5134/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.79 │ Epsilon:1.000 │ Buffer:  5134 │ Loss:N/A\n",
            "Ep  979 │ Global Steps:  5137/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.79 │ Epsilon:1.000 │ Buffer:  5137 │ Loss:N/A\n",
            "Ep  980 │ Global Steps:  5142/10000000 │ Ep Return:   -3.22 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  5142 │ Loss:N/A\n",
            "Ep  981 │ Global Steps:  5150/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.80 │ Epsilon:1.000 │ Buffer:  5150 │ Loss:N/A\n",
            "Ep  982 │ Global Steps:  5153/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.79 │ Epsilon:1.000 │ Buffer:  5153 │ Loss:N/A\n",
            "Ep  983 │ Global Steps:  5156/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.79 │ Epsilon:1.000 │ Buffer:  5156 │ Loss:N/A\n",
            "Ep  984 │ Global Steps:  5161/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.80 │ Epsilon:1.000 │ Buffer:  5161 │ Loss:N/A\n",
            "Ep  985 │ Global Steps:  5164/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  5164 │ Loss:N/A\n",
            "Ep  986 │ Global Steps:  5172/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  5172 │ Loss:N/A\n",
            "Ep  987 │ Global Steps:  5180/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  5180 │ Loss:N/A\n",
            "Ep  988 │ Global Steps:  5183/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  5183 │ Loss:N/A\n",
            "Ep  989 │ Global Steps:  5188/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  5188 │ Loss:N/A\n",
            "Ep  990 │ Global Steps:  5193/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  5193 │ Loss:N/A\n",
            "Ep  991 │ Global Steps:  5196/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  5196 │ Loss:N/A\n",
            "Ep  992 │ Global Steps:  5199/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  5199 │ Loss:N/A\n",
            "Ep  993 │ Global Steps:  5207/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  5207 │ Loss:N/A\n",
            "Ep  994 │ Global Steps:  5212/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.75 │ Epsilon:1.000 │ Buffer:  5212 │ Loss:N/A\n",
            "Ep  995 │ Global Steps:  5215/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  5215 │ Loss:N/A\n",
            "Ep  996 │ Global Steps:  5223/10000000 │ Ep Return:   -5.65 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  5223 │ Loss:N/A\n",
            "Ep  997 │ Global Steps:  5226/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  5226 │ Loss:N/A\n",
            "Ep  998 │ Global Steps:  5234/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  5234 │ Loss:N/A\n",
            "Ep  999 │ Global Steps:  5239/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  5239 │ Loss:N/A\n",
            "Ep 1000 │ Global Steps:  5244/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  5244 │ Loss:N/A\n",
            "Ep 1001 │ Global Steps:  5252/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  5252 │ Loss:N/A\n",
            "Ep 1002 │ Global Steps:  5257/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  5257 │ Loss:N/A\n",
            "Ep 1003 │ Global Steps:  5262/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.73 │ Epsilon:1.000 │ Buffer:  5262 │ Loss:N/A\n",
            "Ep 1004 │ Global Steps:  5270/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.77 │ Epsilon:1.000 │ Buffer:  5270 │ Loss:N/A\n",
            "Ep 1005 │ Global Steps:  5273/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.73 │ Epsilon:1.000 │ Buffer:  5273 │ Loss:N/A\n",
            "Ep 1006 │ Global Steps:  5278/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  5278 │ Loss:N/A\n",
            "Ep 1007 │ Global Steps:  5286/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  5286 │ Loss:N/A\n",
            "Ep 1008 │ Global Steps:  5294/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  5294 │ Loss:N/A\n",
            "Ep 1009 │ Global Steps:  5299/10000000 │ Ep Return:   -3.22 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  5299 │ Loss:N/A\n",
            "Ep 1010 │ Global Steps:  5304/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  5304 │ Loss:N/A\n",
            "Ep 1011 │ Global Steps:  5307/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  5307 │ Loss:N/A\n",
            "Ep 1012 │ Global Steps:  5315/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  5315 │ Loss:N/A\n",
            "Ep 1013 │ Global Steps:  5318/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  5318 │ Loss:N/A\n",
            "Ep 1014 │ Global Steps:  5323/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.75 │ Epsilon:1.000 │ Buffer:  5323 │ Loss:N/A\n",
            "Ep 1015 │ Global Steps:  5326/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  5326 │ Loss:N/A\n",
            "Ep 1016 │ Global Steps:  5334/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  5334 │ Loss:N/A\n",
            "Ep 1017 │ Global Steps:  5339/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.73 │ Epsilon:1.000 │ Buffer:  5339 │ Loss:N/A\n",
            "Ep 1018 │ Global Steps:  5347/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.73 │ Epsilon:1.000 │ Buffer:  5347 │ Loss:N/A\n",
            "Ep 1019 │ Global Steps:  5350/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.73 │ Epsilon:1.000 │ Buffer:  5350 │ Loss:N/A\n",
            "Ep 1020 │ Global Steps:  5355/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.73 │ Epsilon:1.000 │ Buffer:  5355 │ Loss:N/A\n",
            "Ep 1021 │ Global Steps:  5363/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.77 │ Epsilon:1.000 │ Buffer:  5363 │ Loss:N/A\n",
            "Ep 1022 │ Global Steps:  5371/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.77 │ Epsilon:1.000 │ Buffer:  5371 │ Loss:N/A\n",
            "Ep 1023 │ Global Steps:  5376/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  5376 │ Loss:N/A\n",
            "Ep 1024 │ Global Steps:  5381/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  5381 │ Loss:N/A\n",
            "Ep 1025 │ Global Steps:  5389/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  5389 │ Loss:N/A\n",
            "Ep 1026 │ Global Steps:  5392/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  5392 │ Loss:N/A\n",
            "Ep 1027 │ Global Steps:  5397/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  5397 │ Loss:N/A\n",
            "Ep 1028 │ Global Steps:  5402/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.73 │ Epsilon:1.000 │ Buffer:  5402 │ Loss:N/A\n",
            "Ep 1029 │ Global Steps:  5407/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.75 │ Epsilon:1.000 │ Buffer:  5407 │ Loss:N/A\n",
            "Ep 1030 │ Global Steps:  5412/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  5412 │ Loss:N/A\n",
            "Ep 1031 │ Global Steps:  5415/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:  5415 │ Loss:N/A\n",
            "Ep 1032 │ Global Steps:  5418/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:  5418 │ Loss:N/A\n",
            "Ep 1033 │ Global Steps:  5421/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.67 │ Epsilon:1.000 │ Buffer:  5421 │ Loss:N/A\n",
            "Ep 1034 │ Global Steps:  5429/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.67 │ Epsilon:1.000 │ Buffer:  5429 │ Loss:N/A\n",
            "Ep 1035 │ Global Steps:  5437/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.67 │ Epsilon:1.000 │ Buffer:  5437 │ Loss:N/A\n",
            "Ep 1036 │ Global Steps:  5442/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.67 │ Epsilon:1.000 │ Buffer:  5442 │ Loss:N/A\n",
            "Ep 1037 │ Global Steps:  5450/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.67 │ Epsilon:1.000 │ Buffer:  5450 │ Loss:N/A\n",
            "Ep 1038 │ Global Steps:  5453/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.67 │ Epsilon:1.000 │ Buffer:  5453 │ Loss:N/A\n",
            "Ep 1039 │ Global Steps:  5461/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.69 │ Epsilon:1.000 │ Buffer:  5461 │ Loss:N/A\n",
            "Ep 1040 │ Global Steps:  5466/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.69 │ Epsilon:1.000 │ Buffer:  5466 │ Loss:N/A\n",
            "Ep 1041 │ Global Steps:  5469/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:  5469 │ Loss:N/A\n",
            "Ep 1042 │ Global Steps:  5472/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.63 │ Epsilon:1.000 │ Buffer:  5472 │ Loss:N/A\n",
            "Ep 1043 │ Global Steps:  5480/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:  5480 │ Loss:N/A\n",
            "Ep 1044 │ Global Steps:  5488/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:  5488 │ Loss:N/A\n",
            "Ep 1045 │ Global Steps:  5493/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:  5493 │ Loss:N/A\n",
            "Ep 1046 │ Global Steps:  5496/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.63 │ Epsilon:1.000 │ Buffer:  5496 │ Loss:N/A\n",
            "Ep 1047 │ Global Steps:  5504/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.63 │ Epsilon:1.000 │ Buffer:  5504 │ Loss:N/A\n",
            "Ep 1048 │ Global Steps:  5507/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  5507 │ Loss:N/A\n",
            "Ep 1049 │ Global Steps:  5515/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  5515 │ Loss:N/A\n",
            "Ep 1050 │ Global Steps:  5520/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.63 │ Epsilon:1.000 │ Buffer:  5520 │ Loss:N/A\n",
            "Ep 1051 │ Global Steps:  5523/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  5523 │ Loss:N/A\n",
            "Ep 1052 │ Global Steps:  5528/10000000 │ Ep Return:   -3.17 │ Avg Return (100 ep):   -3.57 │ Epsilon:1.000 │ Buffer:  5528 │ Loss:N/A\n",
            "Ep 1053 │ Global Steps:  5533/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  5533 │ Loss:N/A\n",
            "Ep 1054 │ Global Steps:  5536/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.54 │ Epsilon:1.000 │ Buffer:  5536 │ Loss:N/A\n",
            "Ep 1055 │ Global Steps:  5539/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  5539 │ Loss:N/A\n",
            "Ep 1056 │ Global Steps:  5544/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  5544 │ Loss:N/A\n",
            "Ep 1057 │ Global Steps:  5552/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.53 │ Epsilon:1.000 │ Buffer:  5552 │ Loss:N/A\n",
            "Ep 1058 │ Global Steps:  5560/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.57 │ Epsilon:1.000 │ Buffer:  5560 │ Loss:N/A\n",
            "Ep 1059 │ Global Steps:  5565/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.54 │ Epsilon:1.000 │ Buffer:  5565 │ Loss:N/A\n",
            "Ep 1060 │ Global Steps:  5568/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  5568 │ Loss:N/A\n",
            "Ep 1061 │ Global Steps:  5571/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.49 │ Epsilon:1.000 │ Buffer:  5571 │ Loss:N/A\n",
            "Ep 1062 │ Global Steps:  5574/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  5574 │ Loss:N/A\n",
            "Ep 1063 │ Global Steps:  5577/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  5577 │ Loss:N/A\n",
            "Ep 1064 │ Global Steps:  5580/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  5580 │ Loss:N/A\n",
            "Ep 1065 │ Global Steps:  5588/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.44 │ Epsilon:1.000 │ Buffer:  5588 │ Loss:N/A\n",
            "Ep 1066 │ Global Steps:  5596/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  5596 │ Loss:N/A\n",
            "Ep 1067 │ Global Steps:  5604/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  5604 │ Loss:N/A\n",
            "Ep 1068 │ Global Steps:  5607/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  5607 │ Loss:N/A\n",
            "Ep 1069 │ Global Steps:  5615/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.49 │ Epsilon:1.000 │ Buffer:  5615 │ Loss:N/A\n",
            "Ep 1070 │ Global Steps:  5623/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.51 │ Epsilon:1.000 │ Buffer:  5623 │ Loss:N/A\n",
            "Ep 1071 │ Global Steps:  5626/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  5626 │ Loss:N/A\n",
            "Ep 1072 │ Global Steps:  5631/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.49 │ Epsilon:1.000 │ Buffer:  5631 │ Loss:N/A\n",
            "Ep 1073 │ Global Steps:  5634/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.49 │ Epsilon:1.000 │ Buffer:  5634 │ Loss:N/A\n",
            "Ep 1074 │ Global Steps:  5637/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  5637 │ Loss:N/A\n",
            "Ep 1075 │ Global Steps:  5642/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  5642 │ Loss:N/A\n",
            "Ep 1076 │ Global Steps:  5645/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  5645 │ Loss:N/A\n",
            "Ep 1077 │ Global Steps:  5650/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  5650 │ Loss:N/A\n",
            "Ep 1078 │ Global Steps:  5653/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  5653 │ Loss:N/A\n",
            "Ep 1079 │ Global Steps:  5661/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  5661 │ Loss:N/A\n",
            "Ep 1080 │ Global Steps:  5664/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  5664 │ Loss:N/A\n",
            "Ep 1081 │ Global Steps:  5672/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  5672 │ Loss:N/A\n",
            "Ep 1082 │ Global Steps:  5675/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  5675 │ Loss:N/A\n",
            "Ep 1083 │ Global Steps:  5680/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  5680 │ Loss:N/A\n",
            "Ep 1084 │ Global Steps:  5685/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  5685 │ Loss:N/A\n",
            "Ep 1085 │ Global Steps:  5690/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.44 │ Epsilon:1.000 │ Buffer:  5690 │ Loss:N/A\n",
            "Ep 1086 │ Global Steps:  5698/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.44 │ Epsilon:1.000 │ Buffer:  5698 │ Loss:N/A\n",
            "Ep 1087 │ Global Steps:  5706/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.44 │ Epsilon:1.000 │ Buffer:  5706 │ Loss:N/A\n",
            "Ep 1088 │ Global Steps:  5711/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  5711 │ Loss:N/A\n",
            "Ep 1089 │ Global Steps:  5719/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  5719 │ Loss:N/A\n",
            "Ep 1090 │ Global Steps:  5722/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  5722 │ Loss:N/A\n",
            "Ep 1091 │ Global Steps:  5727/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  5727 │ Loss:N/A\n",
            "Ep 1092 │ Global Steps:  5730/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  5730 │ Loss:N/A\n",
            "Ep 1093 │ Global Steps:  5735/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  5735 │ Loss:N/A\n",
            "Ep 1094 │ Global Steps:  5740/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  5740 │ Loss:N/A\n",
            "Ep 1095 │ Global Steps:  5745/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  5745 │ Loss:N/A\n",
            "Ep 1096 │ Global Steps:  5750/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  5750 │ Loss:N/A\n",
            "Ep 1097 │ Global Steps:  5753/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  5753 │ Loss:N/A\n",
            "Ep 1098 │ Global Steps:  5761/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  5761 │ Loss:N/A\n",
            "Ep 1099 │ Global Steps:  5766/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  5766 │ Loss:N/A\n",
            "Ep 1100 │ Global Steps:  5769/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  5769 │ Loss:N/A\n",
            "Ep 1101 │ Global Steps:  5777/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  5777 │ Loss:N/A\n",
            "Ep 1102 │ Global Steps:  5785/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  5785 │ Loss:N/A\n",
            "Ep 1103 │ Global Steps:  5790/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  5790 │ Loss:N/A\n",
            "Ep 1104 │ Global Steps:  5795/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  5795 │ Loss:N/A\n",
            "Ep 1105 │ Global Steps:  5798/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  5798 │ Loss:N/A\n",
            "Ep 1106 │ Global Steps:  5803/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  5803 │ Loss:N/A\n",
            "Ep 1107 │ Global Steps:  5808/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  5808 │ Loss:N/A\n",
            "Ep 1108 │ Global Steps:  5816/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  5816 │ Loss:N/A\n",
            "Ep 1109 │ Global Steps:  5821/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  5821 │ Loss:N/A\n",
            "Ep 1110 │ Global Steps:  5826/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  5826 │ Loss:N/A\n",
            "Ep 1111 │ Global Steps:  5831/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  5831 │ Loss:N/A\n",
            "Ep 1112 │ Global Steps:  5839/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  5839 │ Loss:N/A\n",
            "Ep 1113 │ Global Steps:  5844/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.44 │ Epsilon:1.000 │ Buffer:  5844 │ Loss:N/A\n",
            "Ep 1114 │ Global Steps:  5847/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  5847 │ Loss:N/A\n",
            "Ep 1115 │ Global Steps:  5855/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  5855 │ Loss:N/A\n",
            "Ep 1116 │ Global Steps:  5858/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  5858 │ Loss:N/A\n",
            "Ep 1117 │ Global Steps:  5861/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  5861 │ Loss:N/A\n",
            "Ep 1118 │ Global Steps:  5864/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  5864 │ Loss:N/A\n",
            "Ep 1119 │ Global Steps:  5872/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  5872 │ Loss:N/A\n",
            "Ep 1120 │ Global Steps:  5880/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  5880 │ Loss:N/A\n",
            "Ep 1121 │ Global Steps:  5883/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  5883 │ Loss:N/A\n",
            "Ep 1122 │ Global Steps:  5886/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  5886 │ Loss:N/A\n",
            "Ep 1123 │ Global Steps:  5889/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  5889 │ Loss:N/A\n",
            "Ep 1124 │ Global Steps:  5892/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  5892 │ Loss:N/A\n",
            "Ep 1125 │ Global Steps:  5895/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  5895 │ Loss:N/A\n",
            "Ep 1126 │ Global Steps:  5900/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  5900 │ Loss:N/A\n",
            "Ep 1127 │ Global Steps:  5905/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  5905 │ Loss:N/A\n",
            "Ep 1128 │ Global Steps:  5910/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  5910 │ Loss:N/A\n",
            "Ep 1129 │ Global Steps:  5918/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  5918 │ Loss:N/A\n",
            "Ep 1130 │ Global Steps:  5921/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  5921 │ Loss:N/A\n",
            "Ep 1131 │ Global Steps:  5924/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  5924 │ Loss:N/A\n",
            "Ep 1132 │ Global Steps:  5929/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  5929 │ Loss:N/A\n",
            "Ep 1133 │ Global Steps:  5934/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  5934 │ Loss:N/A\n",
            "Ep 1134 │ Global Steps:  5937/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  5937 │ Loss:N/A\n",
            "Ep 1135 │ Global Steps:  5940/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  5940 │ Loss:N/A\n",
            "Ep 1136 │ Global Steps:  5948/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  5948 │ Loss:N/A\n",
            "Ep 1137 │ Global Steps:  5953/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  5953 │ Loss:N/A\n",
            "Ep 1138 │ Global Steps:  5961/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  5961 │ Loss:N/A\n",
            "Ep 1139 │ Global Steps:  5964/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  5964 │ Loss:N/A\n",
            "Ep 1140 │ Global Steps:  5969/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  5969 │ Loss:N/A\n",
            "Ep 1141 │ Global Steps:  5977/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  5977 │ Loss:N/A\n",
            "Ep 1142 │ Global Steps:  5980/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  5980 │ Loss:N/A\n",
            "Ep 1143 │ Global Steps:  5983/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  5983 │ Loss:N/A\n",
            "Ep 1144 │ Global Steps:  5991/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  5991 │ Loss:N/A\n",
            "Ep 1145 │ Global Steps:  5994/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.24 │ Epsilon:1.000 │ Buffer:  5994 │ Loss:N/A\n",
            "Ep 1146 │ Global Steps:  5999/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  5999 │ Loss:N/A\n",
            "Ep 1147 │ Global Steps:  6002/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  6002 │ Loss:N/A\n",
            "Ep 1148 │ Global Steps:  6007/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  6007 │ Loss:N/A\n",
            "Ep 1149 │ Global Steps:  6012/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:  6012 │ Loss:N/A\n",
            "Ep 1150 │ Global Steps:  6017/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:  6017 │ Loss:N/A\n",
            "Ep 1151 │ Global Steps:  6022/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  6022 │ Loss:N/A\n",
            "Ep 1152 │ Global Steps:  6025/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.20 │ Epsilon:1.000 │ Buffer:  6025 │ Loss:N/A\n",
            "Ep 1153 │ Global Steps:  6028/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.19 │ Epsilon:1.000 │ Buffer:  6028 │ Loss:N/A\n",
            "Ep 1154 │ Global Steps:  6031/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.19 │ Epsilon:1.000 │ Buffer:  6031 │ Loss:N/A\n",
            "Ep 1155 │ Global Steps:  6034/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.19 │ Epsilon:1.000 │ Buffer:  6034 │ Loss:N/A\n",
            "Ep 1156 │ Global Steps:  6037/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.17 │ Epsilon:1.000 │ Buffer:  6037 │ Loss:N/A\n",
            "Ep 1157 │ Global Steps:  6042/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.15 │ Epsilon:1.000 │ Buffer:  6042 │ Loss:N/A\n",
            "Ep 1158 │ Global Steps:  6050/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.15 │ Epsilon:1.000 │ Buffer:  6050 │ Loss:N/A\n",
            "Ep 1159 │ Global Steps:  6058/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.17 │ Epsilon:1.000 │ Buffer:  6058 │ Loss:N/A\n",
            "Ep 1160 │ Global Steps:  6061/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.17 │ Epsilon:1.000 │ Buffer:  6061 │ Loss:N/A\n",
            "Ep 1161 │ Global Steps:  6069/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  6069 │ Loss:N/A\n",
            "Ep 1162 │ Global Steps:  6077/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  6077 │ Loss:N/A\n",
            "Ep 1163 │ Global Steps:  6080/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  6080 │ Loss:N/A\n",
            "Ep 1164 │ Global Steps:  6088/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  6088 │ Loss:N/A\n",
            "Ep 1165 │ Global Steps:  6096/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  6096 │ Loss:N/A\n",
            "Ep 1166 │ Global Steps:  6104/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  6104 │ Loss:N/A\n",
            "Ep 1167 │ Global Steps:  6107/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  6107 │ Loss:N/A\n",
            "Ep 1168 │ Global Steps:  6110/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  6110 │ Loss:N/A\n",
            "Ep 1169 │ Global Steps:  6113/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  6113 │ Loss:N/A\n",
            "Ep 1170 │ Global Steps:  6118/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.19 │ Epsilon:1.000 │ Buffer:  6118 │ Loss:N/A\n",
            "Ep 1171 │ Global Steps:  6123/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  6123 │ Loss:N/A\n",
            "Ep 1172 │ Global Steps:  6131/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  6131 │ Loss:N/A\n",
            "Ep 1173 │ Global Steps:  6139/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  6139 │ Loss:N/A\n",
            "Ep 1174 │ Global Steps:  6147/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  6147 │ Loss:N/A\n",
            "Ep 1175 │ Global Steps:  6150/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  6150 │ Loss:N/A\n",
            "Ep 1176 │ Global Steps:  6155/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  6155 │ Loss:N/A\n",
            "Ep 1177 │ Global Steps:  6158/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  6158 │ Loss:N/A\n",
            "Ep 1178 │ Global Steps:  6161/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  6161 │ Loss:N/A\n",
            "Ep 1179 │ Global Steps:  6169/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  6169 │ Loss:N/A\n",
            "Ep 1180 │ Global Steps:  6172/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  6172 │ Loss:N/A\n",
            "Ep 1181 │ Global Steps:  6177/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  6177 │ Loss:N/A\n",
            "Ep 1182 │ Global Steps:  6182/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  6182 │ Loss:N/A\n",
            "Ep 1183 │ Global Steps:  6190/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  6190 │ Loss:N/A\n",
            "Ep 1184 │ Global Steps:  6198/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  6198 │ Loss:N/A\n",
            "Ep 1185 │ Global Steps:  6206/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  6206 │ Loss:N/A\n",
            "Ep 1186 │ Global Steps:  6209/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  6209 │ Loss:N/A\n",
            "Ep 1187 │ Global Steps:  6212/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  6212 │ Loss:N/A\n",
            "Ep 1188 │ Global Steps:  6215/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  6215 │ Loss:N/A\n",
            "Ep 1189 │ Global Steps:  6218/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  6218 │ Loss:N/A\n",
            "Ep 1190 │ Global Steps:  6226/10000000 │ Ep Return:   -5.65 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  6226 │ Loss:N/A\n",
            "Ep 1191 │ Global Steps:  6234/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  6234 │ Loss:N/A\n",
            "Ep 1192 │ Global Steps:  6239/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  6239 │ Loss:N/A\n",
            "Ep 1193 │ Global Steps:  6242/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  6242 │ Loss:N/A\n",
            "Ep 1194 │ Global Steps:  6245/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  6245 │ Loss:N/A\n",
            "Ep 1195 │ Global Steps:  6253/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  6253 │ Loss:N/A\n",
            "Ep 1196 │ Global Steps:  6256/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  6256 │ Loss:N/A\n",
            "Ep 1197 │ Global Steps:  6261/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  6261 │ Loss:N/A\n",
            "Ep 1198 │ Global Steps:  6266/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  6266 │ Loss:N/A\n",
            "Ep 1199 │ Global Steps:  6274/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  6274 │ Loss:N/A\n",
            "Ep 1200 │ Global Steps:  6279/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  6279 │ Loss:N/A\n",
            "Ep 1201 │ Global Steps:  6287/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  6287 │ Loss:N/A\n",
            "Ep 1202 │ Global Steps:  6292/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  6292 │ Loss:N/A\n",
            "Ep 1203 │ Global Steps:  6297/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  6297 │ Loss:N/A\n",
            "Ep 1204 │ Global Steps:  6305/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  6305 │ Loss:N/A\n",
            "Ep 1205 │ Global Steps:  6308/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  6308 │ Loss:N/A\n",
            "Ep 1206 │ Global Steps:  6313/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  6313 │ Loss:N/A\n",
            "Ep 1207 │ Global Steps:  6321/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  6321 │ Loss:N/A\n",
            "Ep 1208 │ Global Steps:  6326/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  6326 │ Loss:N/A\n",
            "Ep 1209 │ Global Steps:  6331/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  6331 │ Loss:N/A\n",
            "Ep 1210 │ Global Steps:  6339/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  6339 │ Loss:N/A\n",
            "Ep 1211 │ Global Steps:  6342/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  6342 │ Loss:N/A\n",
            "Ep 1212 │ Global Steps:  6345/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  6345 │ Loss:N/A\n",
            "Ep 1213 │ Global Steps:  6353/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  6353 │ Loss:N/A\n",
            "Ep 1214 │ Global Steps:  6361/10000000 │ Ep Return:   -5.65 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  6361 │ Loss:N/A\n",
            "Ep 1215 │ Global Steps:  6366/10000000 │ Ep Return:   -3.22 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  6366 │ Loss:N/A\n",
            "Ep 1216 │ Global Steps:  6369/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  6369 │ Loss:N/A\n",
            "Ep 1217 │ Global Steps:  6377/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  6377 │ Loss:N/A\n",
            "Ep 1218 │ Global Steps:  6380/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  6380 │ Loss:N/A\n",
            "Ep 1219 │ Global Steps:  6383/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  6383 │ Loss:N/A\n",
            "Ep 1220 │ Global Steps:  6391/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  6391 │ Loss:N/A\n",
            "Ep 1221 │ Global Steps:  6399/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  6399 │ Loss:N/A\n",
            "Ep 1222 │ Global Steps:  6402/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  6402 │ Loss:N/A\n",
            "Ep 1223 │ Global Steps:  6407/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  6407 │ Loss:N/A\n",
            "Ep 1224 │ Global Steps:  6412/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  6412 │ Loss:N/A\n",
            "Ep 1225 │ Global Steps:  6415/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  6415 │ Loss:N/A\n",
            "Ep 1226 │ Global Steps:  6423/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  6423 │ Loss:N/A\n",
            "Ep 1227 │ Global Steps:  6431/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.44 │ Epsilon:1.000 │ Buffer:  6431 │ Loss:N/A\n",
            "Ep 1228 │ Global Steps:  6434/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  6434 │ Loss:N/A\n",
            "Ep 1229 │ Global Steps:  6439/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  6439 │ Loss:N/A\n",
            "Ep 1230 │ Global Steps:  6442/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  6442 │ Loss:N/A\n",
            "Ep 1231 │ Global Steps:  6445/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  6445 │ Loss:N/A\n",
            "Ep 1232 │ Global Steps:  6448/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  6448 │ Loss:N/A\n",
            "Ep 1233 │ Global Steps:  6451/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  6451 │ Loss:N/A\n",
            "Ep 1234 │ Global Steps:  6454/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  6454 │ Loss:N/A\n",
            "Ep 1235 │ Global Steps:  6457/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  6457 │ Loss:N/A\n",
            "Ep 1236 │ Global Steps:  6460/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  6460 │ Loss:N/A\n",
            "Ep 1237 │ Global Steps:  6468/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  6468 │ Loss:N/A\n",
            "Ep 1238 │ Global Steps:  6471/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  6471 │ Loss:N/A\n",
            "Ep 1239 │ Global Steps:  6479/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  6479 │ Loss:N/A\n",
            "Ep 1240 │ Global Steps:  6487/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  6487 │ Loss:N/A\n",
            "Ep 1241 │ Global Steps:  6495/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  6495 │ Loss:N/A\n",
            "Ep 1242 │ Global Steps:  6500/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  6500 │ Loss:N/A\n",
            "Ep 1243 │ Global Steps:  6505/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  6505 │ Loss:N/A\n",
            "Ep 1244 │ Global Steps:  6510/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  6510 │ Loss:N/A\n",
            "Ep 1245 │ Global Steps:  6513/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  6513 │ Loss:N/A\n",
            "Ep 1246 │ Global Steps:  6521/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  6521 │ Loss:N/A\n",
            "Ep 1247 │ Global Steps:  6529/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  6529 │ Loss:N/A\n",
            "Ep 1248 │ Global Steps:  6537/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  6537 │ Loss:N/A\n",
            "Ep 1249 │ Global Steps:  6542/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  6542 │ Loss:N/A\n",
            "Ep 1250 │ Global Steps:  6545/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  6545 │ Loss:N/A\n",
            "Ep 1251 │ Global Steps:  6553/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.49 │ Epsilon:1.000 │ Buffer:  6553 │ Loss:N/A\n",
            "Ep 1252 │ Global Steps:  6558/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  6558 │ Loss:N/A\n",
            "Ep 1253 │ Global Steps:  6563/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:  6563 │ Loss:N/A\n",
            "Ep 1254 │ Global Steps:  6571/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:  6571 │ Loss:N/A\n",
            "Ep 1255 │ Global Steps:  6576/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.58 │ Epsilon:1.000 │ Buffer:  6576 │ Loss:N/A\n",
            "Ep 1256 │ Global Steps:  6584/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  6584 │ Loss:N/A\n",
            "Ep 1257 │ Global Steps:  6587/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.60 │ Epsilon:1.000 │ Buffer:  6587 │ Loss:N/A\n",
            "Ep 1258 │ Global Steps:  6592/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.57 │ Epsilon:1.000 │ Buffer:  6592 │ Loss:N/A\n",
            "Ep 1259 │ Global Steps:  6600/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.57 │ Epsilon:1.000 │ Buffer:  6600 │ Loss:N/A\n",
            "Ep 1260 │ Global Steps:  6603/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.57 │ Epsilon:1.000 │ Buffer:  6603 │ Loss:N/A\n",
            "Ep 1261 │ Global Steps:  6608/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:  6608 │ Loss:N/A\n",
            "Ep 1262 │ Global Steps:  6613/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.53 │ Epsilon:1.000 │ Buffer:  6613 │ Loss:N/A\n",
            "Ep 1263 │ Global Steps:  6621/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.57 │ Epsilon:1.000 │ Buffer:  6621 │ Loss:N/A\n",
            "Ep 1264 │ Global Steps:  6626/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.54 │ Epsilon:1.000 │ Buffer:  6626 │ Loss:N/A\n",
            "Ep 1265 │ Global Steps:  6629/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  6629 │ Loss:N/A\n",
            "Ep 1266 │ Global Steps:  6632/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  6632 │ Loss:N/A\n",
            "Ep 1267 │ Global Steps:  6635/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  6635 │ Loss:N/A\n",
            "Ep 1268 │ Global Steps:  6640/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  6640 │ Loss:N/A\n",
            "Ep 1269 │ Global Steps:  6643/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  6643 │ Loss:N/A\n",
            "Ep 1270 │ Global Steps:  6646/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  6646 │ Loss:N/A\n",
            "Ep 1271 │ Global Steps:  6649/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.44 │ Epsilon:1.000 │ Buffer:  6649 │ Loss:N/A\n",
            "Ep 1272 │ Global Steps:  6652/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  6652 │ Loss:N/A\n",
            "Ep 1273 │ Global Steps:  6657/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  6657 │ Loss:N/A\n",
            "Ep 1274 │ Global Steps:  6660/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  6660 │ Loss:N/A\n",
            "Ep 1275 │ Global Steps:  6668/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  6668 │ Loss:N/A\n",
            "Ep 1276 │ Global Steps:  6676/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  6676 │ Loss:N/A\n",
            "Ep 1277 │ Global Steps:  6684/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.44 │ Epsilon:1.000 │ Buffer:  6684 │ Loss:N/A\n",
            "Ep 1278 │ Global Steps:  6689/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  6689 │ Loss:N/A\n",
            "Ep 1279 │ Global Steps:  6697/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  6697 │ Loss:N/A\n",
            "Ep 1280 │ Global Steps:  6705/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  6705 │ Loss:N/A\n",
            "Ep 1281 │ Global Steps:  6708/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  6708 │ Loss:N/A\n",
            "Ep 1282 │ Global Steps:  6711/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  6711 │ Loss:N/A\n",
            "Ep 1283 │ Global Steps:  6719/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  6719 │ Loss:N/A\n",
            "Ep 1284 │ Global Steps:  6722/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  6722 │ Loss:N/A\n",
            "Ep 1285 │ Global Steps:  6725/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  6725 │ Loss:N/A\n",
            "Ep 1286 │ Global Steps:  6730/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  6730 │ Loss:N/A\n",
            "Ep 1287 │ Global Steps:  6738/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.44 │ Epsilon:1.000 │ Buffer:  6738 │ Loss:N/A\n",
            "Ep 1288 │ Global Steps:  6743/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  6743 │ Loss:N/A\n",
            "Ep 1289 │ Global Steps:  6746/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  6746 │ Loss:N/A\n",
            "Ep 1290 │ Global Steps:  6749/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  6749 │ Loss:N/A\n",
            "Ep 1291 │ Global Steps:  6752/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  6752 │ Loss:N/A\n",
            "Ep 1292 │ Global Steps:  6755/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  6755 │ Loss:N/A\n",
            "Ep 1293 │ Global Steps:  6763/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  6763 │ Loss:N/A\n",
            "Ep 1294 │ Global Steps:  6766/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  6766 │ Loss:N/A\n",
            "Ep 1295 │ Global Steps:  6774/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  6774 │ Loss:N/A\n",
            "Ep 1296 │ Global Steps:  6779/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  6779 │ Loss:N/A\n",
            "Ep 1297 │ Global Steps:  6784/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  6784 │ Loss:N/A\n",
            "Ep 1298 │ Global Steps:  6787/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  6787 │ Loss:N/A\n",
            "Ep 1299 │ Global Steps:  6790/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  6790 │ Loss:N/A\n",
            "Ep 1300 │ Global Steps:  6795/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  6795 │ Loss:N/A\n",
            "Ep 1301 │ Global Steps:  6800/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  6800 │ Loss:N/A\n",
            "Ep 1302 │ Global Steps:  6805/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  6805 │ Loss:N/A\n",
            "Ep 1303 │ Global Steps:  6808/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  6808 │ Loss:N/A\n",
            "Ep 1304 │ Global Steps:  6813/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  6813 │ Loss:N/A\n",
            "Ep 1305 │ Global Steps:  6816/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  6816 │ Loss:N/A\n",
            "Ep 1306 │ Global Steps:  6819/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  6819 │ Loss:N/A\n",
            "Ep 1307 │ Global Steps:  6827/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  6827 │ Loss:N/A\n",
            "Ep 1308 │ Global Steps:  6835/10000000 │ Ep Return:   -5.65 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  6835 │ Loss:N/A\n",
            "Ep 1309 │ Global Steps:  6840/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  6840 │ Loss:N/A\n",
            "Ep 1310 │ Global Steps:  6848/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  6848 │ Loss:N/A\n",
            "Ep 1311 │ Global Steps:  6853/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  6853 │ Loss:N/A\n",
            "Ep 1312 │ Global Steps:  6856/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  6856 │ Loss:N/A\n",
            "Ep 1313 │ Global Steps:  6864/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  6864 │ Loss:N/A\n",
            "Ep 1314 │ Global Steps:  6867/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  6867 │ Loss:N/A\n",
            "Ep 1315 │ Global Steps:  6872/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  6872 │ Loss:N/A\n",
            "Ep 1316 │ Global Steps:  6877/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  6877 │ Loss:N/A\n",
            "Ep 1317 │ Global Steps:  6885/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  6885 │ Loss:N/A\n",
            "Ep 1318 │ Global Steps:  6893/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  6893 │ Loss:N/A\n",
            "Ep 1319 │ Global Steps:  6896/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  6896 │ Loss:N/A\n",
            "Ep 1320 │ Global Steps:  6904/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  6904 │ Loss:N/A\n",
            "Ep 1321 │ Global Steps:  6907/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  6907 │ Loss:N/A\n",
            "Ep 1322 │ Global Steps:  6912/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  6912 │ Loss:N/A\n",
            "Ep 1323 │ Global Steps:  6915/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  6915 │ Loss:N/A\n",
            "Ep 1324 │ Global Steps:  6920/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  6920 │ Loss:N/A\n",
            "Ep 1325 │ Global Steps:  6928/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  6928 │ Loss:N/A\n",
            "Ep 1326 │ Global Steps:  6933/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  6933 │ Loss:N/A\n",
            "Ep 1327 │ Global Steps:  6936/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  6936 │ Loss:N/A\n",
            "Ep 1328 │ Global Steps:  6944/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  6944 │ Loss:N/A\n",
            "Ep 1329 │ Global Steps:  6952/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  6952 │ Loss:N/A\n",
            "Ep 1330 │ Global Steps:  6957/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  6957 │ Loss:N/A\n",
            "Ep 1331 │ Global Steps:  6962/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  6962 │ Loss:N/A\n",
            "Ep 1332 │ Global Steps:  6967/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  6967 │ Loss:N/A\n",
            "Ep 1333 │ Global Steps:  6972/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  6972 │ Loss:N/A\n",
            "Ep 1334 │ Global Steps:  6977/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  6977 │ Loss:N/A\n",
            "Ep 1335 │ Global Steps:  6982/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  6982 │ Loss:N/A\n",
            "Ep 1336 │ Global Steps:  6985/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  6985 │ Loss:N/A\n",
            "Ep 1337 │ Global Steps:  6988/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  6988 │ Loss:N/A\n",
            "Ep 1338 │ Global Steps:  6996/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  6996 │ Loss:N/A\n",
            "Ep 1339 │ Global Steps:  7004/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  7004 │ Loss:N/A\n",
            "Ep 1340 │ Global Steps:  7009/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  7009 │ Loss:N/A\n",
            "Ep 1341 │ Global Steps:  7012/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  7012 │ Loss:N/A\n",
            "Ep 1342 │ Global Steps:  7017/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  7017 │ Loss:N/A\n",
            "Ep 1343 │ Global Steps:  7020/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  7020 │ Loss:N/A\n",
            "Ep 1344 │ Global Steps:  7025/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  7025 │ Loss:N/A\n",
            "Ep 1345 │ Global Steps:  7033/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  7033 │ Loss:N/A\n",
            "Ep 1346 │ Global Steps:  7036/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  7036 │ Loss:N/A\n",
            "Ep 1347 │ Global Steps:  7041/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  7041 │ Loss:N/A\n",
            "Ep 1348 │ Global Steps:  7046/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  7046 │ Loss:N/A\n",
            "Ep 1349 │ Global Steps:  7051/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  7051 │ Loss:N/A\n",
            "Ep 1350 │ Global Steps:  7056/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  7056 │ Loss:N/A\n",
            "Ep 1351 │ Global Steps:  7061/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  7061 │ Loss:N/A\n",
            "Ep 1352 │ Global Steps:  7066/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  7066 │ Loss:N/A\n",
            "Ep 1353 │ Global Steps:  7074/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  7074 │ Loss:N/A\n",
            "Ep 1354 │ Global Steps:  7079/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  7079 │ Loss:N/A\n",
            "Ep 1355 │ Global Steps:  7082/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  7082 │ Loss:N/A\n",
            "Ep 1356 │ Global Steps:  7087/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  7087 │ Loss:N/A\n",
            "Ep 1357 │ Global Steps:  7092/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  7092 │ Loss:N/A\n",
            "Ep 1358 │ Global Steps:  7100/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  7100 │ Loss:N/A\n",
            "Ep 1359 │ Global Steps:  7103/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  7103 │ Loss:N/A\n",
            "Ep 1360 │ Global Steps:  7111/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  7111 │ Loss:N/A\n",
            "Ep 1361 │ Global Steps:  7119/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  7119 │ Loss:N/A\n",
            "Ep 1362 │ Global Steps:  7127/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  7127 │ Loss:N/A\n",
            "Ep 1363 │ Global Steps:  7132/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  7132 │ Loss:N/A\n",
            "Ep 1364 │ Global Steps:  7135/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  7135 │ Loss:N/A\n",
            "Ep 1365 │ Global Steps:  7140/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  7140 │ Loss:N/A\n",
            "Ep 1366 │ Global Steps:  7143/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  7143 │ Loss:N/A\n",
            "Ep 1367 │ Global Steps:  7146/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  7146 │ Loss:N/A\n",
            "Ep 1368 │ Global Steps:  7154/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  7154 │ Loss:N/A\n",
            "Ep 1369 │ Global Steps:  7157/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  7157 │ Loss:N/A\n",
            "Ep 1370 │ Global Steps:  7160/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  7160 │ Loss:N/A\n",
            "Ep 1371 │ Global Steps:  7165/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  7165 │ Loss:N/A\n",
            "Ep 1372 │ Global Steps:  7170/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  7170 │ Loss:N/A\n",
            "Ep 1373 │ Global Steps:  7173/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  7173 │ Loss:N/A\n",
            "Ep 1374 │ Global Steps:  7181/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  7181 │ Loss:N/A\n",
            "Ep 1375 │ Global Steps:  7189/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  7189 │ Loss:N/A\n",
            "Ep 1376 │ Global Steps:  7197/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  7197 │ Loss:N/A\n",
            "Ep 1377 │ Global Steps:  7202/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  7202 │ Loss:N/A\n",
            "Ep 1378 │ Global Steps:  7207/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  7207 │ Loss:N/A\n",
            "Ep 1379 │ Global Steps:  7215/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  7215 │ Loss:N/A\n",
            "Ep 1380 │ Global Steps:  7220/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  7220 │ Loss:N/A\n",
            "Ep 1381 │ Global Steps:  7223/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  7223 │ Loss:N/A\n",
            "Ep 1382 │ Global Steps:  7226/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  7226 │ Loss:N/A\n",
            "Ep 1383 │ Global Steps:  7234/10000000 │ Ep Return:   -5.65 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  7234 │ Loss:N/A\n",
            "Ep 1384 │ Global Steps:  7239/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  7239 │ Loss:N/A\n",
            "Ep 1385 │ Global Steps:  7244/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  7244 │ Loss:N/A\n",
            "Ep 1386 │ Global Steps:  7249/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  7249 │ Loss:N/A\n",
            "Ep 1387 │ Global Steps:  7254/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  7254 │ Loss:N/A\n",
            "Ep 1388 │ Global Steps:  7259/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  7259 │ Loss:N/A\n",
            "Ep 1389 │ Global Steps:  7262/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  7262 │ Loss:N/A\n",
            "Ep 1390 │ Global Steps:  7270/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  7270 │ Loss:N/A\n",
            "Ep 1391 │ Global Steps:  7275/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  7275 │ Loss:N/A\n",
            "Ep 1392 │ Global Steps:  7283/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  7283 │ Loss:N/A\n",
            "Ep 1393 │ Global Steps:  7291/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  7291 │ Loss:N/A\n",
            "Ep 1394 │ Global Steps:  7294/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  7294 │ Loss:N/A\n",
            "Ep 1395 │ Global Steps:  7299/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  7299 │ Loss:N/A\n",
            "Ep 1396 │ Global Steps:  7307/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  7307 │ Loss:N/A\n",
            "Ep 1397 │ Global Steps:  7312/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  7312 │ Loss:N/A\n",
            "Ep 1398 │ Global Steps:  7315/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  7315 │ Loss:N/A\n",
            "Ep 1399 │ Global Steps:  7323/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  7323 │ Loss:N/A\n",
            "Ep 1400 │ Global Steps:  7331/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:  7331 │ Loss:N/A\n",
            "Ep 1401 │ Global Steps:  7336/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:  7336 │ Loss:N/A\n",
            "Ep 1402 │ Global Steps:  7344/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.54 │ Epsilon:1.000 │ Buffer:  7344 │ Loss:N/A\n",
            "Ep 1403 │ Global Steps:  7352/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  7352 │ Loss:N/A\n",
            "Ep 1404 │ Global Steps:  7360/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.61 │ Epsilon:1.000 │ Buffer:  7360 │ Loss:N/A\n",
            "Ep 1405 │ Global Steps:  7363/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.61 │ Epsilon:1.000 │ Buffer:  7363 │ Loss:N/A\n",
            "Ep 1406 │ Global Steps:  7366/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.61 │ Epsilon:1.000 │ Buffer:  7366 │ Loss:N/A\n",
            "Ep 1407 │ Global Steps:  7369/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.57 │ Epsilon:1.000 │ Buffer:  7369 │ Loss:N/A\n",
            "Ep 1408 │ Global Steps:  7372/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.53 │ Epsilon:1.000 │ Buffer:  7372 │ Loss:N/A\n",
            "Ep 1409 │ Global Steps:  7380/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:  7380 │ Loss:N/A\n",
            "Ep 1410 │ Global Steps:  7383/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.51 │ Epsilon:1.000 │ Buffer:  7383 │ Loss:N/A\n",
            "Ep 1411 │ Global Steps:  7386/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  7386 │ Loss:N/A\n",
            "Ep 1412 │ Global Steps:  7389/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  7389 │ Loss:N/A\n",
            "Ep 1413 │ Global Steps:  7394/10000000 │ Ep Return:   -3.22 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  7394 │ Loss:N/A\n",
            "Ep 1414 │ Global Steps:  7402/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.51 │ Epsilon:1.000 │ Buffer:  7402 │ Loss:N/A\n",
            "Ep 1415 │ Global Steps:  7405/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  7405 │ Loss:N/A\n",
            "Ep 1416 │ Global Steps:  7408/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  7408 │ Loss:N/A\n",
            "Ep 1417 │ Global Steps:  7413/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  7413 │ Loss:N/A\n",
            "Ep 1418 │ Global Steps:  7418/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  7418 │ Loss:N/A\n",
            "Ep 1419 │ Global Steps:  7421/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  7421 │ Loss:N/A\n",
            "Ep 1420 │ Global Steps:  7426/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  7426 │ Loss:N/A\n",
            "Ep 1421 │ Global Steps:  7429/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  7429 │ Loss:N/A\n",
            "Ep 1422 │ Global Steps:  7437/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  7437 │ Loss:N/A\n",
            "Ep 1423 │ Global Steps:  7442/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  7442 │ Loss:N/A\n",
            "Ep 1424 │ Global Steps:  7445/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  7445 │ Loss:N/A\n",
            "Ep 1425 │ Global Steps:  7448/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  7448 │ Loss:N/A\n",
            "Ep 1426 │ Global Steps:  7456/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  7456 │ Loss:N/A\n",
            "Ep 1427 │ Global Steps:  7464/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  7464 │ Loss:N/A\n",
            "Ep 1428 │ Global Steps:  7472/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  7472 │ Loss:N/A\n",
            "Ep 1429 │ Global Steps:  7480/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  7480 │ Loss:N/A\n",
            "Ep 1430 │ Global Steps:  7483/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.44 │ Epsilon:1.000 │ Buffer:  7483 │ Loss:N/A\n",
            "Ep 1431 │ Global Steps:  7488/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.44 │ Epsilon:1.000 │ Buffer:  7488 │ Loss:N/A\n",
            "Ep 1432 │ Global Steps:  7491/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  7491 │ Loss:N/A\n",
            "Ep 1433 │ Global Steps:  7499/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  7499 │ Loss:N/A\n",
            "Ep 1434 │ Global Steps:  7504/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  7504 │ Loss:N/A\n",
            "Ep 1435 │ Global Steps:  7507/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  7507 │ Loss:N/A\n",
            "Ep 1436 │ Global Steps:  7515/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  7515 │ Loss:N/A\n",
            "Ep 1437 │ Global Steps:  7518/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  7518 │ Loss:N/A\n",
            "Ep 1438 │ Global Steps:  7526/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  7526 │ Loss:N/A\n",
            "Ep 1439 │ Global Steps:  7529/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  7529 │ Loss:N/A\n",
            "Ep 1440 │ Global Steps:  7532/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  7532 │ Loss:N/A\n",
            "Ep 1441 │ Global Steps:  7540/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  7540 │ Loss:N/A\n",
            "Ep 1442 │ Global Steps:  7545/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  7545 │ Loss:N/A\n",
            "Ep 1443 │ Global Steps:  7553/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  7553 │ Loss:N/A\n",
            "Ep 1444 │ Global Steps:  7561/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:  7561 │ Loss:N/A\n",
            "Ep 1445 │ Global Steps:  7566/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  7566 │ Loss:N/A\n",
            "Ep 1446 │ Global Steps:  7569/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  7569 │ Loss:N/A\n",
            "Ep 1447 │ Global Steps:  7574/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  7574 │ Loss:N/A\n",
            "Ep 1448 │ Global Steps:  7577/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  7577 │ Loss:N/A\n",
            "Ep 1449 │ Global Steps:  7582/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  7582 │ Loss:N/A\n",
            "Ep 1450 │ Global Steps:  7585/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  7585 │ Loss:N/A\n",
            "Ep 1451 │ Global Steps:  7593/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.49 │ Epsilon:1.000 │ Buffer:  7593 │ Loss:N/A\n",
            "Ep 1452 │ Global Steps:  7596/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  7596 │ Loss:N/A\n",
            "Ep 1453 │ Global Steps:  7599/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  7599 │ Loss:N/A\n",
            "Ep 1454 │ Global Steps:  7602/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  7602 │ Loss:N/A\n",
            "Ep 1455 │ Global Steps:  7610/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  7610 │ Loss:N/A\n",
            "Ep 1456 │ Global Steps:  7613/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.44 │ Epsilon:1.000 │ Buffer:  7613 │ Loss:N/A\n",
            "Ep 1457 │ Global Steps:  7621/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  7621 │ Loss:N/A\n",
            "Ep 1458 │ Global Steps:  7624/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  7624 │ Loss:N/A\n",
            "Ep 1459 │ Global Steps:  7629/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.44 │ Epsilon:1.000 │ Buffer:  7629 │ Loss:N/A\n",
            "Ep 1460 │ Global Steps:  7637/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.44 │ Epsilon:1.000 │ Buffer:  7637 │ Loss:N/A\n",
            "Ep 1461 │ Global Steps:  7642/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  7642 │ Loss:N/A\n",
            "Ep 1462 │ Global Steps:  7647/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  7647 │ Loss:N/A\n",
            "Ep 1463 │ Global Steps:  7655/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  7655 │ Loss:N/A\n",
            "Ep 1464 │ Global Steps:  7658/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  7658 │ Loss:N/A\n",
            "Ep 1465 │ Global Steps:  7663/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  7663 │ Loss:N/A\n",
            "Ep 1466 │ Global Steps:  7671/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  7671 │ Loss:N/A\n",
            "Ep 1467 │ Global Steps:  7674/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  7674 │ Loss:N/A\n",
            "Ep 1468 │ Global Steps:  7679/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  7679 │ Loss:N/A\n",
            "Ep 1469 │ Global Steps:  7682/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  7682 │ Loss:N/A\n",
            "Ep 1470 │ Global Steps:  7687/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  7687 │ Loss:N/A\n",
            "Ep 1471 │ Global Steps:  7692/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  7692 │ Loss:N/A\n",
            "Ep 1472 │ Global Steps:  7700/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  7700 │ Loss:N/A\n",
            "Ep 1473 │ Global Steps:  7705/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.49 │ Epsilon:1.000 │ Buffer:  7705 │ Loss:N/A\n",
            "Ep 1474 │ Global Steps:  7713/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.49 │ Epsilon:1.000 │ Buffer:  7713 │ Loss:N/A\n",
            "Ep 1475 │ Global Steps:  7716/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  7716 │ Loss:N/A\n",
            "Ep 1476 │ Global Steps:  7721/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  7721 │ Loss:N/A\n",
            "Ep 1477 │ Global Steps:  7724/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  7724 │ Loss:N/A\n",
            "Ep 1478 │ Global Steps:  7732/10000000 │ Ep Return:   -5.72 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  7732 │ Loss:N/A\n",
            "Ep 1479 │ Global Steps:  7737/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  7737 │ Loss:N/A\n",
            "Ep 1480 │ Global Steps:  7742/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  7742 │ Loss:N/A\n",
            "Ep 1481 │ Global Steps:  7750/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  7750 │ Loss:N/A\n",
            "Ep 1482 │ Global Steps:  7755/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  7755 │ Loss:N/A\n",
            "Ep 1483 │ Global Steps:  7758/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  7758 │ Loss:N/A\n",
            "Ep 1484 │ Global Steps:  7763/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  7763 │ Loss:N/A\n",
            "Ep 1485 │ Global Steps:  7768/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  7768 │ Loss:N/A\n",
            "Ep 1486 │ Global Steps:  7773/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  7773 │ Loss:N/A\n",
            "Ep 1487 │ Global Steps:  7776/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  7776 │ Loss:N/A\n",
            "Ep 1488 │ Global Steps:  7781/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  7781 │ Loss:N/A\n",
            "Ep 1489 │ Global Steps:  7784/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  7784 │ Loss:N/A\n",
            "Ep 1490 │ Global Steps:  7787/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  7787 │ Loss:N/A\n",
            "Ep 1491 │ Global Steps:  7792/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  7792 │ Loss:N/A\n",
            "Ep 1492 │ Global Steps:  7797/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  7797 │ Loss:N/A\n",
            "Ep 1493 │ Global Steps:  7805/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  7805 │ Loss:N/A\n",
            "Ep 1494 │ Global Steps:  7813/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  7813 │ Loss:N/A\n",
            "Ep 1495 │ Global Steps:  7818/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  7818 │ Loss:N/A\n",
            "Ep 1496 │ Global Steps:  7826/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  7826 │ Loss:N/A\n",
            "Ep 1497 │ Global Steps:  7831/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  7831 │ Loss:N/A\n",
            "Ep 1498 │ Global Steps:  7836/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.40 │ Epsilon:1.000 │ Buffer:  7836 │ Loss:N/A\n",
            "Ep 1499 │ Global Steps:  7841/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  7841 │ Loss:N/A\n",
            "Ep 1500 │ Global Steps:  7849/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  7849 │ Loss:N/A\n",
            "Ep 1501 │ Global Steps:  7854/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  7854 │ Loss:N/A\n",
            "Ep 1502 │ Global Steps:  7862/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  7862 │ Loss:N/A\n",
            "Ep 1503 │ Global Steps:  7870/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  7870 │ Loss:N/A\n",
            "Ep 1504 │ Global Steps:  7873/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  7873 │ Loss:N/A\n",
            "Ep 1505 │ Global Steps:  7876/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  7876 │ Loss:N/A\n",
            "Ep 1506 │ Global Steps:  7879/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  7879 │ Loss:N/A\n",
            "Ep 1507 │ Global Steps:  7887/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  7887 │ Loss:N/A\n",
            "Ep 1508 │ Global Steps:  7892/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  7892 │ Loss:N/A\n",
            "Ep 1509 │ Global Steps:  7900/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.39 │ Epsilon:1.000 │ Buffer:  7900 │ Loss:N/A\n",
            "Ep 1510 │ Global Steps:  7908/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  7908 │ Loss:N/A\n",
            "Ep 1511 │ Global Steps:  7913/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  7913 │ Loss:N/A\n",
            "Ep 1512 │ Global Steps:  7918/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  7918 │ Loss:N/A\n",
            "Ep 1513 │ Global Steps:  7921/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  7921 │ Loss:N/A\n",
            "Ep 1514 │ Global Steps:  7926/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  7926 │ Loss:N/A\n",
            "Ep 1515 │ Global Steps:  7934/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  7934 │ Loss:N/A\n",
            "Ep 1516 │ Global Steps:  7939/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  7939 │ Loss:N/A\n",
            "Ep 1517 │ Global Steps:  7942/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  7942 │ Loss:N/A\n",
            "Ep 1518 │ Global Steps:  7947/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  7947 │ Loss:N/A\n",
            "Ep 1519 │ Global Steps:  7952/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  7952 │ Loss:N/A\n",
            "Ep 1520 │ Global Steps:  7955/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  7955 │ Loss:N/A\n",
            "Ep 1521 │ Global Steps:  7958/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  7958 │ Loss:N/A\n",
            "Ep 1522 │ Global Steps:  7966/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  7966 │ Loss:N/A\n",
            "Ep 1523 │ Global Steps:  7969/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  7969 │ Loss:N/A\n",
            "Ep 1524 │ Global Steps:  7974/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  7974 │ Loss:N/A\n",
            "Ep 1525 │ Global Steps:  7982/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  7982 │ Loss:N/A\n",
            "Ep 1526 │ Global Steps:  7990/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  7990 │ Loss:N/A\n",
            "Ep 1527 │ Global Steps:  7995/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  7995 │ Loss:N/A\n",
            "Ep 1528 │ Global Steps:  8000/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  8000 │ Loss:N/A\n",
            "Ep 1529 │ Global Steps:  8005/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  8005 │ Loss:N/A\n",
            "Ep 1530 │ Global Steps:  8013/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  8013 │ Loss:N/A\n",
            "Ep 1531 │ Global Steps:  8018/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  8018 │ Loss:N/A\n",
            "Ep 1532 │ Global Steps:  8021/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  8021 │ Loss:N/A\n",
            "Ep 1533 │ Global Steps:  8029/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  8029 │ Loss:N/A\n",
            "Ep 1534 │ Global Steps:  8034/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  8034 │ Loss:N/A\n",
            "Ep 1535 │ Global Steps:  8039/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.49 │ Epsilon:1.000 │ Buffer:  8039 │ Loss:N/A\n",
            "Ep 1536 │ Global Steps:  8044/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  8044 │ Loss:N/A\n",
            "Ep 1537 │ Global Steps:  8052/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  8052 │ Loss:N/A\n",
            "Ep 1538 │ Global Steps:  8060/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  8060 │ Loss:N/A\n",
            "Ep 1539 │ Global Steps:  8068/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.54 │ Epsilon:1.000 │ Buffer:  8068 │ Loss:N/A\n",
            "Ep 1540 │ Global Steps:  8073/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:  8073 │ Loss:N/A\n",
            "Ep 1541 │ Global Steps:  8081/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:  8081 │ Loss:N/A\n",
            "Ep 1542 │ Global Steps:  8086/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:  8086 │ Loss:N/A\n",
            "Ep 1543 │ Global Steps:  8094/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:  8094 │ Loss:N/A\n",
            "Ep 1544 │ Global Steps:  8097/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:  8097 │ Loss:N/A\n",
            "Ep 1545 │ Global Steps:  8100/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  8100 │ Loss:N/A\n",
            "Ep 1546 │ Global Steps:  8105/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:  8105 │ Loss:N/A\n",
            "Ep 1547 │ Global Steps:  8108/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  8108 │ Loss:N/A\n",
            "Ep 1548 │ Global Steps:  8111/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  8111 │ Loss:N/A\n",
            "Ep 1549 │ Global Steps:  8119/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.53 │ Epsilon:1.000 │ Buffer:  8119 │ Loss:N/A\n",
            "Ep 1550 │ Global Steps:  8127/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.57 │ Epsilon:1.000 │ Buffer:  8127 │ Loss:N/A\n",
            "Ep 1551 │ Global Steps:  8130/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.53 │ Epsilon:1.000 │ Buffer:  8130 │ Loss:N/A\n",
            "Ep 1552 │ Global Steps:  8138/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.57 │ Epsilon:1.000 │ Buffer:  8138 │ Loss:N/A\n",
            "Ep 1553 │ Global Steps:  8146/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.61 │ Epsilon:1.000 │ Buffer:  8146 │ Loss:N/A\n",
            "Ep 1554 │ Global Steps:  8151/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  8151 │ Loss:N/A\n",
            "Ep 1555 │ Global Steps:  8154/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.58 │ Epsilon:1.000 │ Buffer:  8154 │ Loss:N/A\n",
            "Ep 1556 │ Global Steps:  8157/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.58 │ Epsilon:1.000 │ Buffer:  8157 │ Loss:N/A\n",
            "Ep 1557 │ Global Steps:  8162/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:  8162 │ Loss:N/A\n",
            "Ep 1558 │ Global Steps:  8170/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.60 │ Epsilon:1.000 │ Buffer:  8170 │ Loss:N/A\n",
            "Ep 1559 │ Global Steps:  8178/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  8178 │ Loss:N/A\n",
            "Ep 1560 │ Global Steps:  8186/10000000 │ Ep Return:   -5.72 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  8186 │ Loss:N/A\n",
            "Ep 1561 │ Global Steps:  8194/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.65 │ Epsilon:1.000 │ Buffer:  8194 │ Loss:N/A\n",
            "Ep 1562 │ Global Steps:  8197/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.63 │ Epsilon:1.000 │ Buffer:  8197 │ Loss:N/A\n",
            "Ep 1563 │ Global Steps:  8202/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.61 │ Epsilon:1.000 │ Buffer:  8202 │ Loss:N/A\n",
            "Ep 1564 │ Global Steps:  8205/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.61 │ Epsilon:1.000 │ Buffer:  8205 │ Loss:N/A\n",
            "Ep 1565 │ Global Steps:  8208/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  8208 │ Loss:N/A\n",
            "Ep 1566 │ Global Steps:  8216/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  8216 │ Loss:N/A\n",
            "Ep 1567 │ Global Steps:  8219/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  8219 │ Loss:N/A\n",
            "Ep 1568 │ Global Steps:  8224/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  8224 │ Loss:N/A\n",
            "Ep 1569 │ Global Steps:  8232/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.64 │ Epsilon:1.000 │ Buffer:  8232 │ Loss:N/A\n",
            "Ep 1570 │ Global Steps:  8235/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  8235 │ Loss:N/A\n",
            "Ep 1571 │ Global Steps:  8238/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.60 │ Epsilon:1.000 │ Buffer:  8238 │ Loss:N/A\n",
            "Ep 1572 │ Global Steps:  8241/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:  8241 │ Loss:N/A\n",
            "Ep 1573 │ Global Steps:  8244/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:  8244 │ Loss:N/A\n",
            "Ep 1574 │ Global Steps:  8247/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.51 │ Epsilon:1.000 │ Buffer:  8247 │ Loss:N/A\n",
            "Ep 1575 │ Global Steps:  8255/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:  8255 │ Loss:N/A\n",
            "Ep 1576 │ Global Steps:  8263/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.57 │ Epsilon:1.000 │ Buffer:  8263 │ Loss:N/A\n",
            "Ep 1577 │ Global Steps:  8268/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  8268 │ Loss:N/A\n",
            "Ep 1578 │ Global Steps:  8271/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:  8271 │ Loss:N/A\n",
            "Ep 1579 │ Global Steps:  8276/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:  8276 │ Loss:N/A\n",
            "Ep 1580 │ Global Steps:  8281/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:  8281 │ Loss:N/A\n",
            "Ep 1581 │ Global Steps:  8289/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:  8289 │ Loss:N/A\n",
            "Ep 1582 │ Global Steps:  8297/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.57 │ Epsilon:1.000 │ Buffer:  8297 │ Loss:N/A\n",
            "Ep 1583 │ Global Steps:  8302/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  8302 │ Loss:N/A\n",
            "Ep 1584 │ Global Steps:  8310/10000000 │ Ep Return:   -5.65 │ Avg Return (100 ep):   -3.61 │ Epsilon:1.000 │ Buffer:  8310 │ Loss:N/A\n",
            "Ep 1585 │ Global Steps:  8318/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.64 │ Epsilon:1.000 │ Buffer:  8318 │ Loss:N/A\n",
            "Ep 1586 │ Global Steps:  8326/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.66 │ Epsilon:1.000 │ Buffer:  8326 │ Loss:N/A\n",
            "Ep 1587 │ Global Steps:  8331/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:  8331 │ Loss:N/A\n",
            "Ep 1588 │ Global Steps:  8339/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  8339 │ Loss:N/A\n",
            "Ep 1589 │ Global Steps:  8347/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  8347 │ Loss:N/A\n",
            "Ep 1590 │ Global Steps:  8352/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  8352 │ Loss:N/A\n",
            "Ep 1591 │ Global Steps:  8360/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.78 │ Epsilon:1.000 │ Buffer:  8360 │ Loss:N/A\n",
            "Ep 1592 │ Global Steps:  8368/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.81 │ Epsilon:1.000 │ Buffer:  8368 │ Loss:N/A\n",
            "Ep 1593 │ Global Steps:  8371/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.77 │ Epsilon:1.000 │ Buffer:  8371 │ Loss:N/A\n",
            "Ep 1594 │ Global Steps:  8374/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.73 │ Epsilon:1.000 │ Buffer:  8374 │ Loss:N/A\n",
            "Ep 1595 │ Global Steps:  8377/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  8377 │ Loss:N/A\n",
            "Ep 1596 │ Global Steps:  8385/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  8385 │ Loss:N/A\n",
            "Ep 1597 │ Global Steps:  8390/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  8390 │ Loss:N/A\n",
            "Ep 1598 │ Global Steps:  8398/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  8398 │ Loss:N/A\n",
            "Ep 1599 │ Global Steps:  8406/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  8406 │ Loss:N/A\n",
            "Ep 1600 │ Global Steps:  8409/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  8409 │ Loss:N/A\n",
            "Ep 1601 │ Global Steps:  8414/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  8414 │ Loss:N/A\n",
            "Ep 1602 │ Global Steps:  8422/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  8422 │ Loss:N/A\n",
            "Ep 1603 │ Global Steps:  8430/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  8430 │ Loss:N/A\n",
            "Ep 1604 │ Global Steps:  8438/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  8438 │ Loss:N/A\n",
            "Ep 1605 │ Global Steps:  8441/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  8441 │ Loss:N/A\n",
            "Ep 1606 │ Global Steps:  8444/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  8444 │ Loss:N/A\n",
            "Ep 1607 │ Global Steps:  8447/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  8447 │ Loss:N/A\n",
            "Ep 1608 │ Global Steps:  8452/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  8452 │ Loss:N/A\n",
            "Ep 1609 │ Global Steps:  8460/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  8460 │ Loss:N/A\n",
            "Ep 1610 │ Global Steps:  8465/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.69 │ Epsilon:1.000 │ Buffer:  8465 │ Loss:N/A\n",
            "Ep 1611 │ Global Steps:  8473/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  8473 │ Loss:N/A\n",
            "Ep 1612 │ Global Steps:  8478/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  8478 │ Loss:N/A\n",
            "Ep 1613 │ Global Steps:  8483/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.73 │ Epsilon:1.000 │ Buffer:  8483 │ Loss:N/A\n",
            "Ep 1614 │ Global Steps:  8491/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.76 │ Epsilon:1.000 │ Buffer:  8491 │ Loss:N/A\n",
            "Ep 1615 │ Global Steps:  8494/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  8494 │ Loss:N/A\n",
            "Ep 1616 │ Global Steps:  8497/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  8497 │ Loss:N/A\n",
            "Ep 1617 │ Global Steps:  8502/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  8502 │ Loss:N/A\n",
            "Ep 1618 │ Global Steps:  8507/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.72 │ Epsilon:1.000 │ Buffer:  8507 │ Loss:N/A\n",
            "Ep 1619 │ Global Steps:  8515/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  8515 │ Loss:N/A\n",
            "Ep 1620 │ Global Steps:  8518/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  8518 │ Loss:N/A\n",
            "Ep 1621 │ Global Steps:  8521/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  8521 │ Loss:N/A\n",
            "Ep 1622 │ Global Steps:  8524/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  8524 │ Loss:N/A\n",
            "Ep 1623 │ Global Steps:  8532/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.74 │ Epsilon:1.000 │ Buffer:  8532 │ Loss:N/A\n",
            "Ep 1624 │ Global Steps:  8535/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.73 │ Epsilon:1.000 │ Buffer:  8535 │ Loss:N/A\n",
            "Ep 1625 │ Global Steps:  8538/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.69 │ Epsilon:1.000 │ Buffer:  8538 │ Loss:N/A\n",
            "Ep 1626 │ Global Steps:  8546/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.69 │ Epsilon:1.000 │ Buffer:  8546 │ Loss:N/A\n",
            "Ep 1627 │ Global Steps:  8554/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.71 │ Epsilon:1.000 │ Buffer:  8554 │ Loss:N/A\n",
            "Ep 1628 │ Global Steps:  8557/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  8557 │ Loss:N/A\n",
            "Ep 1629 │ Global Steps:  8562/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  8562 │ Loss:N/A\n",
            "Ep 1630 │ Global Steps:  8570/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.70 │ Epsilon:1.000 │ Buffer:  8570 │ Loss:N/A\n",
            "Ep 1631 │ Global Steps:  8573/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:  8573 │ Loss:N/A\n",
            "Ep 1632 │ Global Steps:  8576/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.68 │ Epsilon:1.000 │ Buffer:  8576 │ Loss:N/A\n",
            "Ep 1633 │ Global Steps:  8579/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.64 │ Epsilon:1.000 │ Buffer:  8579 │ Loss:N/A\n",
            "Ep 1634 │ Global Steps:  8582/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  8582 │ Loss:N/A\n",
            "Ep 1635 │ Global Steps:  8585/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.61 │ Epsilon:1.000 │ Buffer:  8585 │ Loss:N/A\n",
            "Ep 1636 │ Global Steps:  8588/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  8588 │ Loss:N/A\n",
            "Ep 1637 │ Global Steps:  8591/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:  8591 │ Loss:N/A\n",
            "Ep 1638 │ Global Steps:  8599/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:  8599 │ Loss:N/A\n",
            "Ep 1639 │ Global Steps:  8604/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.53 │ Epsilon:1.000 │ Buffer:  8604 │ Loss:N/A\n",
            "Ep 1640 │ Global Steps:  8609/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:  8609 │ Loss:N/A\n",
            "Ep 1641 │ Global Steps:  8614/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  8614 │ Loss:N/A\n",
            "Ep 1642 │ Global Steps:  8619/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  8619 │ Loss:N/A\n",
            "Ep 1643 │ Global Steps:  8622/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  8622 │ Loss:N/A\n",
            "Ep 1644 │ Global Steps:  8630/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  8630 │ Loss:N/A\n",
            "Ep 1645 │ Global Steps:  8638/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.54 │ Epsilon:1.000 │ Buffer:  8638 │ Loss:N/A\n",
            "Ep 1646 │ Global Steps:  8641/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:  8641 │ Loss:N/A\n",
            "Ep 1647 │ Global Steps:  8646/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.54 │ Epsilon:1.000 │ Buffer:  8646 │ Loss:N/A\n",
            "Ep 1648 │ Global Steps:  8654/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.58 │ Epsilon:1.000 │ Buffer:  8654 │ Loss:N/A\n",
            "Ep 1649 │ Global Steps:  8662/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.58 │ Epsilon:1.000 │ Buffer:  8662 │ Loss:N/A\n",
            "Ep 1650 │ Global Steps:  8665/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.54 │ Epsilon:1.000 │ Buffer:  8665 │ Loss:N/A\n",
            "Ep 1651 │ Global Steps:  8668/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.54 │ Epsilon:1.000 │ Buffer:  8668 │ Loss:N/A\n",
            "Ep 1652 │ Global Steps:  8676/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.54 │ Epsilon:1.000 │ Buffer:  8676 │ Loss:N/A\n",
            "Ep 1653 │ Global Steps:  8679/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  8679 │ Loss:N/A\n",
            "Ep 1654 │ Global Steps:  8687/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:  8687 │ Loss:N/A\n",
            "Ep 1655 │ Global Steps:  8692/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.54 │ Epsilon:1.000 │ Buffer:  8692 │ Loss:N/A\n",
            "Ep 1656 │ Global Steps:  8700/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.58 │ Epsilon:1.000 │ Buffer:  8700 │ Loss:N/A\n",
            "Ep 1657 │ Global Steps:  8705/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.58 │ Epsilon:1.000 │ Buffer:  8705 │ Loss:N/A\n",
            "Ep 1658 │ Global Steps:  8708/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.54 │ Epsilon:1.000 │ Buffer:  8708 │ Loss:N/A\n",
            "Ep 1659 │ Global Steps:  8713/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.51 │ Epsilon:1.000 │ Buffer:  8713 │ Loss:N/A\n",
            "Ep 1660 │ Global Steps:  8716/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  8716 │ Loss:N/A\n",
            "Ep 1661 │ Global Steps:  8721/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  8721 │ Loss:N/A\n",
            "Ep 1662 │ Global Steps:  8726/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  8726 │ Loss:N/A\n",
            "Ep 1663 │ Global Steps:  8731/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  8731 │ Loss:N/A\n",
            "Ep 1664 │ Global Steps:  8736/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  8736 │ Loss:N/A\n",
            "Ep 1665 │ Global Steps:  8744/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:  8744 │ Loss:N/A\n",
            "Ep 1666 │ Global Steps:  8747/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.48 │ Epsilon:1.000 │ Buffer:  8747 │ Loss:N/A\n",
            "Ep 1667 │ Global Steps:  8755/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:  8755 │ Loss:N/A\n",
            "Ep 1668 │ Global Steps:  8758/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  8758 │ Loss:N/A\n",
            "Ep 1669 │ Global Steps:  8766/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  8766 │ Loss:N/A\n",
            "Ep 1670 │ Global Steps:  8769/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  8769 │ Loss:N/A\n",
            "Ep 1671 │ Global Steps:  8772/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  8772 │ Loss:N/A\n",
            "Ep 1672 │ Global Steps:  8775/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  8775 │ Loss:N/A\n",
            "Ep 1673 │ Global Steps:  8778/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  8778 │ Loss:N/A\n",
            "Ep 1674 │ Global Steps:  8781/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  8781 │ Loss:N/A\n",
            "Ep 1675 │ Global Steps:  8784/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.46 │ Epsilon:1.000 │ Buffer:  8784 │ Loss:N/A\n",
            "Ep 1676 │ Global Steps:  8787/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  8787 │ Loss:N/A\n",
            "Ep 1677 │ Global Steps:  8792/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  8792 │ Loss:N/A\n",
            "Ep 1678 │ Global Steps:  8800/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  8800 │ Loss:N/A\n",
            "Ep 1679 │ Global Steps:  8808/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.49 │ Epsilon:1.000 │ Buffer:  8808 │ Loss:N/A\n",
            "Ep 1680 │ Global Steps:  8811/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  8811 │ Loss:N/A\n",
            "Ep 1681 │ Global Steps:  8819/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.47 │ Epsilon:1.000 │ Buffer:  8819 │ Loss:N/A\n",
            "Ep 1682 │ Global Steps:  8824/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  8824 │ Loss:N/A\n",
            "Ep 1683 │ Global Steps:  8829/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  8829 │ Loss:N/A\n",
            "Ep 1684 │ Global Steps:  8832/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.41 │ Epsilon:1.000 │ Buffer:  8832 │ Loss:N/A\n",
            "Ep 1685 │ Global Steps:  8835/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  8835 │ Loss:N/A\n",
            "Ep 1686 │ Global Steps:  8840/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  8840 │ Loss:N/A\n",
            "Ep 1687 │ Global Steps:  8848/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.37 │ Epsilon:1.000 │ Buffer:  8848 │ Loss:N/A\n",
            "Ep 1688 │ Global Steps:  8853/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  8853 │ Loss:N/A\n",
            "Ep 1689 │ Global Steps:  8858/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  8858 │ Loss:N/A\n",
            "Ep 1690 │ Global Steps:  8866/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  8866 │ Loss:N/A\n",
            "Ep 1691 │ Global Steps:  8869/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.30 │ Epsilon:1.000 │ Buffer:  8869 │ Loss:N/A\n",
            "Ep 1692 │ Global Steps:  8874/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  8874 │ Loss:N/A\n",
            "Ep 1693 │ Global Steps:  8879/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  8879 │ Loss:N/A\n",
            "Ep 1694 │ Global Steps:  8884/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  8884 │ Loss:N/A\n",
            "Ep 1695 │ Global Steps:  8889/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  8889 │ Loss:N/A\n",
            "Ep 1696 │ Global Steps:  8897/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  8897 │ Loss:N/A\n",
            "Ep 1697 │ Global Steps:  8900/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  8900 │ Loss:N/A\n",
            "Ep 1698 │ Global Steps:  8903/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  8903 │ Loss:N/A\n",
            "Ep 1699 │ Global Steps:  8908/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  8908 │ Loss:N/A\n",
            "Ep 1700 │ Global Steps:  8911/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  8911 │ Loss:N/A\n",
            "Ep 1701 │ Global Steps:  8914/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  8914 │ Loss:N/A\n",
            "Ep 1702 │ Global Steps:  8922/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  8922 │ Loss:N/A\n",
            "Ep 1703 │ Global Steps:  8927/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  8927 │ Loss:N/A\n",
            "Ep 1704 │ Global Steps:  8935/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  8935 │ Loss:N/A\n",
            "Ep 1705 │ Global Steps:  8938/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  8938 │ Loss:N/A\n",
            "Ep 1706 │ Global Steps:  8941/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  8941 │ Loss:N/A\n",
            "Ep 1707 │ Global Steps:  8946/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  8946 │ Loss:N/A\n",
            "Ep 1708 │ Global Steps:  8951/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  8951 │ Loss:N/A\n",
            "Ep 1709 │ Global Steps:  8959/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  8959 │ Loss:N/A\n",
            "Ep 1710 │ Global Steps:  8962/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  8962 │ Loss:N/A\n",
            "Ep 1711 │ Global Steps:  8970/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  8970 │ Loss:N/A\n",
            "Ep 1712 │ Global Steps:  8973/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.19 │ Epsilon:1.000 │ Buffer:  8973 │ Loss:N/A\n",
            "Ep 1713 │ Global Steps:  8981/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.22 │ Epsilon:1.000 │ Buffer:  8981 │ Loss:N/A\n",
            "Ep 1714 │ Global Steps:  8986/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.19 │ Epsilon:1.000 │ Buffer:  8986 │ Loss:N/A\n",
            "Ep 1715 │ Global Steps:  8989/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.19 │ Epsilon:1.000 │ Buffer:  8989 │ Loss:N/A\n",
            "Ep 1716 │ Global Steps:  8997/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  8997 │ Loss:N/A\n",
            "Ep 1717 │ Global Steps:  9002/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  9002 │ Loss:N/A\n",
            "Ep 1718 │ Global Steps:  9007/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  9007 │ Loss:N/A\n",
            "Ep 1719 │ Global Steps:  9015/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  9015 │ Loss:N/A\n",
            "Ep 1720 │ Global Steps:  9018/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.23 │ Epsilon:1.000 │ Buffer:  9018 │ Loss:N/A\n",
            "Ep 1721 │ Global Steps:  9023/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  9023 │ Loss:N/A\n",
            "Ep 1722 │ Global Steps:  9031/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  9031 │ Loss:N/A\n",
            "Ep 1723 │ Global Steps:  9034/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  9034 │ Loss:N/A\n",
            "Ep 1724 │ Global Steps:  9042/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  9042 │ Loss:N/A\n",
            "Ep 1725 │ Global Steps:  9050/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  9050 │ Loss:N/A\n",
            "Ep 1726 │ Global Steps:  9053/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  9053 │ Loss:N/A\n",
            "Ep 1727 │ Global Steps:  9056/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  9056 │ Loss:N/A\n",
            "Ep 1728 │ Global Steps:  9061/10000000 │ Ep Return:   -3.17 │ Avg Return (100 ep):   -3.26 │ Epsilon:1.000 │ Buffer:  9061 │ Loss:N/A\n",
            "Ep 1729 │ Global Steps:  9064/10000000 │ Ep Return:   -1.64 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  9064 │ Loss:N/A\n",
            "Ep 1730 │ Global Steps:  9067/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.21 │ Epsilon:1.000 │ Buffer:  9067 │ Loss:N/A\n",
            "Ep 1731 │ Global Steps:  9075/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.25 │ Epsilon:1.000 │ Buffer:  9075 │ Loss:N/A\n",
            "Ep 1732 │ Global Steps:  9083/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  9083 │ Loss:N/A\n",
            "Ep 1733 │ Global Steps:  9086/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  9086 │ Loss:N/A\n",
            "Ep 1734 │ Global Steps:  9089/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  9089 │ Loss:N/A\n",
            "Ep 1735 │ Global Steps:  9092/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  9092 │ Loss:N/A\n",
            "Ep 1736 │ Global Steps:  9095/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  9095 │ Loss:N/A\n",
            "Ep 1737 │ Global Steps:  9103/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  9103 │ Loss:N/A\n",
            "Ep 1738 │ Global Steps:  9106/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  9106 │ Loss:N/A\n",
            "Ep 1739 │ Global Steps:  9109/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  9109 │ Loss:N/A\n",
            "Ep 1740 │ Global Steps:  9114/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  9114 │ Loss:N/A\n",
            "Ep 1741 │ Global Steps:  9119/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  9119 │ Loss:N/A\n",
            "Ep 1742 │ Global Steps:  9124/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.27 │ Epsilon:1.000 │ Buffer:  9124 │ Loss:N/A\n",
            "Ep 1743 │ Global Steps:  9132/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  9132 │ Loss:N/A\n",
            "Ep 1744 │ Global Steps:  9137/10000000 │ Ep Return:   -3.24 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  9137 │ Loss:N/A\n",
            "Ep 1745 │ Global Steps:  9145/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  9145 │ Loss:N/A\n",
            "Ep 1746 │ Global Steps:  9153/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.33 │ Epsilon:1.000 │ Buffer:  9153 │ Loss:N/A\n",
            "Ep 1747 │ Global Steps:  9161/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.35 │ Epsilon:1.000 │ Buffer:  9161 │ Loss:N/A\n",
            "Ep 1748 │ Global Steps:  9164/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  9164 │ Loss:N/A\n",
            "Ep 1749 │ Global Steps:  9169/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.29 │ Epsilon:1.000 │ Buffer:  9169 │ Loss:N/A\n",
            "Ep 1750 │ Global Steps:  9174/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  9174 │ Loss:N/A\n",
            "Ep 1751 │ Global Steps:  9177/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  9177 │ Loss:N/A\n",
            "Ep 1752 │ Global Steps:  9185/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.31 │ Epsilon:1.000 │ Buffer:  9185 │ Loss:N/A\n",
            "Ep 1753 │ Global Steps:  9190/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  9190 │ Loss:N/A\n",
            "Ep 1754 │ Global Steps:  9198/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  9198 │ Loss:N/A\n",
            "Ep 1755 │ Global Steps:  9203/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  9203 │ Loss:N/A\n",
            "Ep 1756 │ Global Steps:  9206/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  9206 │ Loss:N/A\n",
            "Ep 1757 │ Global Steps:  9211/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.28 │ Epsilon:1.000 │ Buffer:  9211 │ Loss:N/A\n",
            "Ep 1758 │ Global Steps:  9219/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  9219 │ Loss:N/A\n",
            "Ep 1759 │ Global Steps:  9224/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.32 │ Epsilon:1.000 │ Buffer:  9224 │ Loss:N/A\n",
            "Ep 1760 │ Global Steps:  9229/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  9229 │ Loss:N/A\n",
            "Ep 1761 │ Global Steps:  9237/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  9237 │ Loss:N/A\n",
            "Ep 1762 │ Global Steps:  9242/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  9242 │ Loss:N/A\n",
            "Ep 1763 │ Global Steps:  9247/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  9247 │ Loss:N/A\n",
            "Ep 1764 │ Global Steps:  9252/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.36 │ Epsilon:1.000 │ Buffer:  9252 │ Loss:N/A\n",
            "Ep 1765 │ Global Steps:  9257/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.34 │ Epsilon:1.000 │ Buffer:  9257 │ Loss:N/A\n",
            "Ep 1766 │ Global Steps:  9265/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  9265 │ Loss:N/A\n",
            "Ep 1767 │ Global Steps:  9273/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  9273 │ Loss:N/A\n",
            "Ep 1768 │ Global Steps:  9281/10000000 │ Ep Return:   -5.65 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  9281 │ Loss:N/A\n",
            "Ep 1769 │ Global Steps:  9284/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  9284 │ Loss:N/A\n",
            "Ep 1770 │ Global Steps:  9287/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.38 │ Epsilon:1.000 │ Buffer:  9287 │ Loss:N/A\n",
            "Ep 1771 │ Global Steps:  9295/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.42 │ Epsilon:1.000 │ Buffer:  9295 │ Loss:N/A\n",
            "Ep 1772 │ Global Steps:  9300/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.43 │ Epsilon:1.000 │ Buffer:  9300 │ Loss:N/A\n",
            "Ep 1773 │ Global Steps:  9305/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.45 │ Epsilon:1.000 │ Buffer:  9305 │ Loss:N/A\n",
            "Ep 1774 │ Global Steps:  9313/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.49 │ Epsilon:1.000 │ Buffer:  9313 │ Loss:N/A\n",
            "Ep 1775 │ Global Steps:  9318/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.51 │ Epsilon:1.000 │ Buffer:  9318 │ Loss:N/A\n",
            "Ep 1776 │ Global Steps:  9323/10000000 │ Ep Return:   -3.20 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:  9323 │ Loss:N/A\n",
            "Ep 1777 │ Global Steps:  9331/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:  9331 │ Loss:N/A\n",
            "Ep 1778 │ Global Steps:  9339/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:  9339 │ Loss:N/A\n",
            "Ep 1779 │ Global Steps:  9342/10000000 │ Ep Return:   -1.60 │ Avg Return (100 ep):   -3.50 │ Epsilon:1.000 │ Buffer:  9342 │ Loss:N/A\n",
            "Ep 1780 │ Global Steps:  9347/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:  9347 │ Loss:N/A\n",
            "Ep 1781 │ Global Steps:  9355/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.52 │ Epsilon:1.000 │ Buffer:  9355 │ Loss:N/A\n",
            "Ep 1782 │ Global Steps:  9363/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.54 │ Epsilon:1.000 │ Buffer:  9363 │ Loss:N/A\n",
            "Ep 1783 │ Global Steps:  9368/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.54 │ Epsilon:1.000 │ Buffer:  9368 │ Loss:N/A\n",
            "Ep 1784 │ Global Steps:  9373/10000000 │ Ep Return:   -3.23 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:  9373 │ Loss:N/A\n",
            "Ep 1785 │ Global Steps:  9376/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.56 │ Epsilon:1.000 │ Buffer:  9376 │ Loss:N/A\n",
            "Ep 1786 │ Global Steps:  9384/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  9384 │ Loss:N/A\n",
            "Ep 1787 │ Global Steps:  9387/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:  9387 │ Loss:N/A\n",
            "Ep 1788 │ Global Steps:  9392/10000000 │ Ep Return:   -3.18 │ Avg Return (100 ep):   -3.54 │ Epsilon:1.000 │ Buffer:  9392 │ Loss:N/A\n",
            "Ep 1789 │ Global Steps:  9395/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.53 │ Epsilon:1.000 │ Buffer:  9395 │ Loss:N/A\n",
            "Ep 1790 │ Global Steps:  9403/10000000 │ Ep Return:   -5.71 │ Avg Return (100 ep):   -3.53 │ Epsilon:1.000 │ Buffer:  9403 │ Loss:N/A\n",
            "Ep 1791 │ Global Steps:  9411/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.57 │ Epsilon:1.000 │ Buffer:  9411 │ Loss:N/A\n",
            "Ep 1792 │ Global Steps:  9416/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.57 │ Epsilon:1.000 │ Buffer:  9416 │ Loss:N/A\n",
            "Ep 1793 │ Global Steps:  9424/10000000 │ Ep Return:   -5.67 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  9424 │ Loss:N/A\n",
            "Ep 1794 │ Global Steps:  9427/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.58 │ Epsilon:1.000 │ Buffer:  9427 │ Loss:N/A\n",
            "Ep 1795 │ Global Steps:  9432/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.58 │ Epsilon:1.000 │ Buffer:  9432 │ Loss:N/A\n",
            "Ep 1796 │ Global Steps:  9437/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:  9437 │ Loss:N/A\n",
            "Ep 1797 │ Global Steps:  9440/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:  9440 │ Loss:N/A\n",
            "Ep 1798 │ Global Steps:  9443/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.55 │ Epsilon:1.000 │ Buffer:  9443 │ Loss:N/A\n",
            "Ep 1799 │ Global Steps:  9451/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.58 │ Epsilon:1.000 │ Buffer:  9451 │ Loss:N/A\n",
            "Ep 1800 │ Global Steps:  9456/10000000 │ Ep Return:   -3.19 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  9456 │ Loss:N/A\n",
            "Ep 1801 │ Global Steps:  9464/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.63 │ Epsilon:1.000 │ Buffer:  9464 │ Loss:N/A\n",
            "Ep 1802 │ Global Steps:  9472/10000000 │ Ep Return:   -5.68 │ Avg Return (100 ep):   -3.63 │ Epsilon:1.000 │ Buffer:  9472 │ Loss:N/A\n",
            "Ep 1803 │ Global Steps:  9477/10000000 │ Ep Return:   -3.21 │ Avg Return (100 ep):   -3.63 │ Epsilon:1.000 │ Buffer:  9477 │ Loss:N/A\n",
            "Ep 1804 │ Global Steps:  9480/10000000 │ Ep Return:   -1.61 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  9480 │ Loss:N/A\n",
            "Ep 1805 │ Global Steps:  9483/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.59 │ Epsilon:1.000 │ Buffer:  9483 │ Loss:N/A\n",
            "Ep 1806 │ Global Steps:  9491/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.63 │ Epsilon:1.000 │ Buffer:  9491 │ Loss:N/A\n",
            "Ep 1807 │ Global Steps:  9494/10000000 │ Ep Return:   -1.63 │ Avg Return (100 ep):   -3.62 │ Epsilon:1.000 │ Buffer:  9494 │ Loss:N/A\n",
            "Ep 1808 │ Global Steps:  9502/10000000 │ Ep Return:   -5.66 │ Avg Return (100 ep):   -3.64 │ Epsilon:1.000 │ Buffer:  9502 │ Loss:N/A\n",
            "Ep 1809 │ Global Steps:  9510/10000000 │ Ep Return:   -5.69 │ Avg Return (100 ep):   -3.64 │ Epsilon:1.000 │ Buffer:  9510 │ Loss:N/A\n"
          ]
        }
      ],
      "source": [
        "################################################################################\n",
        "\"\"\"Initialization\"\"\"\n",
        "################################################################################\n",
        "# IDM Initialization\n",
        "control_parameters = [0.1, 5, 4, 4]  # a, b, δ, T\n",
        "desired_parameters = [20, 10.0]       # s0, v0\n",
        "\n",
        "# Set initial Input variables using the gap, current velocity, and relative lead velocity\n",
        "input_variables = [lead_position_idm, ego_velocity_idm, lead_velocity_idm]\n",
        "\n",
        "# Setup the IDM Model for Longitudinal control\n",
        "ego_vehicle_idm = IDM(desired_parameters, control_parameters)\n",
        "\n",
        "# Agent Initialization\n",
        "total_steps_taken = 0\n",
        "lr = 0.01\n",
        "gamma = 0.9\n",
        "\n",
        "buffer_size = int(1e+5)\n",
        "num_episodes = int(2e+5)\n",
        "batch_size = int(1e+4)\n",
        "timesteps = int(1e+7)\n",
        "\n",
        "update_target_frequency = int(1e+4)\n",
        "threshold = 30\n",
        "plot_frequency = 100\n",
        "\n",
        "# Epsilon-Greedy Parameters\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.05\n",
        "epsilon_decay_steps = int(1e+5)\n",
        "epsilon = epsilon_start\n",
        "\n",
        "# Tracking for performance\n",
        "episode_returns = []\n",
        "global_step = 0\n",
        "\n",
        "target_id = env.unwrapped.vehicle.lane_index[-1]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Environment Information\n",
        "L = env.unwrapped.vehicle.LENGTH\n",
        "agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "state_dim = len(agent_state)\n",
        "\n",
        "# Initialize Buffer\n",
        "buffer = Experience_Buffer(buffer_size)\n",
        "\n",
        "# Setup Lateral Control Agent\n",
        "target_network = Agent(state_dim).to(device)\n",
        "policy_network = Agent(state_dim).to(device)\n",
        "\n",
        "# Initialize target network\n",
        "target_network.load_state_dict(policy_network.state_dict())\n",
        "\n",
        "# Loss and Criterion Initialization\n",
        "policy_optimizer = torch.optim.Adam(policy_network.parameters(), lr=lr)\n",
        "policy_loss_fn = nn.MSELoss()\n",
        "\n",
        "# Tracking loss values and timestep rewards over training steps\n",
        "loss_history = []\n",
        "timestep_rewards = []\n",
        "\n",
        "# Flag to indicate when training has started\n",
        "training_started = False\n",
        "\n",
        "# Lists for plotting data that will only be populated after training starts\n",
        "plot_episode_actions = []\n",
        "plot_timestep_rewards = []\n",
        "\n",
        "################################################################################\n",
        "\"\"\"Training & Testing Loop\"\"\"\n",
        "################################################################################\n",
        "ep = 0\n",
        "while global_step < timesteps:\n",
        "    # Reset environment and state manager\n",
        "    obs, _ = env.reset()\n",
        "    state_manager = ENV(obs, 0.0)\n",
        "\n",
        "    # Initial state information\n",
        "    ego_state_idm = state_manager.ego_state_idm()\n",
        "    lead_state = state_manager.longitudinal_lead_state()\n",
        "    agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "\n",
        "    # Ensure on_road_check is initialized correctly\n",
        "    on_road_check = env.unwrapped.vehicle.on_road\n",
        "\n",
        "    on_road_check = True\n",
        "    collision_check = False\n",
        "\n",
        "    # Agent activation flag and target lane storage\n",
        "    activated_target_lane = None\n",
        "\n",
        "    # Update IDM inputs based on initial state\n",
        "    gap = lead_state['x']\n",
        "    delta_velocity = lead_state['vx']\n",
        "    input_variables = [gap, ego_state_idm['vx'], delta_velocity]\n",
        "\n",
        "    steps = 0\n",
        "    terminal = 0\n",
        "    ep_ret = 0.0\n",
        "\n",
        "    # For stepwise plotting, record agent actions per timestep\n",
        "    episode_actions = []\n",
        "\n",
        "    # Main loop for each episode\n",
        "    while terminal == 0 and on_road_check:\n",
        "        steps += 1\n",
        "        global_step += 1  # This increments the global step counter for each environment step\n",
        "\n",
        "        # Recompute gap\n",
        "        gap = lead_state['x']\n",
        "\n",
        "        ########################################################################\n",
        "        \"\"\"Action Preparation\"\"\"\n",
        "        ########################################################################\n",
        "        gap_control = Gap_Controller(obs, threshold)\n",
        "        activated_target_lane = gap_control.lane_checker()\n",
        "\n",
        "        # Determine target lane\n",
        "        target_id = activated_target_lane\n",
        "        agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state_vector = torch.tensor(\n",
        "                list(agent_state.values()),\n",
        "                dtype=torch.float32,\n",
        "                device=device\n",
        "            ).unsqueeze(0)\n",
        "            agent_action = policy_network.action(state_vector).item()\n",
        "\n",
        "        # Store prior state for buffer\n",
        "        old_state = agent_state\n",
        "        obs_old = obs\n",
        "\n",
        "        # IDM Longitudinal Control\n",
        "        idm_acceleration = ego_vehicle_idm.longitudinal_controller(input_variables)\n",
        "        longitudinal_control = idm_acceleration\n",
        "\n",
        "        # Record the action for later stepwise plotting if training has started\n",
        "        if training_started:\n",
        "            episode_actions.append(agent_action)\n",
        "\n",
        "        # Transform agent action to steering angle\n",
        "        lateral_control = state_manager.steering_angle(agent_action, L)\n",
        "\n",
        "        # Combine longitudinal and lateral actions\n",
        "        action = [longitudinal_control, lateral_control]\n",
        "\n",
        "        ########################################################################\n",
        "        \"\"\"Data Collection\"\"\"\n",
        "        ########################################################################\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "        # Update state manager with new observation and applied longitudinal control\n",
        "        state_manager = ENV(obs, longitudinal_control)\n",
        "\n",
        "        # Compute reward based on the new state\n",
        "        reward_per_episode = state_manager.reward_function(obs_old, obs, target_id)\n",
        "        reward = reward_per_episode[0]\n",
        "        ep_ret += reward\n",
        "\n",
        "        # Append the reward from the current timestep only if training has started\n",
        "        if training_started:\n",
        "            plot_timestep_rewards.append(reward_per_episode)\n",
        "\n",
        "        # For tracking overall performance, regardless of training\n",
        "        timestep_rewards.append(reward_per_episode)\n",
        "\n",
        "        # Terminal Condition Check\n",
        "        if steps == timesteps:\n",
        "            terminal = 1\n",
        "\n",
        "        # Check if vehicle is off road\n",
        "        on_road_check = env.unwrapped.vehicle.on_road\n",
        "\n",
        "        ########################################################################\n",
        "        \"\"\"Update IDM Controller Inputs for next step\"\"\"\n",
        "        ########################################################################\n",
        "        ego_state_idm = state_manager.ego_state_idm()\n",
        "        lead_state = state_manager.longitudinal_lead_state()\n",
        "        agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "\n",
        "        gap = lead_state['x']\n",
        "        delta_velocity = lead_state['vx']\n",
        "        input_variables = [gap, ego_state_idm['vx'], delta_velocity]\n",
        "\n",
        "        # Update experience buffer\n",
        "        buffer.add(old_state, agent_action, reward, agent_state, terminal)\n",
        "\n",
        "        ########################################################################\n",
        "        \"\"\"Q-Learning Update\"\"\"\n",
        "        ########################################################################\n",
        "        if buffer.size() >= batch_size:\n",
        "            # Set training flag to true if it hasn't been set yet\n",
        "            if not training_started:\n",
        "                training_started = True\n",
        "                # Reset plot data collection lists when training starts\n",
        "                plot_episode_actions = []\n",
        "                plot_timestep_rewards = []\n",
        "                print(\"Training started - beginning to collect data for plotting\")\n",
        "\n",
        "            rand_experience = buffer.sample_experience(batch_size=batch_size)\n",
        "            transitions = torch.cat(\n",
        "                [t for t in rand_experience if isinstance(t, torch.Tensor)],\n",
        "                dim=0\n",
        "            ).to(device)\n",
        "\n",
        "            states, actions_t, rewards_t, next_states, terminals_t = torch.split(\n",
        "                transitions, [state_dim, 1, 1, state_dim, 1], dim=1\n",
        "            )\n",
        "            current_q_values = policy_network(states, actions_t, terminals_t)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                target_network.eval()\n",
        "                next_state = torch.cat((next_states, terminals_t), dim=1)\n",
        "                max_next_q_values = target_network.networkC(next_state)\n",
        "\n",
        "            target_q_values = rewards_t + gamma * max_next_q_values * (1.0 - terminals_t)\n",
        "            loss = policy_loss_fn(current_q_values, target_q_values)\n",
        "            loss_history.append(loss.item())\n",
        "\n",
        "            policy_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            policy_optimizer.step()\n",
        "\n",
        "            total_steps_taken += 1\n",
        "\n",
        "        # Update epsilon linearly over decay steps - only once training has started\n",
        "        if buffer.size() >= batch_size:\n",
        "            epsilon = max(\n",
        "                epsilon_end,\n",
        "                epsilon_start - (total_steps_taken * (epsilon_start - epsilon_end) / epsilon_decay_steps)\n",
        "            )\n",
        "\n",
        "    ############################################################################\n",
        "    \"\"\"Update Target Policy Network\"\"\"\n",
        "    ############################################################################\n",
        "    if ep % update_target_frequency == 0:\n",
        "        target_network.load_state_dict(policy_network.state_dict())\n",
        "        print(f\"**** Target network updated at episode {ep} ****\")\n",
        "\n",
        "    # Print training progress\n",
        "    episode_returns.append(ep_ret)\n",
        "    avg_return = np.mean(episode_returns[-100:])\n",
        "    last_step_loss = loss_history[-1] if loss_history else None\n",
        "    loss_str_repr = f\"{last_step_loss:.4f}\" if last_step_loss is not None else \"N/A\"\n",
        "    print(f\"Ep {ep:4} │ Global Steps:{global_step:6}/{timesteps} │ Ep Return:{ep_ret:8.2f} │ Avg Return (100 ep):{avg_return:8.2f} │ Epsilon:{epsilon:.3f} │ Buffer:{buffer.size():6} │ Loss:{loss_str_repr}\")\n",
        "\n",
        "    # Increment episode counter\n",
        "    ep += 1\n",
        "\n",
        "    ############################################################################\n",
        "    \"\"\"Periodic Plotting of Reward and Loss after DQN Update (every x timesteps)\"\"\"\n",
        "    ############################################################################\n",
        "    if global_step % plot_frequency == 0 and global_step > 0 and training_started:\n",
        "        if len(plot_episode_actions) > 0:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(range(len(plot_episode_actions)), plot_episode_actions, label=\"Steering Action (a_yaw)\")\n",
        "            plt.xlabel(\"Steps Since Training Started\")\n",
        "            plt.ylabel(f\"Agent Action per Step (Timestep {global_step})\")\n",
        "            plt.legend()\n",
        "            plt.title(\"Agent Action per Step\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        if len(plot_timestep_rewards) > 0:\n",
        "            plt.figure(figsize=(12, 5))\n",
        "            plt.plot(range(len(plot_timestep_rewards)), plot_timestep_rewards, label=['R_Total','R_acce','R_rate','R_time'])\n",
        "            plt.xlabel(\"Steps Since Training Started\")\n",
        "            plt.ylabel(\"Reward\")\n",
        "            plt.title(\"Reward per Timestep\")\n",
        "            plt.legend()\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        if len(loss_history) > 0:\n",
        "            plt.figure()\n",
        "            plt.plot(range(len(loss_history)), loss_history)\n",
        "            plt.xlabel(\"Q-Learning Update\")\n",
        "            plt.ylabel(\"Loss\")\n",
        "            plt.title(\"Loss over Training Updates\")\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inactive Agent Until Lane (Option 2)"
      ],
      "metadata": {
        "id": "bpkFPC5h_hff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "\"\"\"Initialization\"\"\"\n",
        "################################################################################\n",
        "# IDM Initialization\n",
        "control_parameters = [0.1, 5, 4, 4]  # a, b, δ, T\n",
        "desired_parameters = [40, 10.0]       # s0, v0\n",
        "\n",
        "# Set initial Input variables using the gap, current velocity, and relative lead velocity\n",
        "input_variables = [lead_position_idm, ego_velocity_idm, lead_velocity_idm]\n",
        "\n",
        "# Setup the IDM Model for Longitudinal control\n",
        "ego_vehicle_idm = IDM(desired_parameters, control_parameters)\n",
        "\n",
        "# Agent Initialization\n",
        "total_steps_taken = 0\n",
        "lr = 0.01\n",
        "gamma = 0.9\n",
        "buffer_size = 1000\n",
        "num_episodes = 10000\n",
        "batch_size = 500\n",
        "timesteps = 40000\n",
        "update_target_frequency = 10\n",
        "# epsilon_decay_rate = 10000\n",
        "threshold = 30\n",
        "plot_frequency = 10\n",
        "threshold = 30\n",
        "thetha_threshold = 0.1\n",
        "\n",
        "target_id = env.unwrapped.vehicle.lane_index[-1]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Environment Information\n",
        "L = env.unwrapped.vehicle.LENGTH\n",
        "agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "state_dim = len(agent_state)\n",
        "\n",
        "# Initialize Buffer\n",
        "buffer = Experience_Buffer(buffer_size)\n",
        "\n",
        "# Setup Lateral Control Agent\n",
        "target_network = Agent(state_dim).to(device)\n",
        "policy_network = Agent(state_dim).to(device)\n",
        "\n",
        "# Initialize target network\n",
        "target_network.load_state_dict(policy_network.state_dict())\n",
        "\n",
        "# Loss and Criterion Initialization\n",
        "policy_optimizer = torch.optim.Adam(policy_network.parameters(), lr=lr)\n",
        "policy_loss_fn = nn.MSELoss()\n",
        "\n",
        "# Tracking loss values and storing per-timestep rewards for plotting.\n",
        "loss_history = []\n",
        "# We remove the per-episode accumulation.\n",
        "# episode_rewards = []\n",
        "timestep_rewards = []  # New list: store each timestep's reward directly.\n",
        "\n",
        "################################################################################\n",
        "\"\"\"Training & Testing Loop\"\"\"\n",
        "################################################################################\n",
        "for episodes in range(num_episodes):\n",
        "    # Reset environment and state manager\n",
        "    obs, _ = env.reset()\n",
        "    state_manager = ENV(obs, 0.0)\n",
        "\n",
        "    # Initial state information\n",
        "    ego_state_idm = state_manager.ego_state_idm()\n",
        "    lead_state = state_manager.longitudinal_lead_state()\n",
        "    agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "\n",
        "    on_road_check = True\n",
        "    collision_check = False\n",
        "\n",
        "    # Agent activation flag and target lane storage\n",
        "    activated_target_lane = None\n",
        "\n",
        "    # Update IDM inputs based on initial state\n",
        "    gap = lead_state['x']\n",
        "    delta_velocity = lead_state['vx']\n",
        "    input_variables = [gap, ego_state_idm['vx'], delta_velocity]\n",
        "\n",
        "    steps = 0\n",
        "    terminal = 0\n",
        "    # Removed cumulative episode reward accumulation.\n",
        "    # current_episode_reward = 0\n",
        "    # episode_acce_reward = 0\n",
        "    # episode_rate_reward = 0\n",
        "    # episode_time_reward = 0\n",
        "    counter = 0\n",
        "    # epsilon = 0.1\n",
        "\n",
        "    # For stepwise plotting, record agent actions per timestep\n",
        "    episode_actions = []\n",
        "\n",
        "    # Main loop for each episode\n",
        "    while steps < timesteps and counter < 1:\n",
        "        steps += 1\n",
        "        # Epsilon decay based on total_steps_taken\n",
        "        # epsilon = max(0.05, 0.1 - (0.1 - 0.05) * (total_steps_taken / epsilon_decay_rate))\n",
        "\n",
        "        # Recompute gap\n",
        "        gap = lead_state['x']\n",
        "\n",
        "        ########################################################################\n",
        "        \"\"\"Action Preparation\"\"\"\n",
        "        ########################################################################\n",
        "        gap_control = Gap_Controller(obs, threshold)\n",
        "        activated_target_lane = gap_control.lane_checker()\n",
        "\n",
        "        # Determine target lane\n",
        "        target_id = activated_target_lane\n",
        "        agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Agent Remains Inactive Unless Heading and Gap Meet a Requirement\n",
        "            if (gap < threshold and target_id != env.unwrapped.vehicle.lane_index[2]) or abs(agent_state['thetha']) > thetha_threshold:\n",
        "                agent_action = policy_network.action(agent_state).item()\n",
        "            else:\n",
        "                agent_action = 0\n",
        "\n",
        "        # Store prior state for buffer\n",
        "        old_state = agent_state\n",
        "        obs_old = obs\n",
        "\n",
        "        # IDM Longitudinal Control\n",
        "        idm_acceleration = ego_vehicle_idm.longitudinal_controller(input_variables)\n",
        "        longitudinal_control = idm_acceleration\n",
        "\n",
        "        # Record the action for later stepwise plotting (e.g., steering angle per timestep)\n",
        "        episode_actions.append(agent_action)\n",
        "\n",
        "        # Transform agent action (acceleration prediction) to steering angle\n",
        "        steering_angle = state_manager.steering_angle(agent_action, L)\n",
        "        lateral_control = steering_angle\n",
        "\n",
        "        # Combine longitudinal and lateral actions\n",
        "        action = [longitudinal_control, lateral_control]\n",
        "\n",
        "        ########################################################################\n",
        "        \"\"\"Data Collection\"\"\"\n",
        "        ########################################################################\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "        # Check for off-road or collision events\n",
        "        on_road_check = env.unwrapped.vehicle.on_road\n",
        "        collision_check = info['crashed']\n",
        "\n",
        "        # Update state manager with new observation and applied longitudinal control\n",
        "        state_manager = ENV(obs, longitudinal_control)\n",
        "        # Compute reward based on the new state\n",
        "        reward_per_episode = state_manager.reward_function(obs_old, obs, target_id)\n",
        "        # Grab the reward (the first element of the returned list)\n",
        "        reward = reward_per_episode[0]\n",
        "\n",
        "        # Instead of adding up rewards per episode, simply append the reward from this timestep.\n",
        "        timestep_rewards.append(reward)\n",
        "\n",
        "        # Removed cumulative sums:\n",
        "        # current_episode_reward += reward\n",
        "        # episode_acce_reward += reward_per_episode[1]\n",
        "        # episode_rate_reward += reward_per_episode[2]\n",
        "        # episode_time_reward += reward_per_episode[3]\n",
        "\n",
        "        ########################################################################\n",
        "        \"\"\"Update IDM Controller Inputs for next step\"\"\"\n",
        "        ########################################################################\n",
        "        ego_state_idm = state_manager.ego_state_idm()\n",
        "        lead_state = state_manager.longitudinal_lead_state()\n",
        "        agent_state = state_manager.ego_state_agent(target_id=target_id)\n",
        "\n",
        "        gap = lead_state['x']\n",
        "        delta_velocity = lead_state['vx']\n",
        "        input_variables = [gap, ego_state_idm['vx'], delta_velocity]\n",
        "\n",
        "        ########################################################################\n",
        "        \"\"\"Terminal Condition Check\"\"\"\n",
        "        ########################################################################\n",
        "        # Update error counter\n",
        "        if not on_road_check or collision_check:\n",
        "            counter += 1\n",
        "            terminal = 1\n",
        "\n",
        "        # Terminal condition check: if steps reaches timesteps, mark terminal.\n",
        "        if steps == timesteps:\n",
        "            terminal = 1\n",
        "        else:\n",
        "            terminal = 0\n",
        "\n",
        "        # Update experience buffer\n",
        "        buffer.add\n"
      ],
      "metadata": {
        "id": "BBIc0gWQ_QAn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}